{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4aed2b59",
   "metadata": {},
   "source": [
    "# Turkish Earthquake Clustering Analysis\n",
    "\n",
    "This notebook implements unsupervised learning techniques to identify intrinsic patterns and risk zones in our AFAD earthquake dataset. While supervised learning (in the companion notebook) predicts specific earthquake magnitudes, unsupervised methods reveal natural groupings and spatial correlations without predefined output variables.\n",
    "\n",
    "Our multi-faceted unsupervised analysis will:\n",
    "\n",
    "1. **Discover natural earthquake clusters** using density-based and centroid-based algorithms to identify regions with similar seismic characteristics, potentially corresponding to distinct tectonic regimes or fault systems\n",
    "   \n",
    "2. **Identify high-risk seismic zones** by analyzing the spatial density, magnitude distribution, and temporal patterns of earthquake clusters in relation to Turkey's complex fault network\n",
    "   \n",
    "3. **Quantify regional vulnerability** for major Turkish population centers by combining seismic pattern data with demographic information to assess potential human impact\n",
    "   \n",
    "4. **Visualize multidimensional patterns** through interactive maps, 3D projections, and dimensionality reduction techniques that reveal relationships not visible in raw data\n",
    "\n",
    "The insights from this unsupervised analysis complement our supervised prediction models by revealing the underlying structure of Turkey's seismic landscape. By identifying natural groupings in the data, we can better understand regional risk variations and potentially improve hazard assessment beyond what is possible with regression models alone.\n",
    "\n",
    "This approach aligns with modern seismological practice, where clustering analysis is increasingly used to identify seismic sources, aftershock patterns, and earthquake swarms that indicate areas of elevated tectonic stress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4e90a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import folium\n",
    "from folium.plugins import HeatMap, MarkerCluster\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import joblib\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, LineString\n",
    "import math\n",
    "import os\n",
    "\n",
    "# Set visualization settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('viridis')\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Load the clean earthquake dataset with original coordinates\n",
    "earthquake_df = pd.read_csv('produced_data/clean_earthquake_data.csv')\n",
    "\n",
    "# Load fault line data\n",
    "fault_gdf = gpd.read_file('data\\\\tr_faults_imp.geojson')\n",
    "print(f\"Number of fault lines: {len(fault_gdf)}\")\n",
    "print(f\"Available properties: {fault_gdf.columns.tolist()}\")\n",
    "\n",
    "# Verify coordinate range\n",
    "print(f\"Dataset shape: {earthquake_df.shape}\")\n",
    "print(f\"Coordinate ranges:\")\n",
    "print(f\"Longitude: {earthquake_df['Longitude'].min():.2f} to {earthquake_df['Longitude'].max():.2f}\")\n",
    "print(f\"Latitude: {earthquake_df['Latitude'].min():.2f} to {earthquake_df['Latitude'].max():.2f}\")\n",
    "\n",
    "earthquake_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f873bb2",
   "metadata": {},
   "source": [
    "## 1. Data Preparation for Clustering\n",
    "\n",
    "Before applying clustering algorithms, we must carefully prepare our earthquake data to ensure meaningful and interpretable results. Unsupervised learning algorithms are particularly sensitive to feature selection, scaling, and preprocessing decisions.\n",
    "\n",
    "Our preparation process includes:\n",
    "\n",
    "1. **Feature selection and extraction**:\n",
    "   - **Spatial coordinates** (Longitude, Latitude): Capturing the geographic distribution of events\n",
    "   - **Physical parameters** (Depth, Magnitude): Representing earthquake characteristics\n",
    "   - **Fault proximity metrics**: Calculating the minimum distance from each earthquake to the nearest fault line using spatial geometry algorithms\n",
    "   - **Fault importance weighting**: Incorporating the classification of fault significance based on historical activity and length\n",
    "\n",
    "2. **Missing value treatment**:\n",
    "   - Identifying nulls in each clustering feature\n",
    "   - Applying median imputation for continuous variables to maintain distributional characteristics\n",
    "   - Documenting imputation decisions for reproducibility\n",
    "\n",
    "3. **Feature standardization**:\n",
    "   - Implementing StandardScaler (z-score normalization) to transform each feature to zero mean and unit variance\n",
    "   - This step is essential because clustering algorithms use distance metrics that are highly sensitive to feature scales\n",
    "   - Without standardization, features with larger numerical ranges (like geographic coordinates) would dominate the clustering process\n",
    "\n",
    "4. **Dimensionality considerations**:\n",
    "   - Balancing the inclusion of informative features with the \"curse of dimensionality\"\n",
    "   - In high-dimensional spaces, distance metrics become less meaningful as points tend to become equidistant\n",
    "   - Selecting features with clear geophysical relevance to ensure clusters have physical interpretability\n",
    "\n",
    "The quality of clustering results depends heavily on these preprocessing decisions. By carefully selecting and standardizing features that capture the key aspects of earthquake events, we create a foundation for discovering meaningful patterns that correspond to real geological phenomena rather than artifacts of the data representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadc5fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If fault features aren't in the data (because we're running this independently of supervised notebook)\n",
    "# Calculate distance to fault lines\n",
    "def calc_fault_distance(row, fault_gdf):\n",
    "    point = Point(row['Longitude'], row['Latitude'])\n",
    "    \n",
    "    # Calculate distance to each fault line\n",
    "    distances = []\n",
    "    for idx, fault in fault_gdf.iterrows():\n",
    "        fault_geom = fault.geometry\n",
    "        dist = point.distance(fault_geom)\n",
    "        distances.append((dist, idx))\n",
    "    \n",
    "    # Find the closest fault\n",
    "    closest_dist, closest_idx = min(distances, key=lambda x: x[0])\n",
    "    \n",
    "    # Convert distance to kilometers (approximation)\n",
    "    # 1 degree ≈ 111 km at the equator\n",
    "    dist_km = closest_dist * 111\n",
    "    \n",
    "    # Get fault properties\n",
    "    closest_fault = fault_gdf.iloc[closest_idx]\n",
    "    \n",
    "    return pd.Series({\n",
    "        'distance_to_fault': dist_km,\n",
    "        'nearest_fault_name': closest_fault.get('FAULT_NAME', 'Unknown'),\n",
    "        'nearest_fault_importance': closest_fault.get('importance', 0)\n",
    "    })\n",
    "\n",
    "# Only calculate if these features don't exist yet\n",
    "if 'distance_to_fault' not in earthquake_df.columns:\n",
    "    print(\"Calculating fault distances...\")\n",
    "    # Process in batches to avoid memory issues\n",
    "    batch_size = 1000\n",
    "    all_results = []\n",
    "    \n",
    "    for i in range(0, len(earthquake_df), batch_size):\n",
    "        batch_end = min(i + batch_size, len(earthquake_df))\n",
    "        print(f\"Processing batch {i} to {batch_end}...\")\n",
    "        batch = earthquake_df.iloc[i:batch_end]\n",
    "        results = batch.apply(lambda row: calc_fault_distance(row, fault_gdf), axis=1)\n",
    "        all_results.append(results)\n",
    "    \n",
    "    fault_features = pd.concat(all_results)\n",
    "    earthquake_df = pd.concat([earthquake_df, fault_features], axis=1)\n",
    "\n",
    "# Select features for clustering\n",
    "# Include geographic features and fault-related features for spatial clustering\n",
    "clustering_features = ['Longitude', 'Latitude', 'Depth', 'Magnitude']\n",
    "\n",
    "# Add fault-related features if available\n",
    "fault_features = ['distance_to_fault', 'nearest_fault_importance']\n",
    "for feature in fault_features:\n",
    "    if feature in earthquake_df.columns:\n",
    "        clustering_features.append(feature)\n",
    "\n",
    "# Create a subset of data for clustering\n",
    "cluster_data = earthquake_df[clustering_features].copy()\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing values in clustering features:\")\n",
    "print(cluster_data.isnull().sum())\n",
    "\n",
    "# Fill any missing values if needed\n",
    "cluster_data.fillna(cluster_data.median(), inplace=True)\n",
    "\n",
    "# Standardize features for clustering algorithms\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(cluster_data)\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=clustering_features)\n",
    "\n",
    "print(\"Data prepared for clustering:\")\n",
    "scaled_df.describe()\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(\"maps\", exist_ok=True)\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "os.makedirs(\"produced_data\", exist_ok=True)\n",
    "\n",
    "print(\"Created output directories: maps, models, produced_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9a1cbc",
   "metadata": {},
   "source": [
    "## 2. K-Means Clustering\n",
    "\n",
    "K-Means clustering will partition our earthquake data into distinct groups based on feature similarity, potentially revealing natural seismic zones across Turkey's complex tectonic landscape. This centroid-based algorithm assigns each earthquake to the nearest of k cluster centers, then iteratively refines these centers.\n",
    "\n",
    "Our K-Means implementation follows these steps:\n",
    "\n",
    "1. **Optimal cluster number determination**:\n",
    "   - **Elbow Method**: Plotting inertia (sum of squared distances to nearest centroid) across different k values to identify the point of diminishing returns\n",
    "   - **Silhouette Analysis**: Calculating the mean silhouette coefficient, which measures how similar points are to their assigned cluster versus other clusters\n",
    "   - **Domain knowledge integration**: Considering Turkey's known tectonic regions when interpreting optimal k values\n",
    "\n",
    "2. **Algorithm application**:\n",
    "   - Implementing K-Means with the sklearn library using the determined optimal k\n",
    "   - Using multiple random initializations (n_init=10) to avoid local minima\n",
    "   - Ensuring convergence with sufficient iterations\n",
    "\n",
    "3. **Cluster analysis**:\n",
    "   - Examining the distribution of earthquakes across clusters\n",
    "   - Calculating cluster-specific statistics (mean magnitude, depth, etc.)\n",
    "   - Assessing geographical coherence of the resulting clusters\n",
    "\n",
    "4. **Validation**:\n",
    "   - Evaluating cluster separation using silhouette scores\n",
    "   - Assessing cluster stability across different random initializations\n",
    "\n",
    "K-Means is particularly effective for identifying general earthquake zones that might correspond to distinct tectonic provinces or fault systems. For example, separate clusters might emerge along the North Anatolian Fault versus the East Anatolian Fault, reflecting their different seismic characteristics. The resulting partition provides a macroscopic view of Turkey's seismic landscape, although K-Means' preference for spherical clusters may limit its ability to capture irregularly shaped seismic zones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf86d87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine optimal number of clusters using the Elbow Method\n",
    "inertia = []\n",
    "silhouette_scores = []\n",
    "k_range = range(2, 12)\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(scaled_data)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "    \n",
    "    # Compute silhouette score\n",
    "    labels = kmeans.labels_\n",
    "    silhouette_avg = silhouette_score(scaled_data, labels)\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "    \n",
    "    print(f\"K={k}, Inertia={kmeans.inertia_:.2f}, Silhouette Score={silhouette_avg:.3f}\")\n",
    "\n",
    "# Plot the Elbow Method results\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(k_range, inertia, 'o-')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(k_range, silhouette_scores, 'o-')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Analysis for Optimal k')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Select the optimal k based on the elbow method and silhouette score\n",
    "optimal_k = 5  # You should adjust this based on the plots\n",
    "\n",
    "# Apply K-Means with the optimal number of clusters\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "cluster_labels = kmeans.fit_predict(scaled_data)\n",
    "\n",
    "# Add cluster labels to the original dataframe\n",
    "earthquake_df['KMeans_Cluster'] = cluster_labels\n",
    "\n",
    "# Display the distribution of clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "cluster_counts = earthquake_df['KMeans_Cluster'].value_counts().sort_index()\n",
    "cluster_counts.plot(kind='bar')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Number of Earthquakes')\n",
    "plt.title('Distribution of Earthquakes Across K-Means Clusters')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0db857",
   "metadata": {},
   "source": [
    "## 3. Visualizing K-Means Clusters\n",
    "\n",
    "Visualizing the identified clusters helps interpret their geological significance and spatial distribution across Turkey. These visualizations transform abstract mathematical groupings into meaningful patterns that can be related to tectonic features.\n",
    "\n",
    "We'll implement multiple visualization approaches:\n",
    "\n",
    "1. **Interactive geographic mapping**:\n",
    "   - Creating a Folium-based map with cluster-specific coloring\n",
    "   - Adding circle markers sized by magnitude and colored by cluster assignment\n",
    "   - Overlaying major fault lines with importance-based styling\n",
    "   - Visualizing cluster centroids as larger markers to show the focal point of each group\n",
    "   - Adding interactive popups with detailed earthquake information\n",
    "\n",
    "2. **Cluster characteristic analysis**:\n",
    "   - Generating parallel coordinate plots showing feature distributions within each cluster\n",
    "   - Creating violin plots comparing magnitude and depth distributions across clusters\n",
    "   - Calculating and visualizing the mean characteristics of each cluster (depth, magnitude, fault proximity)\n",
    "\n",
    "3. **Fault relationship visualization**:\n",
    "   - Plotting the distance relationship between cluster members and nearest faults\n",
    "   - Visualizing how clusters align with major fault systems\n",
    "   - Analyzing whether certain clusters consistently occur near specific fault types\n",
    "\n",
    "4. **3D visualization**:\n",
    "   - Implementing interactive 3D scatter plots incorporating longitude, latitude, and depth\n",
    "   - Using color to represent cluster assignment and size to encode magnitude\n",
    "   - Enabling rotation and zooming to explore the subsurface distribution of clusters\n",
    "\n",
    "These visualizations help answer key questions about the identified clusters:\n",
    "- Do earthquakes naturally group by geographic region, depth profile, or magnitude range?\n",
    "- How do cluster boundaries relate to known geological features like fault zones?\n",
    "- Are certain clusters characterized by specific types of seismic activity?\n",
    "- How does the 3D distribution of clusters relate to subsurface tectonic structures?\n",
    "\n",
    "By visually exploring these relationships, we can begin translating mathematical clusters into geologically meaningful interpretations about Turkey's seismic patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c40eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze cluster characteristics\n",
    "cluster_analysis = earthquake_df.groupby('KMeans_Cluster').agg({\n",
    "    'Longitude': 'mean',\n",
    "    'Latitude': 'mean',\n",
    "    'Depth': 'mean',\n",
    "    'Magnitude': 'mean',\n",
    "    'KMeans_Cluster': 'count'\n",
    "}).rename(columns={'KMeans_Cluster': 'Count'})\n",
    "\n",
    "# Add fault distance stats if available\n",
    "if 'distance_to_fault' in earthquake_df.columns:\n",
    "    fault_stats = earthquake_df.groupby('KMeans_Cluster').agg({\n",
    "        'distance_to_fault': ['mean', 'min'],\n",
    "        'nearest_fault_importance': 'mean'\n",
    "    })\n",
    "    \n",
    "    # Flatten MultiIndex columns\n",
    "    fault_stats.columns = ['_'.join(col).strip() for col in fault_stats.columns.values]\n",
    "    cluster_analysis = pd.concat([cluster_analysis, fault_stats], axis=1)\n",
    "\n",
    "print(\"Cluster characteristics:\")\n",
    "print(cluster_analysis)\n",
    "\n",
    "# Create K-means map visualization with folium - FIXED coordinates\n",
    "kmeans_map = folium.Map(location=[38.5, 35.5], zoom_start=6)\n",
    "\n",
    "# Create a discrete color map for clusters\n",
    "cluster_colors = ['red', 'blue', 'green', 'purple', 'orange', 'darkred', 'darkblue', 'cadetblue']\n",
    "\n",
    "# Add clusters as markers, ensuring coordinates are within Turkey's boundaries\n",
    "for idx, row in earthquake_df.iterrows():\n",
    "    cluster_idx = int(row['KMeans_Cluster']) % len(cluster_colors)\n",
    "    color = cluster_colors[cluster_idx]\n",
    "    \n",
    "    # Use the enhanced radius calculation from risk map\n",
    "    if row['Magnitude'] >= 7:\n",
    "        # Much more exponential growth for major earthquakes\n",
    "        radius = 15 + ((row['Magnitude'] - 7) ** 2) * 6\n",
    "    else:\n",
    "        radius = 3 + (row['Magnitude'] - 4) ** 1.5  # Exponential growth\n",
    "    \n",
    "    folium.CircleMarker(\n",
    "        location=[row['Latitude'], row['Longitude']],  # Latitude first, Longitude second\n",
    "        radius=radius,\n",
    "        color=color,\n",
    "        fill=True,\n",
    "        fill_color=color,\n",
    "        fill_opacity=0.7,\n",
    "        popup=f\"Cluster: {row['KMeans_Cluster']}<br>Magnitude: {row['Magnitude']}\"\n",
    "    ).add_to(kmeans_map)\n",
    "\n",
    "# Add cluster centers as larger markers\n",
    "for cluster_id, group in earthquake_df.groupby('KMeans_Cluster'):\n",
    "    center_lat = group['Latitude'].mean()\n",
    "    center_lon = group['Longitude'].mean()\n",
    "    cluster_idx = int(cluster_id) % len(cluster_colors)\n",
    "    color = cluster_colors[cluster_idx]\n",
    "    \n",
    "    folium.CircleMarker(\n",
    "        location=[center_lat, center_lon],  # Latitude first, Longitude second\n",
    "        radius=8,\n",
    "        color='black',\n",
    "        fill=True,\n",
    "        fill_color=color,\n",
    "        fill_opacity=0.9,\n",
    "        popup=f\"Cluster Center {cluster_id}\"\n",
    "    ).add_to(kmeans_map)\n",
    "\n",
    "# Add fault lines to the map\n",
    "def add_faults_to_map(map_obj, fault_gdf, importance_threshold=0):\n",
    "    # Filter faults by importance if desired\n",
    "    if importance_threshold > 0:\n",
    "        fault_data = fault_gdf[fault_gdf['importance'] >= importance_threshold]\n",
    "    else:\n",
    "        fault_data = fault_gdf\n",
    "    \n",
    "    # Color by importance\n",
    "    def style_function(feature):\n",
    "        importance = feature['properties']['importance']\n",
    "        color = '#FF0000' if importance >= 4 else '#FFA500' if importance >= 3 else '#FFFF00'\n",
    "        return {\n",
    "            'color': color,\n",
    "            'weight': importance * 0.5,  # Thicker lines for more important faults\n",
    "            'opacity': 0.7\n",
    "        }\n",
    "    \n",
    "    # Add GeoJSON to map\n",
    "    folium.GeoJson(\n",
    "        fault_data,\n",
    "        name='Fault Lines',\n",
    "        style_function=style_function,\n",
    "        tooltip=folium.GeoJsonTooltip(fields=['FAULT_NAME', 'importance']),\n",
    "    ).add_to(map_obj)\n",
    "    \n",
    "    return map_obj\n",
    "\n",
    "# Add fault lines to the map\n",
    "kmeans_map = add_faults_to_map(kmeans_map, fault_gdf, importance_threshold=3)\n",
    "\n",
    "# Add better tile layer\n",
    "folium.TileLayer('cartodbpositron').add_to(kmeans_map)\n",
    "\n",
    "# Add layer control\n",
    "folium.LayerControl().add_to(kmeans_map)\n",
    "\n",
    "# For K-means map - adding clearer legend\n",
    "kmeans_legend_html = '''\n",
    "<div style=\"position: fixed; bottom: 50px; right: 50px; width: 200px; \n",
    "    background-color: white; border:2px solid grey; z-index:9999; \n",
    "    padding: 10px; border-radius: 5px; font-family: Arial;\">\n",
    "    <h4 style=\"margin-top:0;\">K-Means Clusters</h4>\n",
    "    \n",
    "    <div style=\"margin-top:5px;\">\n",
    "      <p><b>Cluster Colors:</b></p>\n",
    "      <div style=\"display: flex; align-items: center; margin-bottom: 5px;\">\n",
    "        <div style=\"background-color: red; width: 15px; height: 15px; margin-right: 5px; border-radius: 50%;\"></div>\n",
    "        <span>Cluster 0</span>\n",
    "      </div>\n",
    "      <div style=\"display: flex; align-items: center; margin-bottom: 5px;\">\n",
    "        <div style=\"background-color: blue; width: 15px; height: 15px; margin-right: 5px; border-radius: 50%;\"></div>\n",
    "        <span>Cluster 1</span>\n",
    "      </div>\n",
    "      <div style=\"display: flex; align-items: center; margin-bottom: 5px;\">\n",
    "        <div style=\"background-color: green; width: 15px; height: 15px; margin-right: 5px; border-radius: 50%;\"></div>\n",
    "        <span>Cluster 2</span>\n",
    "      </div>\n",
    "      <div style=\"display: flex; align-items: center; margin-bottom: 5px;\">\n",
    "        <div style=\"background-color: purple; width: 15px; height: 15px; margin-right: 5px; border-radius: 50%;\"></div>\n",
    "        <span>Cluster 3</span>\n",
    "      </div>\n",
    "      <div style=\"display: flex; align-items: center; margin-bottom: 5px;\">\n",
    "        <div style=\"background-color: orange; width: 15px; height: 15px; margin-right: 5px; border-radius: 50%;\"></div>\n",
    "        <span>Cluster 4</span>\n",
    "      </div>\n",
    "    </div>\n",
    "    \n",
    "    <div style=\"margin-top:10px;\">\n",
    "      <p><b>Marker Size:</b> Proportional to earthquake magnitude</p>\n",
    "    </div>\n",
    "    \n",
    "    <div style=\"margin-top:10px;\">\n",
    "      <p><b>Fault Line Importance:</b></p>\n",
    "      <p><span style=\"color:#FF0000;\">━━━</span> High (4+)</p>\n",
    "      <p><span style=\"color:#FFA500;\">━━━</span> Medium (3)</p>\n",
    "      <p><span style=\"color:#FFFF00;\">━━━</span> Low (<3)</p>\n",
    "    </div>\n",
    "</div>\n",
    "'''\n",
    "kmeans_map.get_root().html.add_child(folium.Element(kmeans_legend_html))\n",
    "\n",
    "# Save the map\n",
    "kmeans_map.save('maps/kmeans_clusters_map.html')\n",
    "print(\"K-means cluster map saved as 'maps/kmeans_clusters_map.html'\")\n",
    "\n",
    "# Also create a Plotly version for the notebook \n",
    "fig_kmeans = px.scatter_mapbox(\n",
    "    earthquake_df.sample(min(3000, len(earthquake_df))), \n",
    "    lat='Latitude',  # Ensure correct parameter for latitude \n",
    "    lon='Longitude',  # Ensure correct parameter for longitude\n",
    "    color='KMeans_Cluster',\n",
    "    size='Magnitude',\n",
    "    color_continuous_scale=px.colors.qualitative.Bold,\n",
    "    size_max=15,\n",
    "    zoom=5,\n",
    "    center={\"lat\": 38.5, \"lon\": 35.5},  # Centered on Turkey\n",
    "    mapbox_style=\"open-street-map\",\n",
    "    title='K-Means Clusters of Turkish Earthquakes',\n",
    "    hover_data=['Depth', 'Magnitude', 'KMeans_Cluster']\n",
    ")\n",
    "fig_kmeans.update_layout(margin={\"r\":0,\"t\":40,\"l\":0,\"b\":0})\n",
    "fig_kmeans.write_html('maps/kmeans_clusters_map_plotly.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8be909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 3D scatter plot of KMeans clusters with fault information\n",
    "fig = px.scatter_3d(\n",
    "    earthquake_df.sample(min(5000, len(earthquake_df))), \n",
    "    x='Longitude', \n",
    "    y='Latitude', \n",
    "    z='Depth',\n",
    "    color='KMeans_Cluster', \n",
    "    size='Magnitude',\n",
    "    color_continuous_scale=px.colors.qualitative.Bold,\n",
    "    hover_data=['distance_to_fault', 'nearest_fault_importance'] if 'distance_to_fault' in earthquake_df.columns else None,\n",
    "    title='3D Visualization of KMeans Earthquake Clusters'\n",
    ")\n",
    "\n",
    "# Ensure proper axis orientation and labels\n",
    "fig.update_layout(scene=dict(\n",
    "    xaxis_title='Longitude',\n",
    "    yaxis_title='Latitude',\n",
    "    zaxis_title='Depth (km)',\n",
    "    zaxis=dict(autorange=\"reversed\")  # Reverse depth axis so deeper is lower\n",
    "))\n",
    "\n",
    "# Save the interactive 3D visualization\n",
    "fig.write_html('maps/kmeans_clusters_3d.html')\n",
    "print(\"3D KMeans visualization saved to maps/kmeans_clusters_3d.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0c470a",
   "metadata": {},
   "source": [
    "## 4. DBSCAN Clustering\n",
    "\n",
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) offers several advantages over K-Means for earthquake analysis, as it can identify clusters of arbitrary shape based on local density characteristics. This approach is particularly suitable for seismic data, where earthquake distributions often follow irregular fault geometries.\n",
    "\n",
    "DBSCAN's key characteristics and implementation steps include:\n",
    "\n",
    "1. **Density-based cluster definition**:\n",
    "   - **Core points**: Points with at least `min_samples` neighbors within distance `epsilon`\n",
    "   - **Directly reachable points**: Points within `epsilon` distance of a core point\n",
    "   - **Density-connected points**: Points connected through chains of directly reachable points\n",
    "   - **Noise points**: Points that don't belong to any cluster, labeled as -1\n",
    "\n",
    "2. **Parameter selection methodology**:\n",
    "   - **Epsilon determination**: Using k-distance graphs to identify the \"elbow point\" where distance to kth nearest neighbor increases significantly\n",
    "   - **Min_samples tuning**: Balancing between too many small clusters and too few large clusters\n",
    "   - **Domain knowledge integration**: Considering the expected spatial density of earthquakes along fault systems\n",
    "\n",
    "3. **Key advantages for seismological applications**:\n",
    "   - **No predefined cluster number**: Automatically determines the number of clusters based on data density\n",
    "   - **Irregular cluster shapes**: Can identify elongated clusters that may follow fault lines\n",
    "   - **Noise identification**: Explicitly identifies outliers/noise points, which may represent isolated events\n",
    "   - **Variable density handling**: Can detect clusters of varying densities with appropriate parameterization\n",
    "\n",
    "4. **Implementation challenges**:\n",
    "   - **Parameter sensitivity**: Results highly dependent on epsilon and min_samples values\n",
    "   - **Varying densities**: Potential difficulty with regions of significantly different earthquake densities\n",
    "   - **High-dimensional limitations**: Reduced effectiveness in high-dimensional feature spaces due to distance concentration\n",
    "\n",
    "DBSCAN may reveal micro-regions with distinct earthquake patterns that K-Means would merge into larger clusters. For example, it might identify elongated clusters following specific fault segments or isolate unusual earthquake swarms that don't fit the broader regional pattern. These fine-grained insights complement the more general groupings from K-Means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50ca231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply DBSCAN clustering\n",
    "# We need to find appropriate epsilon and min_samples values\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Determine epsilon using k-distance graph\n",
    "neighbors = NearestNeighbors(n_neighbors=20)\n",
    "neighbors_fit = neighbors.fit(scaled_data)\n",
    "distances, indices = neighbors_fit.kneighbors(scaled_data)\n",
    "\n",
    "# Sort distances for k-distance graph\n",
    "distances = np.sort(distances, axis=0)\n",
    "distances = distances[:, 19]  # 20th neighbor\n",
    "\n",
    "# Plot k-distance graph\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(distances)\n",
    "plt.xlabel('Points sorted by distance')\n",
    "plt.ylabel('20th Nearest Neighbor Distance')\n",
    "plt.title('K-Distance Graph for DBSCAN Epsilon Selection')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Based on the k-distance plot, choose an appropriate epsilon\n",
    "# Look for the \"elbow\" in the plot\n",
    "epsilon = 0.5  # Adjust based on the plot\n",
    "min_samples = 20  # Minimum neighbors for a core point\n",
    "\n",
    "# Apply DBSCAN with chosen parameters\n",
    "dbscan = DBSCAN(eps=epsilon, min_samples=min_samples)\n",
    "dbscan_labels = dbscan.fit_predict(scaled_data)\n",
    "\n",
    "# Add DBSCAN labels to the dataframe\n",
    "earthquake_df['DBSCAN_Cluster'] = dbscan_labels\n",
    "\n",
    "# Count the number of clusters and noise points\n",
    "n_clusters = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)\n",
    "n_noise = list(dbscan_labels).count(-1)\n",
    "\n",
    "print(f\"DBSCAN found {n_clusters} clusters and {n_noise} noise points.\")\n",
    "print(f\"Percentage of noise points: {n_noise / len(dbscan_labels) * 100:.2f}%\")\n",
    "\n",
    "# Display the distribution of DBSCAN clusters\n",
    "plt.figure(figsize=(12, 6))\n",
    "cluster_counts = earthquake_df['DBSCAN_Cluster'].value_counts().sort_index()\n",
    "cluster_counts.plot(kind='bar')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Number of Earthquakes')\n",
    "plt.title('Distribution of Earthquakes Across DBSCAN Clusters')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfa43c8",
   "metadata": {},
   "source": [
    "## 5. Visualizing DBSCAN Clusters\n",
    "\n",
    "Visualizing DBSCAN clusters reveals density-based earthquake patterns and outlier events that may have special significance. These visualizations highlight the advantage of density-based clustering for identifying irregularly shaped seismic zones and isolated events.\n",
    "\n",
    "Our DBSCAN visualization approach includes:\n",
    "\n",
    "1. **Spatial distribution mapping**:\n",
    "   - Creating an interactive Folium map with distinct colors for each DBSCAN cluster\n",
    "   - Using a special designation (typically black) for noise points that don't belong to any cluster\n",
    "   - Differentiating marker size by magnitude to show energy distribution\n",
    "   - Overlaying major fault systems to examine cluster-fault relationships\n",
    "   - Implementing interactive elements for detailed exploration\n",
    "\n",
    "2. **Density heatmap creation**:\n",
    "   - Generating kernel density estimation (KDE) heatmaps to visualize earthquake concentration\n",
    "   - Using Plotly's density_mapbox to create interactive density visualizations\n",
    "   - Comparing density patterns with identified clusters to verify alignment\n",
    "\n",
    "3. **Cluster vs. noise analysis**:\n",
    "   - Creating comparative visualizations of clustered points versus noise points\n",
    "   - Analyzing the characteristics (magnitude, depth, temporal distribution) of noise points\n",
    "   - Determining whether noise points represent truly anomalous events or sampling artifacts\n",
    "\n",
    "4. **3D visualization with density encoding**:\n",
    "   - Implementing 3D scatter plots with longitude, latitude, and depth\n",
    "   - Encoding cluster assignment through color and density through opacity\n",
    "   - Identifying potential subsurface patterns in cluster formation\n",
    "\n",
    "These visualizations address important questions about density-based earthquake patterns:\n",
    "- Do high-density earthquake zones correspond to known fault systems?\n",
    "- What characteristics distinguish noise points from clustered events?\n",
    "- Do certain regions show more scattered activity while others show tight clustering?\n",
    "- How do the density-based clusters compare with the centroid-based (K-Means) clusters?\n",
    "\n",
    "By examining these density-based patterns, we gain insights into areas of concentrated seismic activity that may represent zones of elevated strain release or tectonic stress accumulation. The noise points are equally important, as they may represent unusual events worth individual investigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e7ba0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DBSCAN map visualization with folium\n",
    "dbscan_map = folium.Map(location=[38.5, 35.5], zoom_start=6)\n",
    "\n",
    "# Number of clusters excluding noise\n",
    "num_clusters = len(set(earthquake_df['DBSCAN_Cluster'])) - (1 if -1 in earthquake_df['DBSCAN_Cluster'].values else 0)\n",
    "colormap = cm.get_cmap('tab20', max(num_clusters + 1, 2))  # +1 for noise points\n",
    "\n",
    "# Function to get color for cluster\n",
    "def get_cluster_color(cluster_id):\n",
    "    if cluster_id == -1:  # Noise points\n",
    "        return '#000000'  # Black\n",
    "    else:\n",
    "        rgba = colormap(cluster_id % max(num_clusters, 1))\n",
    "        return mcolors.rgb2hex(rgba)\n",
    "\n",
    "# Add data points with proper colors (no sampling)\n",
    "for idx, row in earthquake_df.iterrows():\n",
    "    cluster_id = int(row['DBSCAN_Cluster'])\n",
    "    color = get_cluster_color(cluster_id)\n",
    "    \n",
    "    # Use the enhanced radius calculation from risk map\n",
    "    if row['Magnitude'] >= 7:\n",
    "        # Much more exponential growth for major earthquakes\n",
    "        radius = 15 + ((row['Magnitude'] - 7) ** 2) * 6\n",
    "    else:\n",
    "        radius = 3 + (row['Magnitude'] - 4) ** 1.5  # Exponential growth\n",
    "    \n",
    "    folium.CircleMarker(\n",
    "        location=[row['Latitude'], row['Longitude']],  # Latitude first, then Longitude\n",
    "        radius=radius,\n",
    "        color=color,\n",
    "        fill=True,\n",
    "        fill_color=color,\n",
    "        fill_opacity=0.7 if cluster_id != -1 else 0.3,  # Make noise points more transparent\n",
    "        popup=f\"Cluster: {cluster_id}<br>Magnitude: {row['Magnitude']}\"\n",
    "    ).add_to(dbscan_map)\n",
    "\n",
    "# Add fault lines to the map\n",
    "dbscan_map = add_faults_to_map(dbscan_map, fault_gdf, importance_threshold=3)\n",
    "\n",
    "# Add better tile layer\n",
    "folium.TileLayer('cartodbpositron').add_to(dbscan_map)\n",
    "\n",
    "# Add layer control\n",
    "folium.LayerControl().add_to(dbscan_map)\n",
    "\n",
    "# For DBSCAN map - adding clearer legend\n",
    "dbscan_legend_html = '''\n",
    "<div style=\"position: fixed; bottom: 50px; right: 50px; width: 200px; \n",
    "    background-color: white; border:2px solid grey; z-index:9999; \n",
    "    padding: 10px; border-radius: 5px; font-family: Arial;\">\n",
    "    <h4 style=\"margin-top:0;\">DBSCAN Clusters</h4>\n",
    "    \n",
    "    <div style=\"margin-top:5px;\">\n",
    "      <p><b>Cluster Colors:</b> Each color represents a distinct spatial cluster</p>\n",
    "      <div style=\"display: flex; align-items: center; margin-bottom: 5px;\">\n",
    "        <div style=\"background-color: black; width: 15px; height: 15px; margin-right: 5px; border-radius: 50%; opacity: 0.3;\"></div>\n",
    "        <span>Noise points (-1)</span>\n",
    "      </div>\n",
    "    </div>\n",
    "    \n",
    "    <div style=\"margin-top:10px;\">\n",
    "      <p><b>Marker Size:</b> Proportional to earthquake magnitude</p>\n",
    "    </div>\n",
    "    \n",
    "    <div style=\"margin-top:10px;\">\n",
    "      <p><b>Fault Line Importance:</b></p>\n",
    "      <p><span style=\"color:#FF0000;\">━━━</span> High (4+)</p>\n",
    "      <p><span style=\"color:#FFA500;\">━━━</span> Medium (3)</p>\n",
    "      <p><span style=\"color:#FFFF00;\">━━━</span> Low (<3)</p>\n",
    "    </div>\n",
    "</div>\n",
    "'''\n",
    "dbscan_map.get_root().html.add_child(folium.Element(dbscan_legend_html))\n",
    "\n",
    "# Save the map\n",
    "dbscan_map.save('maps/dbscan_clusters_map.html')\n",
    "print(\"DBSCAN cluster map saved as 'maps/dbscan_clusters_map.html'\")\n",
    "\n",
    "# Also create a Plotly version for the notebook\n",
    "fig_dbscan = px.scatter_mapbox(\n",
    "    earthquake_df.sample(min(3000, len(earthquake_df))), \n",
    "    lat='Latitude',  # Correct parameter for latitude\n",
    "    lon='Longitude',  # Correct parameter for longitude\n",
    "    color='DBSCAN_Cluster',\n",
    "    size='Magnitude',\n",
    "    color_continuous_scale=px.colors.qualitative.Dark24,\n",
    "    size_max=15,\n",
    "    zoom=5,\n",
    "    center={\"lat\": 38.5, \"lon\": 35.5},  # Centered on Turkey\n",
    "    mapbox_style=\"open-street-map\",\n",
    "    title='DBSCAN Clusters of Turkish Earthquakes',\n",
    "    hover_data=['Depth', 'Magnitude', 'DBSCAN_Cluster']\n",
    ")\n",
    "fig_dbscan.update_layout(margin={\"r\":0,\"t\":40,\"l\":0,\"b\":0})\n",
    "fig_dbscan.write_html('maps/dbscan_clusters_map_plotly.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86609c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create density heatmap with Plotly\n",
    "fig_density = px.density_mapbox(\n",
    "    earthquake_df, \n",
    "    lat='Latitude',  # Correct parameter for latitude\n",
    "    lon='Longitude',  # Correct parameter for longitude\n",
    "    z='Magnitude',\n",
    "    radius=10,\n",
    "    zoom=5, \n",
    "    center={\"lat\": 38.5, \"lon\": 35.5},  # Centered on Turkey\n",
    "    mapbox_style=\"open-street-map\",\n",
    "    title='Earthquake Density Heatmap'\n",
    ")\n",
    "fig_density.update_layout(margin={\"r\":0,\"t\":40,\"l\":0,\"b\":0})\n",
    "fig_density.write_html('maps/earthquake_density_map.html')\n",
    "\n",
    "# Analyze DBSCAN cluster characteristics\n",
    "dbscan_analysis = earthquake_df[earthquake_df['DBSCAN_Cluster'] != -1].groupby('DBSCAN_Cluster').agg({\n",
    "    'Longitude': 'mean',\n",
    "    'Latitude': 'mean',\n",
    "    'Depth': 'mean',\n",
    "    'Magnitude': 'mean',\n",
    "    'DBSCAN_Cluster': 'count'\n",
    "}).rename(columns={'DBSCAN_Cluster': 'Count'})\n",
    "\n",
    "# Add fault distance stats if available\n",
    "if 'distance_to_fault' in earthquake_df.columns:\n",
    "    fault_stats_dbscan = earthquake_df[earthquake_df['DBSCAN_Cluster'] != -1].groupby('DBSCAN_Cluster').agg({\n",
    "        'distance_to_fault': ['mean', 'min'],\n",
    "        'nearest_fault_importance': 'mean'\n",
    "    })\n",
    "    \n",
    "    # Flatten MultiIndex columns\n",
    "    fault_stats_dbscan.columns = ['_'.join(col).strip() for col in fault_stats_dbscan.columns.values]\n",
    "    dbscan_analysis = pd.concat([dbscan_analysis, fault_stats_dbscan], axis=1)\n",
    "\n",
    "print(\"DBSCAN cluster characteristics (excluding noise):\")\n",
    "print(dbscan_analysis)\n",
    "\n",
    "# Noise point characteristics\n",
    "noise_points = earthquake_df[earthquake_df['DBSCAN_Cluster'] == -1]\n",
    "print(\"\\nNoise point characteristics:\")\n",
    "print(noise_points[['Longitude', 'Latitude', 'Depth', 'Magnitude']].describe())\n",
    "\n",
    "# Calculate average distance to fault for noise points if available\n",
    "if 'distance_to_fault' in earthquake_df.columns:\n",
    "    print(\"\\nNoise points fault distance statistics:\")\n",
    "    print(noise_points[['distance_to_fault', 'nearest_fault_importance']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632a6e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 3D scatter plot of DBSCAN clusters with fault information if available\n",
    "if 'distance_to_fault' in earthquake_df.columns:\n",
    "    fig = px.scatter_3d(\n",
    "        earthquake_df.sample(min(5000, len(earthquake_df))), \n",
    "        x='Longitude', \n",
    "        y='Latitude', \n",
    "        z='Depth',\n",
    "        color='DBSCAN_Cluster', \n",
    "        size='Magnitude',\n",
    "        color_continuous_scale=px.colors.qualitative.G10,\n",
    "        hover_data=['distance_to_fault', 'nearest_fault_importance'],\n",
    "        title='3D Visualization of DBSCAN Earthquake Clusters with Fault Information'\n",
    "    )\n",
    "else:\n",
    "    fig = px.scatter_3d(\n",
    "        earthquake_df.sample(min(5000, len(earthquake_df))), \n",
    "        x='Longitude', \n",
    "        y='Latitude', \n",
    "        z='Depth',\n",
    "        color='DBSCAN_Cluster', \n",
    "        size='Magnitude',\n",
    "        color_continuous_scale=px.colors.qualitative.G10,\n",
    "        title='3D Visualization of DBSCAN Earthquake Clusters'\n",
    "    )\n",
    "# Ensure proper axis orientation and labels\n",
    "fig.update_layout(scene=dict(\n",
    "    xaxis_title='Longitude',\n",
    "    yaxis_title='Latitude',\n",
    "    zaxis_title='Depth (km)',\n",
    "    zaxis=dict(autorange=\"reversed\")  # Reverse depth axis so deeper is lower\n",
    "))\n",
    "fig.write_html('maps/dbscan_clusters_3d.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e0d167",
   "metadata": {},
   "source": [
    "## 6. Dimensionality Reduction with PCA\n",
    "\n",
    "Principal Component Analysis (PCA) helps visualize high-dimensional earthquake data in a reduced 2D space, revealing patterns and relationships that might not be evident in the original feature space. This technique is particularly valuable for understanding the structure of multivariate geophysical data.\n",
    "\n",
    "Our PCA implementation and analysis includes:\n",
    "\n",
    "1. **Mathematical foundation**:\n",
    "   - **Linear transformation**: Finding orthogonal axes (principal components) that maximize variance\n",
    "   - **Eigendecomposition**: Computing eigenvectors and eigenvalues of the feature covariance matrix\n",
    "   - **Projection**: Transforming the original data onto these principal components\n",
    "\n",
    "2. **PCA application to earthquake features**:\n",
    "   - Applying PCA to our standardized feature matrix\n",
    "   - Extracting the first two principal components for visualization\n",
    "   - Calculating and interpreting the explained variance ratio for each component\n",
    "   - Assessing whether two dimensions adequately capture the data structure\n",
    "\n",
    "3. **Cluster visualization in reduced space**:\n",
    "   - Plotting both K-Means and DBSCAN cluster assignments in the PCA-reduced space\n",
    "   - Analyzing whether clusters remain well-separated in the reduced representation\n",
    "   - Identifying potential subclusters or structure within larger clusters\n",
    "\n",
    "4. **Feature contribution analysis**:\n",
    "   - Examining component loadings to understand which features contribute most to each principal component\n",
    "   - Interpreting the geological meaning of principal components\n",
    "   - For example, PC1 might represent a \"depth-magnitude axis\" if these features strongly influence it\n",
    "\n",
    "PCA reveals the underlying structure of our data by identifying the directions of maximum variance. For earthquake data, these directions often have physical interpretations - they might represent combinations of spatial, temporal, and physical characteristics that vary systematically across different tectonic regimes.\n",
    "\n",
    "By visualizing previously identified clusters in this reduced space, we can validate whether our clusters represent genuinely distinct groups and potentially discover relationships between features that weren't obvious in the original multidimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308893c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA for dimensionality reduction\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(scaled_data)\n",
    "\n",
    "# Create a DataFrame with PCA results\n",
    "pca_df = pd.DataFrame(data=pca_result, columns=['PC1', 'PC2'])\n",
    "pca_df['KMeans_Cluster'] = earthquake_df['KMeans_Cluster']\n",
    "pca_df['DBSCAN_Cluster'] = earthquake_df['DBSCAN_Cluster']\n",
    "pca_df['Magnitude'] = earthquake_df['Magnitude']\n",
    "\n",
    "# Add fault distance if available\n",
    "if 'distance_to_fault' in earthquake_df.columns:\n",
    "    pca_df['distance_to_fault'] = earthquake_df['distance_to_fault']\n",
    "    pca_df['nearest_fault_importance'] = earthquake_df['nearest_fault_importance']\n",
    "\n",
    "# Explained variance ratio\n",
    "print(\"Explained variance ratio:\")\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(f\"Total explained variance: {sum(pca.explained_variance_ratio_) * 100:.2f}%\")\n",
    "\n",
    "# Visualize PCA results with K-Means clusters\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.subplot(2, 1, 1)\n",
    "sns.scatterplot(data=pca_df, x='PC1', y='PC2', hue='KMeans_Cluster', palette='viridis', \n",
    "                size='Magnitude', sizes=(20, 200), alpha=0.6)\n",
    "plt.title('PCA of Earthquake Data with K-Means Clusters')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Visualize PCA results with DBSCAN clusters\n",
    "plt.subplot(2, 1, 2)\n",
    "sns.scatterplot(data=pca_df, x='PC1', y='PC2', hue='DBSCAN_Cluster', palette='viridis', \n",
    "                size='Magnitude', sizes=(20, 200), alpha=0.6)\n",
    "plt.title('PCA of Earthquake Data with DBSCAN Clusters')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f366c196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature loadings on principal components\n",
    "loadings = pd.DataFrame(\n",
    "    pca.components_.T, \n",
    "    columns=['PC1', 'PC2'],\n",
    "    index=clustering_features\n",
    ")\n",
    "\n",
    "print(\"\\nPCA Feature Loadings:\")\n",
    "print(loadings)\n",
    "\n",
    "# Visualize the feature loadings\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.bar(loadings.index, loadings['PC1'], alpha=0.7, label='PC1')\n",
    "plt.bar(loadings.index, loadings['PC2'], alpha=0.7, label='PC2')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Loading Strength')\n",
    "plt.title('PCA Feature Loadings')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39308bca",
   "metadata": {},
   "source": [
    "## 7. Risk Zone Identification\n",
    "\n",
    "Using our clustering results, we'll develop a comprehensive seismic risk assessment framework for Turkey, combining multiple factors into a quantitative risk classification system. This approach translates purely mathematical clusters into actionable risk information.\n",
    "\n",
    "Our risk assessment methodology includes:\n",
    "\n",
    "1. **Risk score calculation**:\n",
    "   - **Magnitude component**: Scaled earthquake magnitude (exponential weighting to reflect logarithmic energy release)\n",
    "   - **Density component**: Spatial density of seismic events, reflecting strain release frequency\n",
    "   - **Fault proximity component**: Inverse distance to nearest fault, weighted by fault importance\n",
    "   - **Composite calculation**: Weighted combination of these factors using a formula that balances their relative contributions\n",
    "   - **Mathematical formulation**: Risk = w₁(Magnitude_score) + w₂(Density_score) + w₃(Fault_proximity_score)\n",
    "\n",
    "2. **Risk zone classification**:\n",
    "   - **Tripartite categorization**: Dividing the continuous risk scores into High, Medium, and Low zones\n",
    "   - **Threshold determination**: Using statistical methods (e.g., Jenks natural breaks) to identify appropriate thresholds\n",
    "   - **Validation**: Comparing automated classification with historical high-impact earthquake locations\n",
    "\n",
    "3. **Zone analysis**:\n",
    "   - Calculating summary statistics for each risk zone (average magnitude, event frequency, etc.)\n",
    "   - Examining the geographic distribution of different risk zones\n",
    "   - Analyzing the temporal stability of high-risk zones over different time periods\n",
    "\n",
    "4. **Visualization**:\n",
    "   - Creating color-coded risk maps with clear visual differentiation between risk levels\n",
    "   - Implementing interactive elements to explore zone characteristics\n",
    "   - Overlaying population centers to highlight areas of potential human impact\n",
    "\n",
    "This risk assessment framework provides actionable information for several stakeholders:\n",
    "- Emergency management agencies prioritizing resource allocation\n",
    "- Urban planners developing building codes and land use regulations\n",
    "- Insurance companies calculating regional premium adjustments\n",
    "- Researchers focusing on high-risk zones for detailed studies\n",
    "\n",
    "The resulting risk zones represent a synthesis of our unsupervised learning results with domain knowledge about seismic hazard factors, creating a product with practical applications beyond pure data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17715318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine clustering information with magnitude to identify risk zones\n",
    "earthquake_df['Risk_Score'] = 0\n",
    "\n",
    "# Calculate risk score based on:\n",
    "# 1. Magnitude\n",
    "# 2. Cluster density\n",
    "# 3. Fault data (if available)\n",
    "\n",
    "# Scale magnitude to 0-10 range (assuming most are between 4-8)\n",
    "earthquake_df['Magnitude_Score'] = (earthquake_df['Magnitude'] - 4) * 2.5\n",
    "earthquake_df['Magnitude_Score'] = earthquake_df['Magnitude_Score'].clip(0, 10)\n",
    "\n",
    "# Calculate density of clusters (earthquakes per area)\n",
    "# First, verify we have valid coordinate ranges to avoid division by zero\n",
    "kmeans_spatial_range = earthquake_df.groupby('KMeans_Cluster').agg({\n",
    "    'Longitude': lambda x: max(x) - min(x),\n",
    "    'Latitude': lambda x: max(x) - min(x)\n",
    "})\n",
    "\n",
    "# Ensure we don't divide by zero by adding a small value\n",
    "kmeans_spatial_range = kmeans_spatial_range + 0.001  # Add small value to prevent division by zero\n",
    "\n",
    "# Now calculate density safely\n",
    "kmeans_density = earthquake_df.groupby('KMeans_Cluster').size() / kmeans_spatial_range.prod(axis=1)\n",
    "\n",
    "# Normalize density to 0-10 scale\n",
    "max_density = kmeans_density.max()\n",
    "if max_density > 0:  # Avoid division by zero\n",
    "    normalized_density = (kmeans_density / max_density) * 10\n",
    "else:\n",
    "    normalized_density = kmeans_density * 0  # All zeros if max_density is 0\n",
    "\n",
    "# Map density scores back to dataframe\n",
    "density_map = dict(zip(normalized_density.index, normalized_density.values))\n",
    "earthquake_df['Density_Score'] = earthquake_df['KMeans_Cluster'].map(density_map)\n",
    "\n",
    "# Add fault-based risk if features available\n",
    "if 'distance_to_fault' in earthquake_df.columns and 'nearest_fault_importance' in earthquake_df.columns:\n",
    "    print(\"Adding fault-based risk factors...\")\n",
    "    \n",
    "    # Inverse distance to fault (closer = higher risk)\n",
    "    earthquake_df['Fault_Distance_Score'] = 10 / (earthquake_df['distance_to_fault'] + 1)\n",
    "    \n",
    "    # Scale by fault importance\n",
    "    earthquake_df['Fault_Importance_Score'] = earthquake_df['nearest_fault_importance'] * 2\n",
    "    \n",
    "    # Calculate final risk score with fault information\n",
    "    earthquake_df['Risk_Score'] = (\n",
    "        earthquake_df['Magnitude_Score'] * 0.4 + \n",
    "        earthquake_df['Density_Score'] * 0.3 +\n",
    "        earthquake_df['Fault_Distance_Score'] * 0.2 +\n",
    "        earthquake_df['Fault_Importance_Score'] * 0.1\n",
    "    )\n",
    "else:\n",
    "    # Calculate risk score without fault data\n",
    "    earthquake_df['Risk_Score'] = (\n",
    "        earthquake_df['Magnitude_Score'] * 0.6 + \n",
    "        earthquake_df['Density_Score'] * 0.4\n",
    "    )\n",
    "\n",
    "# Classify risk zones\n",
    "earthquake_df['Risk_Zone'] = pd.cut(\n",
    "    earthquake_df['Risk_Score'], \n",
    "    bins=[0, 3, 6, 10], \n",
    "    labels=['Low', 'Medium', 'High']\n",
    ")\n",
    "\n",
    "# Display risk zone distribution\n",
    "risk_distribution = earthquake_df['Risk_Zone'].value_counts()\n",
    "print(\"Risk Zone Distribution:\")\n",
    "print(risk_distribution)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "risk_distribution.plot(kind='bar', color=['green', 'yellow', 'red'])\n",
    "plt.title('Distribution of Earthquake Risk Zones')\n",
    "plt.xlabel('Risk Zone')\n",
    "plt.ylabel('Number of Earthquakes')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9270091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a risk zone map with folium\n",
    "risk_map = folium.Map(location=[38.5, 35.5], zoom_start=6, tiles=\"cartodbpositron\")\n",
    "\n",
    "# Color mapping for risk zones\n",
    "risk_colors = {\n",
    "    'Low': 'green',\n",
    "    'Medium': 'orange',\n",
    "    'High': 'red'\n",
    "}\n",
    "\n",
    "# Color mapping for magnitude ranges\n",
    "magnitude_colors = {\n",
    "    'Minor (4-5)': '#fee5d9',\n",
    "    'Moderate (5-6)': '#fcbba1',\n",
    "    'Strong (6-7)': '#fc9272',\n",
    "    'Major (7-8)': '#fb6a4a',\n",
    "    'Great (8+)': '#de2d26'\n",
    "}\n",
    "\n",
    "# Create separate marker clusters for better performance\n",
    "marker_clusters = {\n",
    "    'Low': folium.plugins.MarkerCluster(name='Low Risk').add_to(risk_map),\n",
    "    'Medium': folium.plugins.MarkerCluster(name='Medium Risk').add_to(risk_map),\n",
    "    'High': folium.plugins.MarkerCluster(name='High Risk').add_to(risk_map)\n",
    "}\n",
    "\n",
    "# Function to get magnitude category and color\n",
    "def get_magnitude_info(magnitude):\n",
    "    if magnitude >= 8:\n",
    "        return 'Great (8+)', magnitude_colors['Great (8+)']\n",
    "    elif magnitude >= 7:\n",
    "        return 'Major (7-8)', magnitude_colors['Major (7-8)']\n",
    "    elif magnitude >= 6:\n",
    "        return 'Strong (6-7)', magnitude_colors['Strong (6-7)']\n",
    "    elif magnitude >= 5:\n",
    "        return 'Moderate (5-6)', magnitude_colors['Moderate (5-6)']\n",
    "    else:\n",
    "        return 'Minor (4-5)', magnitude_colors['Minor (4-5)']\n",
    "\n",
    "# Add markers for ALL earthquakes\n",
    "for idx, row in earthquake_df.iterrows():\n",
    "    risk_zone = row['Risk_Zone']\n",
    "    magnitude = row['Magnitude']\n",
    "    mag_category, mag_color = get_magnitude_info(magnitude)\n",
    "    \n",
    "    # Enhanced exponential radius calculation with greater emphasis on large earthquakes\n",
    "    if magnitude >= 7:\n",
    "        # Much more exponential growth for major earthquakes\n",
    "        radius = 15 + ((magnitude - 7) ** 2) * 6\n",
    "    else:\n",
    "        radius = 3 + (magnitude - 4) ** 1.5  # Exponential growth\n",
    "        \n",
    "    popup_content = f\"\"\"\n",
    "    <div style=\"font-family: Arial; min-width: 180px;\">\n",
    "        <h4 style=\"margin-bottom: 5px;\">Earthquake Details</h4>\n",
    "        <b>Magnitude:</b> {magnitude:.1f} ({mag_category})<br>\n",
    "        <b>Risk Zone:</b> {risk_zone}<br>\n",
    "        <b>Risk Score:</b> {row['Risk_Score']:.2f}<br>\n",
    "        <b>Depth:</b> {row['Depth']:.1f} km<br>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add date if available\n",
    "    if 'Date' in row:\n",
    "        popup_content += f\"<b>Date:</b> {row['Date']}<br>\"\n",
    "        \n",
    "    # Add location if available\n",
    "    if 'Location' in row:\n",
    "        popup_content += f\"<b>Location:</b> {row['Location']}<br>\"\n",
    "        \n",
    "    # Add fault information if available\n",
    "    if 'distance_to_fault' in row and 'nearest_fault_name' in row:\n",
    "        popup_content += f\"<b>Nearest Fault:</b> {row['nearest_fault_name']}<br>\"\n",
    "        popup_content += f\"<b>Distance to Fault:</b> {row['distance_to_fault']:.2f} km<br>\"\n",
    "    \n",
    "    popup_content += \"</div>\"\n",
    "    \n",
    "    folium.CircleMarker(\n",
    "        location=[row['Latitude'], row['Longitude']],\n",
    "        radius=radius,\n",
    "        color=risk_colors[risk_zone],  # Border color shows risk zone\n",
    "        weight=2,  # Thicker border\n",
    "        fill=True,\n",
    "        fill_color=mag_color,  # Fill color shows magnitude\n",
    "        fill_opacity=0.7,\n",
    "        popup=folium.Popup(popup_content, max_width=300)\n",
    "    ).add_to(marker_clusters[risk_zone])\n",
    "\n",
    "# Add fault lines to the map\n",
    "risk_map = add_faults_to_map(risk_map, fault_gdf, importance_threshold=3)\n",
    "\n",
    "# Add additional tile layers for users to choose\n",
    "folium.TileLayer('Stamen Toner', name='High Contrast').add_to(risk_map)\n",
    "folium.TileLayer('cartodbpositron', name='Light').add_to(risk_map)\n",
    "folium.TileLayer('OpenStreetMap', name='Detailed').add_to(risk_map)\n",
    "\n",
    "# Enhance risk zone map legend\n",
    "risk_legend_html = '''\n",
    "<div style=\"position: fixed; bottom: 50px; right: 50px; width: 250px; \n",
    "    background-color: white; border:2px solid grey; z-index:9999; \n",
    "    padding: 10px; border-radius: 5px; font-family: Arial;\">\n",
    "    <h4 style=\"margin-top:0;\">Earthquake Risk Zones</h4>\n",
    "    \n",
    "    <div style=\"margin-top:5px;\">\n",
    "      <p><b>Border Color:</b> Risk Zone</p>\n",
    "      <div style=\"display: flex; align-items: center; margin-bottom: 5px;\">\n",
    "        <div style=\"background-color: green; width: 15px; height: 15px; margin-right: 5px; border-radius: 50%;\"></div>\n",
    "        <span>Low Risk</span>\n",
    "      </div>\n",
    "      <div style=\"display: flex; align-items: center; margin-bottom: 5px;\">\n",
    "        <div style=\"background-color: orange; width: 15px; height: 15px; margin-right: 5px; border-radius: 50%;\"></div>\n",
    "        <span>Medium Risk</span>\n",
    "      </div>\n",
    "      <div style=\"display: flex; align-items: center; margin-bottom: 5px;\">\n",
    "        <div style=\"background-color: red; width: 15px; height: 15px; margin-right: 5px; border-radius: 50%;\"></div>\n",
    "        <span>High Risk</span>\n",
    "      </div>\n",
    "    </div>\n",
    "    \n",
    "    <div style=\"margin-top:10px;\">\n",
    "      <p><b>Fill Color:</b> Magnitude</p>\n",
    "      <div style=\"display: flex; align-items: center; margin-bottom: 5px;\">\n",
    "        <div style=\"background-color: #fee5d9; width: 15px; height: 15px; margin-right: 5px; border-radius: 50%;\"></div>\n",
    "        <span>Minor (4-5)</span>\n",
    "      </div>\n",
    "      <div style=\"display: flex; align-items: center; margin-bottom: 5px;\">\n",
    "        <div style=\"background-color: #fcbba1; width: 15px; height: 15px; margin-right: 5px; border-radius: 50%;\"></div>\n",
    "        <span>Moderate (5-6)</span>\n",
    "      </div>\n",
    "      <div style=\"display: flex; align-items: center; margin-bottom: 5px;\">\n",
    "        <div style=\"background-color: #fc9272; width: 15px; height: 15px; margin-right: 5px; border-radius: 50%;\"></div>\n",
    "        <span>Strong (6-7)</span>\n",
    "      </div>\n",
    "      <div style=\"display: flex; align-items: center; margin-bottom: 5px;\">\n",
    "        <div style=\"background-color: #fb6a4a; width: 15px; height: 15px; margin-right: 5px; border-radius: 50%;\"></div>\n",
    "        <span>Major (7-8)</span>\n",
    "      </div>\n",
    "      <div style=\"display: flex; align-items: center; margin-bottom: 5px;\">\n",
    "        <div style=\"background-color: #de2d26; width: 15px; height: 15px; margin-right: 5px; border-radius: 50%;\"></div>\n",
    "        <span>Great (8+)</span>\n",
    "      </div>\n",
    "    </div>\n",
    "    \n",
    "    <div style=\"margin-top:5px;\">\n",
    "      <p><b>Marker Size:</b> Proportional to magnitude</p>\n",
    "    </div>\n",
    "</div>\n",
    "'''\n",
    "risk_map.get_root().html.add_child(folium.Element(risk_legend_html))\n",
    "\n",
    "# Add layer control\n",
    "folium.LayerControl().add_to(risk_map)\n",
    "\n",
    "# Save risk map to HTML file\n",
    "risk_map.save('maps/earthquake_risk_map.html')\n",
    "print(\"Enhanced earthquake risk map saved as 'maps/earthquake_risk_map.html'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548f0451",
   "metadata": {},
   "source": [
    "## 8. Risk Assessment for Population Centers\n",
    "\n",
    "Converting our technical risk analysis into human impact assessment requires integrating seismic patterns with population data. This section evaluates earthquake risk for major Turkish cities, providing practical information for urban planning and emergency preparedness.\n",
    "\n",
    "Our population-centric risk assessment includes:\n",
    "\n",
    "1. **City-specific risk metric development**:\n",
    "   - **Proximity weighting**: Calculating distance-weighted earthquake frequency for each city\n",
    "   - **Magnitude amplification**: Applying exponential weighting for larger events (reflecting that M7 events cause exponentially more damage than M6)\n",
    "   - **Population vulnerability factor**: Scaling risk by population size and density\n",
    "   - **Mathematical formulation**: City_Risk = Σ[Magnitude^a × (1/Distance^b) × Population_Factor]\n",
    "\n",
    "2. **Risk ranking methodology**:\n",
    "   - Calculating comparative risk scores for major Turkish cities (population >100,000)\n",
    "   - Ranking cities by combined seismic risk and potential human impact\n",
    "   - Categorizing cities into risk tiers (High, Medium, Low)\n",
    "   - Validating rankings against historical earthquake damage records\n",
    "   - Analyzing proximity to multiple fault systems   \n",
    "\n",
    "5. **Visualization and communication**:\n",
    "   - Creating interactive maps showing city risk levels\n",
    "   - Implementing population-scaled markers to visualize both risk and potential impact\n",
    "   - Developing clear summary charts ranking cities by risk score\n",
    "   - Creating visualizations that communicate risk effectively to non-specialists\n",
    "\n",
    "This analysis addresses critical questions for urban planning and disaster preparedness:\n",
    "- Which major population centers face the highest earthquake risk?\n",
    "- How does population size amplify the potential impact of seismic events?\n",
    "- Where should emergency response resources be prioritized?\n",
    "- Which cities might benefit most from enhanced building codes or retrofit programs?\n",
    "\n",
    "By translating pure seismic analysis into population impact assessment, we provide actionable information that connects earth science with human geography and public policy considerations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49578370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of major Turkish cities for risk assessment\n",
    "cities = [\n",
    "    {\"name\": \"Istanbul\", \"lat\": 41.0082, \"lon\": 28.9784, \"population\": 15655924},\n",
    "    {\"name\": \"Ankara\", \"lat\": 39.9334, \"lon\": 32.8597, \"population\": 5803482},\n",
    "    {\"name\": \"Izmir\", \"lat\": 38.4237, \"lon\": 27.1428, \"population\": 4479525},\n",
    "    {\"name\": \"Bursa\", \"lat\": 40.1885, \"lon\": 29.0610, \"population\": 3214571},\n",
    "    {\"name\": \"Antalya\", \"lat\": 36.8969, \"lon\": 30.7133, \"population\": 2696249},\n",
    "    {\"name\": \"Adana\", \"lat\": 37.0000, \"lon\": 35.3213, \"population\": 2270298},\n",
    "    {\"name\": \"Konya\", \"lat\": 37.8667, \"lon\": 32.4833, \"population\": 2320241},\n",
    "    {\"name\": \"Gaziantep\", \"lat\": 37.0662, \"lon\": 37.3833, \"population\": 2164134},\n",
    "    {\"name\": \"Şanlıurfa\", \"lat\": 37.1591, \"lon\": 38.7969, \"population\": 2213964},\n",
    "    {\"name\": \"Mersin\", \"lat\": 36.8000, \"lon\": 34.6333, \"population\": 1938389},\n",
    "    {\"name\": \"Diyarbakır\", \"lat\": 37.9144, \"lon\": 40.2306, \"population\": 1818133},\n",
    "    {\"name\": \"Hatay\", \"lat\": 36.2025, \"lon\": 36.1606, \"population\": 1544640},\n",
    "    {\"name\": \"Manisa\", \"lat\": 38.6191, \"lon\": 27.4289, \"population\": 1475716},\n",
    "    {\"name\": \"Kayseri\", \"lat\": 38.7205, \"lon\": 35.4826, \"population\": 1445683},\n",
    "    {\"name\": \"Samsun\", \"lat\": 41.2867, \"lon\": 36.3300, \"population\": 1377546},\n",
    "    {\"name\": \"Balıkesir\", \"lat\": 39.6484, \"lon\": 27.8826, \"population\": 1273519},\n",
    "    {\"name\": \"Kahramanmaraş\", \"lat\": 37.5750, \"lon\": 36.9228, \"population\": 1116618},\n",
    "    {\"name\": \"Van\", \"lat\": 38.4942, \"lon\": 43.3800, \"population\": 1127612},\n",
    "    {\"name\": \"Aydın\", \"lat\": 37.8444, \"lon\": 27.8458, \"population\": 1161702},\n",
    "    {\"name\": \"Denizli\", \"lat\": 37.7765, \"lon\": 29.0864, \"population\": 1061043},\n",
    "    {\"name\": \"Sakarya\", \"lat\": 40.7731, \"lon\": 30.3943, \"population\": 1100747},\n",
    "    {\"name\": \"Tekirdağ\", \"lat\": 40.9833, \"lon\": 27.5167, \"population\": 1167059},\n",
    "    {\"name\": \"Muğla\", \"lat\": 37.2153, \"lon\": 28.3636, \"population\": 1066736},\n",
    "    {\"name\": \"Eskişehir\", \"lat\": 39.7767, \"lon\": 30.5206, \"population\": 915418},\n",
    "    {\"name\": \"Mardin\", \"lat\": 37.3212, \"lon\": 40.7245, \"population\": 888874},\n",
    "    {\"name\": \"Malatya\", \"lat\": 38.3552, \"lon\": 38.3095, \"population\": 742725},\n",
    "    {\"name\": \"Trabzon\", \"lat\": 41.0053, \"lon\": 39.7267, \"population\": 824352},\n",
    "    {\"name\": \"Erzurum\", \"lat\": 39.9042, \"lon\": 41.2705, \"population\": 749993},\n",
    "    {\"name\": \"Ordu\", \"lat\": 40.9839, \"lon\": 37.8764, \"population\": 775800},\n",
    "    {\"name\": \"Afyonkarahisar\", \"lat\": 38.7587, \"lon\": 30.5387, \"population\": 751344},\n",
    "    {\"name\": \"Çanakkale\", \"lat\": 40.1553, \"lon\": 26.4142, \"population\": 570499},\n",
    "    {\"name\": \"Düzce\", \"lat\": 40.8438, \"lon\": 31.1565, \"population\": 409865},\n",
    "    {\"name\": \"Bingöl\", \"lat\": 38.8855, \"lon\": 40.4983, \"population\": 285655},\n",
    "    {\"name\": \"Tokat\", \"lat\": 40.3167, \"lon\": 36.5500, \"population\": 606934},\n",
    "    {\"name\": \"Kütahya\", \"lat\": 39.4167, \"lon\": 29.9833, \"population\": 575671},\n",
    "    {\"name\": \"Batman\", \"lat\": 37.8812, \"lon\": 41.1351, \"population\": 647205},\n",
    "    {\"name\": \"Elazığ\", \"lat\": 38.6748, \"lon\": 39.2225, \"population\": 604411},\n",
    "    {\"name\": \"Çorum\", \"lat\": 40.5489, \"lon\": 34.9533, \"population\": 528351},\n",
    "    {\"name\": \"Biga (Çanakkale)(Memleket <3)\", \"lat\": 40.2322, \"lon\": 27.2464, \"population\": 87000},\n",
    "    {\"name\": \"Simav (Kütahya)\", \"lat\": 39.0922, \"lon\": 28.9789, \"population\": 64000},\n",
    "    {\"name\": \"Göksun (Kahramanmaraş)\", \"lat\": 38.0203, \"lon\": 36.4825, \"population\": 31000},\n",
    "    {\"name\": \"Kastamonu\", \"lat\": 41.3887, \"lon\": 33.7827, \"population\": 383000},\n",
    "    {\"name\": \"Burdur\", \"lat\": 37.7215, \"lon\": 30.2886, \"population\": 270000},\n",
    "    {\"name\": \"Kars\", \"lat\": 40.6013, \"lon\": 43.0950, \"population\": 289000},\n",
    "    {\"name\": \"Adıyaman\", \"lat\": 37.7636, \"lon\": 38.2773, \"population\": 632000},\n",
    "    {\"name\": \"Çankırı\", \"lat\": 40.6013, \"lon\": 33.6134, \"population\": 195000},\n",
    "    {\"name\": \"Edirne\", \"lat\": 41.6771, \"lon\": 26.5557, \"population\": 411000},\n",
    "    {\"name\": \"Bartın\", \"lat\": 41.6358, \"lon\": 32.3375, \"population\": 198000},\n",
    "    {\"name\": \"Erzincan\", \"lat\": 39.75, \"lon\": 39.49, \"population\": 234000},\n",
    "    {\"name\": \"Hakkari\", \"lat\": 37.57, \"lon\": 43.74, \"population\": 267000},\n",
    "    {\"name\": \"Osmaniye\", \"lat\": 37.0746, \"lon\": 36.2464, \"population\": 534000}\n",
    "]\n",
    "\n",
    "# Function to calculate earthquake risk for a city\n",
    "def calculate_city_risk(city, earthquake_df, radius_km=50):\n",
    "    # Haversine formula to calculate distance\n",
    "    def haversine(lon1, lat1, lon2, lat2):\n",
    "        # Convert decimal degrees to radians\n",
    "        lon1, lat1, lon2, lat2 = map(math.radians, [lon1, lat1, lon2, lat2])\n",
    "        \n",
    "        # Haversine formula\n",
    "        dlon = lon2 - lon1\n",
    "        dlat = lat2 - lat1\n",
    "        a = math.sin(dlat/2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon/2)**2\n",
    "        c = 2 * math.asin(math.sqrt(a))\n",
    "        r = 6371  # Radius of earth in km\n",
    "        return c * r\n",
    "    \n",
    "    # Find earthquakes within the radius\n",
    "    nearby_earthquakes = []\n",
    "    for idx, quake in earthquake_df.iterrows():\n",
    "        distance = haversine(city[\"lon\"], city[\"lat\"], quake[\"Longitude\"], quake[\"Latitude\"])\n",
    "        if distance <= radius_km:\n",
    "            nearby_earthquakes.append({\n",
    "                \"distance\": distance,\n",
    "                \"magnitude\": quake[\"Magnitude\"],\n",
    "                \"risk_score\": quake[\"Risk_Score\"] if \"Risk_Score\" in quake else 0\n",
    "            })\n",
    "    \n",
    "    # If no earthquakes found, return minimal risk\n",
    "    if not nearby_earthquakes:\n",
    "        return {\n",
    "            \"city\": city[\"name\"],\n",
    "            \"population\": city[\"population\"],\n",
    "            \"earthquake_count\": 0,\n",
    "            \"avg_magnitude\": 0,\n",
    "            \"max_magnitude\": 0,\n",
    "            \"avg_risk_score\": 0,\n",
    "            \"weighted_risk\": 0,\n",
    "            \"risk_category\": \"Unknown\"\n",
    "        }\n",
    "    \n",
    "    # Calculate risk metrics\n",
    "    earthquake_count = len(nearby_earthquakes)\n",
    "    avg_magnitude = sum(q[\"magnitude\"] for q in nearby_earthquakes) / earthquake_count\n",
    "    max_magnitude = max(q[\"magnitude\"] for q in nearby_earthquakes)\n",
    "    \n",
    "    # Calculate average risk score\n",
    "    avg_risk_score = sum(q[\"risk_score\"] for q in nearby_earthquakes) / earthquake_count\n",
    "    \n",
    "    # Weight by magnitude and distance\n",
    "    weighted_risks = []\n",
    "    for quake in nearby_earthquakes:\n",
    "        # Higher magnitude and closer distance = higher risk\n",
    "        weight = quake[\"magnitude\"] * (1 / (quake[\"distance\"] + 1))\n",
    "        weighted_risks.append(weight)\n",
    "    \n",
    "    weighted_risk = sum(weighted_risks) * (city[\"population\"] / 1000000)  # Scale by population in millions\n",
    "    \n",
    "    # Determine risk category\n",
    "    risk_category = \"Low\"\n",
    "    if weighted_risk > 50:\n",
    "        risk_category = \"High\"\n",
    "    elif weighted_risk > 20:\n",
    "        risk_category = \"Medium\"\n",
    "    \n",
    "    return {\n",
    "        \"city\": city[\"name\"],\n",
    "        \"population\": city[\"population\"],\n",
    "        \"earthquake_count\": earthquake_count,\n",
    "        \"avg_magnitude\": avg_magnitude,\n",
    "        \"max_magnitude\": max_magnitude,\n",
    "        \"avg_risk_score\": avg_risk_score,\n",
    "        \"weighted_risk\": weighted_risk,\n",
    "        \"risk_category\": risk_category\n",
    "    }\n",
    "\n",
    "# Calculate risk for each city\n",
    "city_risks = []\n",
    "for city in cities:\n",
    "    risk = calculate_city_risk(city, earthquake_df)\n",
    "    city_risks.append(risk)\n",
    "\n",
    "# Create DataFrame for city risks\n",
    "city_risk_df = pd.DataFrame(city_risks)\n",
    "city_risk_df = city_risk_df.sort_values(\"weighted_risk\", ascending=False)\n",
    "\n",
    "print(\"City Earthquake Risk Assessment:\")\n",
    "print(city_risk_df[[\"city\", \"population\", \"earthquake_count\", \"avg_magnitude\", \"weighted_risk\", \"risk_category\"]])\n",
    "\n",
    "# Visualize city risks\n",
    "plt.figure(figsize=(14, 8))\n",
    "bars = plt.bar(city_risk_df[\"city\"], city_risk_df[\"weighted_risk\"])\n",
    "\n",
    "# Color bars by risk category\n",
    "for i, bar in enumerate(bars):\n",
    "    if city_risk_df.iloc[i][\"risk_category\"] == \"High\":\n",
    "        bar.set_color(\"red\")\n",
    "    elif city_risk_df.iloc[i][\"risk_category\"] == \"Medium\":\n",
    "        bar.set_color(\"orange\")\n",
    "    else:\n",
    "        bar.set_color(\"green\")\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel(\"City\")\n",
    "plt.ylabel(\"Weighted Risk Score\")\n",
    "plt.title(\"Earthquake Risk Assessment for Major Turkish Cities\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea00cf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a city risk map\n",
    "city_risk_map = folium.Map(location=[38.5, 35.5], zoom_start=6)\n",
    "\n",
    "# Add the fault lines\n",
    "city_risk_map = add_faults_to_map(city_risk_map, fault_gdf, importance_threshold=3)\n",
    "\n",
    "# Add city markers with risk color\n",
    "for _, city_data in city_risk_df.iterrows():\n",
    "    # Find the city's coordinates\n",
    "    city_info = next((c for c in cities if c[\"name\"] == city_data[\"city\"]), None)\n",
    "    if not city_info:\n",
    "        continue\n",
    "    \n",
    "    # Color based on risk category\n",
    "    if city_data[\"risk_category\"] == \"High\":\n",
    "        color = \"red\"\n",
    "    elif city_data[\"risk_category\"] == \"Medium\":\n",
    "        color = \"orange\"\n",
    "    else:\n",
    "        color = \"green\"\n",
    "    \n",
    "    # Size based on both population and risk score\n",
    "    # Use weighted_risk directly as it already factors in population\n",
    "    radius = 5 + (city_data[\"weighted_risk\"] / 10)  # Base size + proportional to risk\n",
    "    \n",
    "    # Create enhanced popup with styled HTML\n",
    "    popup_content = f\"\"\"\n",
    "    <div style=\"font-family: Arial; min-width: 220px;\">\n",
    "        <h4 style=\"margin-bottom: 5px; color: #1976d2; border-bottom: 1px solid #ddd; padding-bottom: 5px;\">\n",
    "            {city_data['city']}\n",
    "        </h4>\n",
    "        <div style=\"margin-top: 8px;\">\n",
    "            <b>Population:</b> {city_data['population']:,}<br>\n",
    "            <b>Risk Category:</b> <span style=\"color: {color}; font-weight: bold;\">{city_data['risk_category']}</span><br>\n",
    "            <b>Weighted Risk Score:</b> {city_data['weighted_risk']:.2f}<br>\n",
    "            <hr style=\"margin: 8px 0; border-top: 1px dotted #ddd;\">\n",
    "            <b>Earthquake Statistics:</b><br>\n",
    "            <span style=\"margin-left: 10px;\">Nearby Earthquakes: {city_data['earthquake_count']}</span><br>\n",
    "            <span style=\"margin-left: 10px;\">Average Magnitude: {city_data['avg_magnitude']:.2f}</span><br>\n",
    "            <span style=\"margin-left: 10px;\">Maximum Magnitude: {city_data['max_magnitude']:.2f}</span><br>\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create marker\n",
    "    folium.CircleMarker(\n",
    "        location=[city_info[\"lat\"], city_info[\"lon\"]],\n",
    "        radius=radius,\n",
    "        color=color,\n",
    "        fill=True,\n",
    "        fill_color=color,\n",
    "        fill_opacity=0.7,\n",
    "        popup=folium.Popup(popup_content, max_width=300)\n",
    "    ).add_to(city_risk_map)\n",
    "\n",
    "# Add earthquake heat layer for context\n",
    "heat_data = [[row['Latitude'], row['Longitude']] for _, row in earthquake_df.sample(min(3000, len(earthquake_df))).iterrows()]\n",
    "HeatMap(heat_data, radius=10, blur=15, gradient={'0.4': 'blue', '0.6': 'cyan', '0.8': 'yellow', '1.0': 'red'}, name=\"Earthquake Density\").add_to(city_risk_map)\n",
    "\n",
    "# Add tile layer and controls\n",
    "folium.TileLayer('cartodbpositron').add_to(city_risk_map)\n",
    "folium.LayerControl().add_to(city_risk_map)\n",
    "\n",
    "# Save the map\n",
    "city_risk_map.save('maps/city_earthquake_risk_map.html')\n",
    "print(\"City risk map saved as 'maps/city_earthquake_risk_map.html'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde563db",
   "metadata": {},
   "source": [
    "## 9. Conclusion and Findings\n",
    "\n",
    "Our unsupervised learning analysis has revealed natural patterns in Turkey's seismic landscape and translated these patterns into actionable risk information. This section summarizes key findings and their implications for earthquake hazard understanding.\n",
    "\n",
    "### Key analytical outcomes:\n",
    "\n",
    "1. **Clustering insights**:\n",
    "   - K-Means identified {number} distinct earthquake zones across Turkey, largely corresponding to major tectonic provinces\n",
    "   - DBSCAN revealed finer-grained patterns, including linear clusters along fault segments and isolated high-activity zones\n",
    "   - Noise points identified by DBSCAN may represent either truly anomalous events or regions of diffuse seismicity\n",
    "\n",
    "2. **Fault-earthquake relationships**:\n",
    "   - Clear correlation between cluster density and proximity to major fault systems\n",
    "   - Statistical significance of distance-to-fault as a clustering factor\n",
    "   - Identification of fault segments with particularly high seismic activity\n",
    "\n",
    "3. **Risk zone delineation**:\n",
    "   - Successfully classified regions into three risk categories based on multiple seismic indicators\n",
    "   - Validation of high-risk zones against historical high-impact earthquake locations\n",
    "   - Quantification of relative risk levels across different regions\n",
    "\n",
    "4. **Urban vulnerability assessment**:\n",
    "   - Ranked major Turkish cities by combined seismic hazard and population exposure\n",
    "   - Identified urban areas where risk mitigation should be prioritized\n",
    "   - Provided comparative risk metrics for emergency planning purposes\n",
    "\n",
    "### Implications and applications:\n",
    "\n",
    "- **Scientific understanding**: Our unsupervised analysis supplements traditional seismological approaches by objectively identifying patterns without prior assumptions\n",
    "- **Hazard assessment**: The risk zones provide a data-driven foundation for regional hazard mapping\n",
    "- **Emergency planning**: City risk rankings offer prioritization guidance for resource allocation\n",
    "- **Research direction**: Identified high-risk zones warrant focused geological and geophysical investigation\n",
    "\n",
    "### Methodological advancements:\n",
    "\n",
    "- Successfully combined multiple clustering approaches to capture different aspects of seismic patterns\n",
    "- Developed an integrated risk scoring framework combining magnitude, density, and fault proximity\n",
    "- Created population-weighted risk metrics that translate geophysical hazard into potential human impact\n",
    "\n",
    "### Limitations and future work:\n",
    "\n",
    "- Earthquake catalog completeness varies across regions and time periods\n",
    "- Fault mapping may be incomplete, particularly for smaller structures\n",
    "- Temporal evolution of risk patterns remains to be fully explored\n",
    "- Further integration with geophysical models would enhance risk assessment\n",
    "\n",
    "This unsupervised analysis complements our supervised magnitude predictions by providing a broader context of regional patterns and risk zones, demonstrating how different machine learning approaches can be combined to develop a more comprehensive understanding of complex geophysical phenomena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60dc353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the clustering results\n",
    "earthquake_df.to_csv('produced_data/earthquake_clusters.csv', index=False)\n",
    "\n",
    "# Summarize cluster and risk zone characteristics\n",
    "cluster_risk_summary = earthquake_df.groupby(['KMeans_Cluster', 'Risk_Zone']).agg({\n",
    "    'Magnitude': ['mean', 'min', 'max', 'count'],\n",
    "    'Depth': ['mean', 'min', 'max'],\n",
    "    'Risk_Score': ['mean', 'min', 'max']\n",
    "})\n",
    "\n",
    "# Add fault information to summary if available\n",
    "if 'distance_to_fault' in earthquake_df.columns:\n",
    "    fault_summary = earthquake_df.groupby(['KMeans_Cluster', 'Risk_Zone']).agg({\n",
    "        'distance_to_fault': ['mean', 'min'],\n",
    "        'nearest_fault_importance': ['mean', 'max']\n",
    "    })\n",
    "    \n",
    "    # Combine summaries\n",
    "    cluster_risk_summary = pd.concat([cluster_risk_summary, fault_summary], axis=1)\n",
    "\n",
    "print(\"Cluster and Risk Zone Summary:\")\n",
    "print(cluster_risk_summary)\n",
    "\n",
    "# Final visualization: Risk zones by longitude/latitude\n",
    "plt.figure(figsize=(12, 8))\n",
    "scatter = plt.scatter(\n",
    "    earthquake_df['Longitude'],  # X-axis is Longitude\n",
    "    earthquake_df['Latitude'],   # Y-axis is Latitude\n",
    "    c=earthquake_df['Risk_Score'],\n",
    "    cmap='RdYlGn_r',  # Red for high risk, green for low risk\n",
    "    alpha=0.7,\n",
    "    s=20\n",
    ")\n",
    "plt.colorbar(scatter, label='Risk Score')\n",
    "plt.title('Earthquake Risk Zones in Turkey')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add fault lines to plot if available\n",
    "if 'fault_gdf' in locals():\n",
    "    for _, fault in fault_gdf.iterrows():\n",
    "        xs, ys = fault.geometry.xy\n",
    "        importance = fault.get('importance', 1)\n",
    "        color = 'red' if importance >= 4 else 'orange' if importance >= 3 else 'yellow'\n",
    "        plt.plot(xs, ys, color=color, linewidth=importance*0.3, alpha=0.6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('maps/earthquake_risk_zones.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"Unsupervised learning analysis completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f650a054",
   "metadata": {},
   "source": [
    "## GPU-Accelerated Clustering (Bonus)\n",
    "\n",
    "This section implements clustering algorithms using GPU acceleration through PyTorch, demonstrating the performance advantages of parallel computing for unsupervised learning tasks. We'll compare execution times and results between CPU and GPU implementations.\n",
    "\n",
    "### GPU acceleration principles:\n",
    "\n",
    "1. **Parallelization benefits for clustering**:\n",
    "   - **Distance calculations**: Computing pairwise distances between points is highly parallelizable\n",
    "   - **Centroid updates**: Aggregating points to update centroids can leverage parallel reduction operations\n",
    "   - **Matrix operations**: Core linear algebra operations benefit from GPU's SIMD (Single Instruction, Multiple Data) architecture\n",
    "\n",
    "2. **PyTorch implementation**:\n",
    "   - Custom implementation of K-Means using PyTorch tensors and CUDA operations\n",
    "   - Utilizing GPU-optimized distance metrics (torch.cdist)\n",
    "   - Leveraging automatic differentiation capabilities for gradient-based optimizations\n",
    "   - Implementing parallelized assignment and update steps\n",
    "\n",
    "3. **Performance comparison methodology**:\n",
    "   - Benchmarking identical algorithms on CPU vs. GPU\n",
    "   - Measuring wall-clock execution time for both implementations\n",
    "   - Assessing scalability with increasing dataset size\n",
    "   - Evaluating result consistency between implementations\n",
    "\n",
    "4. **Advantages and limitations**:\n",
    "   - **Speedup potential**: Quantifying actual performance improvement for our dataset\n",
    "   - **Data transfer overhead**: Considering the cost of moving data between CPU and GPU memory\n",
    "   - **Result consistency**: Verifying that both implementations produce equivalent clustering results\n",
    "   - **Scalability**: Analyzing how performance advantage scales with problem size\n",
    "\n",
    "The implementation demonstrates how modern deep learning frameworks can accelerate traditional machine learning algorithms through GPU parallelization. For large geospatial datasets like earthquake catalogs, this acceleration can transform what would be prohibitively expensive analyses into practical workflows, enabling more comprehensive exploration of parameter spaces and larger datasets.\n",
    "\n",
    "This bonus section fulfills the project requirement to utilize GPU libraries while providing tangible benefits for computational efficiency in our earthquake pattern analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf10c7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for GPU availability\n",
    "import torch\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"No GPU available, using CPU instead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd64d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch GPU-accelerated K-Means implementation\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "class KMeansGPU:\n",
    "    def __init__(self, n_clusters=5, max_iters=100, tol=1e-4):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.max_iters = max_iters\n",
    "        self.tol = tol\n",
    "        self.centroids = None\n",
    "        self.labels_ = None\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    def fit(self, X):\n",
    "        # Convert numpy array to PyTorch tensor\n",
    "        if isinstance(X, np.ndarray):\n",
    "            X_tensor = torch.FloatTensor(X).to(self.device)\n",
    "        else:\n",
    "            X_tensor = X.to(self.device)\n",
    "        \n",
    "        n_samples, n_features = X_tensor.shape\n",
    "        \n",
    "        # Initialize centroids randomly from data points\n",
    "        idx = torch.randperm(n_samples)[:self.n_clusters]\n",
    "        self.centroids = X_tensor[idx].clone()\n",
    "        \n",
    "        # Previous centroids used for convergence check\n",
    "        prev_centroids = torch.zeros_like(self.centroids)\n",
    "        \n",
    "        # Labels for each data point\n",
    "        self.labels_ = torch.zeros(n_samples, dtype=torch.long).to(self.device)\n",
    "        \n",
    "        # Main K-means loop\n",
    "        for iteration in range(self.max_iters):\n",
    "            # Compute distances between data points and centroids\n",
    "            distances = torch.cdist(X_tensor, self.centroids)\n",
    "            \n",
    "            # Assign each data point to the nearest centroid\n",
    "            self.labels_ = torch.argmin(distances, dim=1)\n",
    "            \n",
    "            # Save previous centroids for convergence check\n",
    "            prev_centroids = self.centroids.clone()\n",
    "            \n",
    "            # Update centroids\n",
    "            for i in range(self.n_clusters):\n",
    "                # Get points in this cluster\n",
    "                mask = self.labels_ == i\n",
    "                if torch.sum(mask) > 0:  # Avoid empty clusters\n",
    "                    self.centroids[i] = torch.mean(X_tensor[mask], dim=0)\n",
    "            \n",
    "            # Check for convergence\n",
    "            centroid_change = torch.sum(torch.sqrt(torch.sum((self.centroids - prev_centroids) ** 2, dim=1)))\n",
    "            if centroid_change < self.tol:\n",
    "                print(f\"Converged at iteration {iteration}\")\n",
    "                break\n",
    "        \n",
    "        # Convert labels to numpy array\n",
    "        self.labels_ = self.labels_.cpu().numpy()\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Convert numpy array to PyTorch tensor\n",
    "        if isinstance(X, np.ndarray):\n",
    "            X_tensor = torch.FloatTensor(X).to(self.device)\n",
    "        else:\n",
    "            X_tensor = X.to(self.device)\n",
    "        \n",
    "        # Compute distances\n",
    "        distances = torch.cdist(X_tensor, self.centroids)\n",
    "        \n",
    "        # Assign to nearest centroid\n",
    "        labels = torch.argmin(distances, dim=1).cpu().numpy()\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6181f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare CPU vs GPU K-Means clustering performance\n",
    "print(\"Comparing CPU vs GPU K-Means performance...\")\n",
    "\n",
    "# Create a copy of the scaled data as a numpy array\n",
    "scaled_data_np = scaled_data.copy()\n",
    "\n",
    "# Convert data to PyTorch tensor\n",
    "X_tensor = torch.FloatTensor(scaled_data_np)\n",
    "\n",
    "# Optimal k from previous analysis\n",
    "k = optimal_k  # This was determined earlier in the notebook\n",
    "\n",
    "# CPU timing (sklearn implementation)\n",
    "start_time = time.time()\n",
    "kmeans_cpu = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "kmeans_cpu.fit(scaled_data_np)\n",
    "cpu_time = time.time() - start_time\n",
    "print(f\"CPU K-Means time: {cpu_time:.2f} seconds\")\n",
    "\n",
    "# GPU timing (PyTorch implementation)\n",
    "start_time = time.time()\n",
    "kmeans_gpu = KMeansGPU(n_clusters=k, max_iters=100)\n",
    "kmeans_gpu.fit(X_tensor)\n",
    "gpu_time = time.time() - start_time\n",
    "print(f\"GPU K-Means time: {gpu_time:.2f} seconds\")\n",
    "\n",
    "# Calculate speedup\n",
    "speedup = cpu_time / gpu_time\n",
    "print(f\"GPU speedup: {speedup:.2f}x faster\")\n",
    "\n",
    "# Verify silhouette scores\n",
    "cpu_silhouette = silhouette_score(scaled_data_np, kmeans_cpu.labels_)\n",
    "gpu_silhouette = silhouette_score(scaled_data_np, kmeans_gpu.labels_)\n",
    "\n",
    "print(f\"CPU K-Means silhouette score: {cpu_silhouette:.3f}\")\n",
    "print(f\"GPU K-Means silhouette score: {gpu_silhouette:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79f7a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize performance comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "bar_width = 0.35\n",
    "index = np.arange(1)\n",
    "\n",
    "plt.bar(index, [cpu_time], bar_width, label='CPU (scikit-learn)', color='blue', alpha=0.7)\n",
    "plt.bar(index + bar_width, [gpu_time], bar_width, label='GPU (PyTorch)', color='orange', alpha=0.7)\n",
    "\n",
    "plt.ylabel('Time (seconds)')\n",
    "plt.title('K-Means Clustering Performance: CPU vs GPU')\n",
    "plt.xticks(index + bar_width/2, ['K-Means'])\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd5cb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the GPU clustered data to the original dataframe for comparison\n",
    "earthquake_df['KMeans_GPU_Cluster'] = kmeans_gpu.labels_\n",
    "\n",
    "# Visualize CPU vs GPU cluster assignments\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# CPU clustering\n",
    "scatter0 = axes[0].scatter(earthquake_df['Longitude'], earthquake_df['Latitude'], \n",
    "                           c=earthquake_df['KMeans_Cluster'], cmap='viridis', \n",
    "                           alpha=0.7, s=20)\n",
    "axes[0].set_title('CPU K-Means Clustering')\n",
    "axes[0].set_xlabel('Longitude')\n",
    "axes[0].set_ylabel('Latitude')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "fig.colorbar(scatter0, ax=axes[0], label='Cluster')\n",
    "\n",
    "# GPU clustering\n",
    "scatter1 = axes[1].scatter(earthquake_df['Longitude'], earthquake_df['Latitude'], \n",
    "                           c=earthquake_df['KMeans_GPU_Cluster'], cmap='viridis', \n",
    "                           alpha=0.7, s=20)\n",
    "axes[1].set_title('GPU K-Means Clustering')\n",
    "axes[1].set_xlabel('Longitude')\n",
    "axes[1].set_ylabel('Latitude')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "fig.colorbar(scatter1, ax=axes[1], label='Cluster')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the updated data with GPU clustering results\n",
    "earthquake_df.to_csv('produced_data/earthquake_clusters_gpu.csv', index=False)\n",
    "print(\"Updated data with GPU clusters saved as 'produced_data/earthquake_clusters_gpu.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
