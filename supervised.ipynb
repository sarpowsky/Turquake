{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "708e6cfa",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Initial Exploration\n",
    "\n",
    "In this section, we'll load the Turkish earthquake dataset and perform initial exploratory analysis to understand its structure, size, and basic properties. We'll also check for missing values and outliers that might affect our analysis.\n",
    "\n",
    "Our dataset contains earthquake records with magnitude >4.0 from AFAD (Disaster and Emergency Management Presidency), including:\n",
    "- Geographic coordinates (Longitude, Latitude)\n",
    "- Earthquake characteristics (Magnitude, Depth, Type)\n",
    "- Temporal information (Date)\n",
    "- Location descriptions\n",
    "- Fault line data for contextual analysis\n",
    "\n",
    "We'll also set up the necessary directory structure for organizing our output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a335ec9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "from datetime import datetime\n",
    "import plotly.express as px\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, LineString\n",
    "import math\n",
    "import os\n",
    "\n",
    "# Set visualization settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('viridis')\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Load the earthquake dataset\n",
    "earthquake_df = pd.read_csv('data\\\\earthquake_data.csv')\n",
    "\n",
    "# Load fault line data\n",
    "fault_gdf = gpd.read_file('data\\\\tr_faults_imp.geojson')\n",
    "print(f\"Number of fault lines: {len(fault_gdf)}\")\n",
    "print(f\"Available properties: {fault_gdf.columns.tolist()}\")\n",
    "\n",
    "# Display first few rows to understand the structure\n",
    "earthquake_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1471f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic information about the dataset\n",
    "print(f\"Dataset shape: {earthquake_df.shape}\")\n",
    "print(f\"Number of earthquakes: {len(earthquake_df)}\")\n",
    "print(\"\\nData types:\")\n",
    "print(earthquake_df.dtypes)\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values:\")\n",
    "print(earthquake_df.isnull().sum())\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\nBasic statistics:\")\n",
    "earthquake_df.describe()\n",
    "\n",
    "os.makedirs(\"maps\", exist_ok=True)\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "os.makedirs(\"produced_data\", exist_ok=True)\n",
    "\n",
    "print(\"Created output directories: maps, models, produced_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9213cf9d",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis (EDA)\n",
    "\n",
    "### 2.1 Temporal Feature Creation\n",
    "\n",
    "Converting date information to structured temporal features is crucial for analyzing patterns over time. We'll extract year, month, day, and create seasonal variables to identify potential cyclical patterns in earthquake occurrences.\n",
    "\n",
    "These temporal features will help us understand if earthquakes follow certain seasonal patterns or have increased/decreased in frequency over time periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a506fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Date column to datetime format with explicit format\n",
    "earthquake_df['Date'] = pd.to_datetime(earthquake_df['Date'], format=\"%d/%m/%Y %H:%M:%S\", errors='coerce')\n",
    "\n",
    "# Check if any dates couldn't be parsed\n",
    "null_dates = earthquake_df['Date'].isnull().sum()\n",
    "print(f\"Number of dates that couldn't be parsed: {null_dates}\")\n",
    "\n",
    "# If we have null dates, we can try alternate formats\n",
    "if null_dates > 0:\n",
    "    print(\"Trying alternative date formats...\")\n",
    "    # Try another common format\n",
    "    earthquake_df['Date'] = pd.to_datetime(earthquake_df['Date'], format=\"%d-%m-%Y %H:%M:%S\", errors='coerce')\n",
    "    # If still having issues, try auto-detection with dayfirst=True\n",
    "    if earthquake_df['Date'].isnull().sum() > 0:\n",
    "        earthquake_df['Date'] = pd.to_datetime(earthquake_df['Date'], dayfirst=True, errors='coerce')\n",
    "    \n",
    "    print(f\"Remaining null dates after fixes: {earthquake_df['Date'].isnull().sum()}\")\n",
    "\n",
    "# Create additional time-based features\n",
    "earthquake_df['Year'] = earthquake_df['Date'].dt.year\n",
    "earthquake_df['Month'] = earthquake_df['Date'].dt.month\n",
    "earthquake_df['Day'] = earthquake_df['Date'].dt.day\n",
    "earthquake_df['DayOfWeek'] = earthquake_df['Date'].dt.dayofweek\n",
    "earthquake_df['Season'] = earthquake_df['Month'].apply(lambda x: \n",
    "                                                     'Winter' if x in [12, 1, 2] else\n",
    "                                                     'Spring' if x in [3, 4, 5] else\n",
    "                                                     'Summer' if x in [6, 7, 8] else\n",
    "                                                     'Fall')\n",
    "\n",
    "# Display the updated dataframe\n",
    "earthquake_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0e7d30",
   "metadata": {},
   "source": [
    "### 2.2 Geographic Visualization\n",
    "\n",
    "Creating geographic visualizations is essential for understanding the spatial distribution of earthquakes across Turkey. We'll generate interactive maps showing:\n",
    "\n",
    "1. Earthquake hotspots using heatmap visualization\n",
    "2. Strong earthquakes (magnitude > 6) with detailed information\n",
    "3. Fault lines with importance classification\n",
    "\n",
    "These visualizations will help identify regions with higher seismic activity and their proximity to known fault lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a98bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First check and clean coordinate data\n",
    "print(\"Coordinate ranges before cleaning:\")\n",
    "print(f\"Longitude: {earthquake_df['Longitude'].min()} to {earthquake_df['Longitude'].max()}\")\n",
    "print(f\"Latitude: {earthquake_df['Latitude'].min()} to {earthquake_df['Latitude'].max()}\")\n",
    "\n",
    "# Filter out any extreme outliers (coordinates that are clearly wrong)\n",
    "# Turkey coordinates should be roughly: Longitude 26-45 E, Latitude 36-42 N\n",
    "valid_coords = (\n",
    "    (earthquake_df['Longitude'] >= 25) & \n",
    "    (earthquake_df['Longitude'] <= 45) & \n",
    "    (earthquake_df['Latitude'] >= 35) & \n",
    "    (earthquake_df['Latitude'] <= 43)\n",
    ")\n",
    "\n",
    "# Filter the dataframe to keep only valid coordinates\n",
    "clean_df = earthquake_df[valid_coords].copy()\n",
    "outliers_removed = len(earthquake_df) - len(clean_df)\n",
    "print(f\"Removed {outliers_removed} records with coordinates outside Turkey's boundaries\")\n",
    "\n",
    "print(\"Coordinate ranges after cleaning:\")\n",
    "print(f\"Longitude: {clean_df['Longitude'].min()} to {clean_df['Longitude'].max()}\")\n",
    "print(f\"Latitude: {clean_df['Latitude'].min()} to {clean_df['Latitude'].max()}\")\n",
    "\n",
    "# Create a map centered on Turkey\n",
    "# Create a map centered on Turkey\n",
    "turkey_map = folium.Map(location=[38.5, 35.5], zoom_start=6)\n",
    "\n",
    "# Sample points for better visualization performance\n",
    "sample_df = clean_df.sample(min(2000, len(clean_df)))\n",
    "\n",
    "# Create a heatmap layer with cleaned data\n",
    "heat_data = [[row['Latitude'], row['Longitude']] for index, row in sample_df.iterrows()]\n",
    "HeatMap(heat_data, radius=8, gradient={'0.4': 'blue', '0.6': 'cyan', '0.8': 'yellow', '1.0': 'red'}).add_to(turkey_map)\n",
    "\n",
    "# Add markers for strong earthquakes (magnitude > 6)\n",
    "for idx, row in clean_df[clean_df['Magnitude'] > 6].iterrows():\n",
    "    # Create enhanced popup content with styled HTML\n",
    "    popup_content = f\"\"\"\n",
    "    <div style=\"font-family: Arial; min-width: 200px;\">\n",
    "        <h4 style=\"margin-bottom: 5px; color: #d32f2f;\">Earthquake Details</h4>\n",
    "        <b>Magnitude:</b> {row['Magnitude']:.1f}<br>\n",
    "        <b>Depth:</b> {row['Depth']:.1f} km<br>\n",
    "        <b>Date:</b> {row['Date']}<br>\n",
    "        <b>Location:</b> {row['Location']}<br>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add type information if available\n",
    "    if 'Type' in row:\n",
    "        popup_content += f\"<b>Type:</b> {row['Type']}<br>\"\n",
    "    \n",
    "    # Add additional information if available  \n",
    "    if 'TypeName' in row and not pd.isna(row['TypeName']):\n",
    "        popup_content += f\"<b>Type Description:</b> {row['TypeName']}<br>\"\n",
    "        \n",
    "    # Add EventID if available\n",
    "    if 'EventID' in row and not pd.isna(row['EventID']):\n",
    "        popup_content += f\"<b>Event ID:</b> {row['EventID']}<br>\"\n",
    "    \n",
    "    popup_content += \"</div>\"\n",
    "    \n",
    "    folium.CircleMarker(\n",
    "        location=[row['Latitude'], row['Longitude']],\n",
    "        radius=row['Magnitude'] * 1.5,\n",
    "        color='red',\n",
    "        fill=True,\n",
    "        fill_color='red',\n",
    "        fill_opacity=0.7,\n",
    "        popup=folium.Popup(popup_content, max_width=300),\n",
    "    ).add_to(turkey_map)\n",
    "\n",
    "# Add fault lines to the map\n",
    "def add_faults_to_map(map_obj, fault_gdf, importance_threshold=0):\n",
    "    # Filter faults by importance if desired\n",
    "    if importance_threshold > 0:\n",
    "        fault_data = fault_gdf[fault_gdf['importance'] >= importance_threshold]\n",
    "    else:\n",
    "        fault_data = fault_gdf\n",
    "    \n",
    "    # Color by importance\n",
    "    def style_function(feature):\n",
    "        importance = feature['properties']['importance']\n",
    "        color = '#FF0000' if importance >= 4 else '#FFA500' if importance >= 3 else '#FFFF00'\n",
    "        return {\n",
    "            'color': color,\n",
    "            'weight': importance * 0.5,  # Thicker lines for more important faults\n",
    "            'opacity': 0.7\n",
    "        }\n",
    "    \n",
    "    # Add GeoJSON to map\n",
    "    folium.GeoJson(\n",
    "        fault_data,\n",
    "        name='Fault Lines',\n",
    "        style_function=style_function,\n",
    "        tooltip=folium.GeoJsonTooltip(fields=['FAULT_NAME', 'importance']),\n",
    "    ).add_to(map_obj)\n",
    "    \n",
    "    return map_obj\n",
    "\n",
    "# Add fault lines to the map\n",
    "turkey_map = add_faults_to_map(turkey_map, fault_gdf, importance_threshold=3)\n",
    "\n",
    "# Add a tile layer for better visualization\n",
    "folium.TileLayer('cartodbpositron').add_to(turkey_map)\n",
    "\n",
    "# Add a legend for the map\n",
    "legend_html = '''\n",
    "<div style=\"position: fixed; bottom: 50px; right: 50px; width: 200px; height: auto; \n",
    "    background-color: white; border:2px solid grey; z-index:9999; font-size:12px;\n",
    "    padding: 10px; border-radius: 5px;\">\n",
    "    <p><b>Earthquake Map Legend</b></p>\n",
    "    <p><i class=\"fa fa-circle\" style=\"color:red\"></i> Magnitude > 6</p>\n",
    "    <div style=\"margin-top:5px;\">\n",
    "      <p><b>Heatmap Intensity:</b></p>\n",
    "      <div style=\"display:inline-block; height:15px; width:30px; background:blue;\"></div>\n",
    "      <div style=\"display:inline-block; height:15px; width:30px; background:cyan;\"></div>\n",
    "      <div style=\"display:inline-block; height:15px; width:30px; background:yellow;\"></div>\n",
    "      <div style=\"display:inline-block; height:15px; width:30px; background:red;\"></div>\n",
    "      <p style=\"font-size:10px;\">Low &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; High</p>\n",
    "    </div>\n",
    "    <div style=\"margin-top:5px;\">\n",
    "      <p><b>Fault Line Importance:</b></p>\n",
    "      <p><span style=\"color:#FF0000;\">━━━</span> High (4+)</p>\n",
    "      <p><span style=\"color:#FFA500;\">━━━</span> Medium (3)</p>\n",
    "      <p><span style=\"color:#FFFF00;\">━━━</span> Low (<3)</p>\n",
    "    </div>\n",
    "</div>\n",
    "'''\n",
    "turkey_map.get_root().html.add_child(folium.Element(legend_html))\n",
    "\n",
    "# Save map to HTML file to view it\n",
    "turkey_map.save('maps/earthquake_map.html')\n",
    "\n",
    "# Display in notebook if you have ipywidgets installed\n",
    "# from IPython.display import display\n",
    "# display(turkey_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ae904e",
   "metadata": {},
   "source": [
    "### 2.3 Temporal Analysis\n",
    "\n",
    "Analyzing earthquake patterns over different time periods can reveal long-term trends and cyclical behaviors. We'll examine:\n",
    "\n",
    "1. Yearly earthquake frequency to identify long-term trends\n",
    "2. Seasonal distribution to detect potential seasonal patterns\n",
    "3. Monthly patterns for finer granularity\n",
    "\n",
    "These analyses may reveal whether earthquakes are becoming more frequent over time or occur more often during certain periods of the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35e3dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the cleaned dataframe for temporal analysis\n",
    "# Yearly earthquake frequency\n",
    "yearly_counts = clean_df.groupby('Year').size()\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "yearly_counts.plot(kind='bar')\n",
    "plt.title('Yearly Earthquake Frequency')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of Earthquakes')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Seasonal patterns\n",
    "seasonal_counts = clean_df.groupby('Season').size()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "seasonal_counts.plot(kind='pie', autopct='%1.1f%%')\n",
    "plt.title('Seasonal Distribution of Earthquakes')\n",
    "plt.ylabel('')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Monthly patterns\n",
    "monthly_counts = clean_df.groupby('Month').size()\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "monthly_counts.plot(kind='bar')\n",
    "plt.title('Monthly Earthquake Frequency')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Number of Earthquakes')\n",
    "plt.xticks(range(12), ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800470e6",
   "metadata": {},
   "source": [
    "### 2.4 Magnitude and Depth Analysis\n",
    "\n",
    "Examining the distribution of earthquake magnitudes and depths provides insight into the nature of seismic activity in Turkey. We'll analyze:\n",
    "\n",
    "1. Magnitude distribution to understand the relative frequency of different earthquake sizes\n",
    "2. Depth distribution to examine how deep earthquakes typically occur\n",
    "3. Relationship between magnitude and depth to identify any correlation\n",
    "\n",
    "This analysis will help us understand whether larger earthquakes tend to occur at specific depths and the typical characteristics of Turkish earthquakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0816c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magnitude distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(clean_df['Magnitude'], bins=30, kde=True)\n",
    "plt.title('Distribution of Earthquake Magnitudes')\n",
    "plt.xlabel('Magnitude')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axvline(clean_df['Magnitude'].mean(), color='red', linestyle='--', label=f'Mean: {clean_df[\"Magnitude\"].mean():.2f}')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Depth distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(clean_df['Depth'], bins=30, kde=True)\n",
    "plt.title('Distribution of Earthquake Depths')\n",
    "plt.xlabel('Depth (km)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axvline(clean_df['Depth'].mean(), color='red', linestyle='--', label=f'Mean: {clean_df[\"Depth\"].mean():.2f}')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Relationship between magnitude and depth\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(x='Depth', y='Magnitude', data=clean_df, alpha=0.6)\n",
    "plt.title('Relationship Between Earthquake Depth and Magnitude')\n",
    "plt.xlabel('Depth (km)')\n",
    "plt.ylabel('Magnitude')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f2d9ac",
   "metadata": {},
   "source": [
    "### 2.5 Correlation Analysis\n",
    "\n",
    "Exploring correlations between numerical features helps identify relationships that might be useful for prediction. We'll create a correlation matrix to visualize the relationships between:\n",
    "\n",
    "- Longitude and Latitude\n",
    "- Depth and Magnitude\n",
    "- Temporal features and earthquake characteristics\n",
    "\n",
    "Strong correlations may indicate predictive relationships that our models can leverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fecb856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis of numerical columns\n",
    "numerical_cols = clean_df.select_dtypes(include=[np.number]).columns\n",
    "correlation_matrix = clean_df[numerical_cols].corr()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e413279a",
   "metadata": {},
   "source": [
    "### 2.6 Additional Visualizations\n",
    "\n",
    "Further exploration of earthquake patterns through specialized visualizations:\n",
    "\n",
    "1. Geographic distribution by magnitude with color coding\n",
    "2. Magnitude distribution over years using box plots\n",
    "3. Depth trends over time\n",
    "4. 3D visualization incorporating longitude, latitude, and depth\n",
    "\n",
    "These visualizations provide additional perspectives on the data and may reveal patterns not evident in simpler analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8756ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geographic distribution by magnitude\n",
    "plt.figure(figsize=(14, 10))\n",
    "scatter = plt.scatter(clean_df['Longitude'], clean_df['Latitude'], \n",
    "                     c=clean_df['Magnitude'], cmap='YlOrRd', \n",
    "                     alpha=0.7, s=clean_df['Magnitude']**2)\n",
    "plt.colorbar(scatter, label='Magnitude')\n",
    "plt.title('Geographic Distribution of Earthquakes by Magnitude')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68364255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magnitude distribution over years (box plot)\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.boxplot(x='Year', y='Magnitude', data=clean_df)\n",
    "plt.title('Magnitude Distribution Over Years')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Magnitude')\n",
    "plt.xticks(rotation=90)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f6dc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depth vs Year analysis\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.boxplot(x='Year', y='Depth', data=clean_df)\n",
    "plt.title('Depth Distribution Over Years')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Depth (km)')\n",
    "plt.xticks(rotation=90)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bc98ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D visualization with Plotly\n",
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter_3d(clean_df.sample(min(3000, len(clean_df))), \n",
    "                   x='Longitude', y='Latitude', z='Depth',\n",
    "                   color='Magnitude', size='Magnitude',\n",
    "                   color_continuous_scale='Viridis',\n",
    "                   title='3D Visualization of Earthquakes')\n",
    "# Ensure proper axis orientation\n",
    "fig.update_layout(scene=dict(\n",
    "    xaxis_title='Longitude',\n",
    "    yaxis_title='Latitude',\n",
    "    zaxis_title='Depth (km)',\n",
    "    # Reverse the depth axis to show deeper earthquakes lower\n",
    "    zaxis=dict(autorange=\"reversed\")\n",
    "))\n",
    "fig.write_html('maps/earthquake_3d.html')  # Save the interactive plot\n",
    "# fig.show()  # Display in notebook if supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ac56e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magnitude frequency plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "counts, bins, _ = plt.hist(clean_df['Magnitude'], bins=30, alpha=0.7)\n",
    "plt.plot(bins[:-1], counts, '-o', color='darkred')\n",
    "plt.title('Frequency Distribution of Earthquake Magnitudes')\n",
    "plt.xlabel('Magnitude')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adafc833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regional magnitude comparison\n",
    "# Extract region from location (assuming format includes region at end)\n",
    "# Modify this based on your actual data format\n",
    "if 'Location' in clean_df.columns:\n",
    "    # Extract the first part of the location as the region\n",
    "    clean_df['Region'] = clean_df['Location'].str.split(',').str[-1].str.strip()\n",
    "    \n",
    "    # Get top 10 regions by earthquake count\n",
    "    top_regions = clean_df['Region'].value_counts().head(10).index\n",
    "    \n",
    "    # Plot magnitude distribution by region\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.boxplot(x='Region', y='Magnitude', data=clean_df[clean_df['Region'].isin(top_regions)])\n",
    "    plt.title('Magnitude Distribution by Top 10 Regions')\n",
    "    plt.xlabel('Region')\n",
    "    plt.ylabel('Magnitude')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecce5ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of earthquake frequency by month and year\n",
    "if len(clean_df) > 0:\n",
    "    # Create pivot table\n",
    "    heatmap_data = pd.pivot_table(\n",
    "        clean_df,\n",
    "        values='Magnitude',\n",
    "        index=clean_df['Date'].dt.year,\n",
    "        columns=clean_df['Date'].dt.month,\n",
    "        aggfunc='count'\n",
    "    )\n",
    "    \n",
    "    plt.figure(figsize=(14, 10))\n",
    "    sns.heatmap(heatmap_data, cmap='YlOrRd', annot=False)\n",
    "    plt.title('Earthquake Frequency by Month and Year')\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Year')\n",
    "    plt.xticks(range(1, 13), ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f86caf",
   "metadata": {},
   "source": [
    "### 2.7 Fault Line Analysis\n",
    "\n",
    "Examining the relationship between earthquakes and fault lines is crucial for understanding seismic risk. We'll:\n",
    "\n",
    "1. Calculate the distance from each earthquake to the nearest fault line\n",
    "2. Analyze how earthquake magnitude relates to proximity to faults\n",
    "3. Examine the relationship between fault importance and earthquake characteristics\n",
    "\n",
    "This analysis will help determine whether earthquakes closer to major fault lines tend to have higher magnitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6636c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate distances to fault lines\n",
    "def calc_fault_distance(row, fault_gdf):\n",
    "    point = Point(row['Longitude'], row['Latitude'])\n",
    "    \n",
    "    # Calculate distance to each fault line\n",
    "    distances = []\n",
    "    for idx, fault in fault_gdf.iterrows():\n",
    "        fault_geom = fault.geometry\n",
    "        dist = point.distance(fault_geom)\n",
    "        distances.append((dist, idx))\n",
    "    \n",
    "    # Find the closest fault\n",
    "    closest_dist, closest_idx = min(distances, key=lambda x: x[0])\n",
    "    \n",
    "    # Convert distance to kilometers (approximation)\n",
    "    # 1 degree ≈ 111 km at the equator\n",
    "    dist_km = closest_dist * 111\n",
    "    \n",
    "    # Get fault properties\n",
    "    closest_fault = fault_gdf.iloc[closest_idx]\n",
    "    \n",
    "    return pd.Series({\n",
    "        'distance_to_fault': dist_km,\n",
    "        'nearest_fault_name': closest_fault.get('FAULT_NAME', 'Unknown'),\n",
    "        'nearest_fault_importance': closest_fault.get('importance', 0)\n",
    "    })\n",
    "\n",
    "# Apply to a sample for visualization (full calculation will be done later)\n",
    "sample_size = min(1000, len(clean_df))\n",
    "fault_distance_sample = clean_df.sample(sample_size).apply(\n",
    "    lambda row: calc_fault_distance(row, fault_gdf), axis=1\n",
    ")\n",
    "\n",
    "# Visualize relationship between earthquake magnitude and distance to fault\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(fault_distance_sample['distance_to_fault'], \n",
    "           clean_df.loc[fault_distance_sample.index, 'Magnitude'],\n",
    "           alpha=0.6, c=fault_distance_sample['nearest_fault_importance'], \n",
    "           cmap='viridis')\n",
    "plt.colorbar(label='Fault Importance')\n",
    "plt.xlabel('Distance to Nearest Fault (km)')\n",
    "plt.ylabel('Magnitude')\n",
    "plt.title('Relationship Between Earthquake Magnitude and Distance to Fault')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7d0bbc",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n",
    "\n",
    "This section focuses on preparing our data for machine learning by handling missing values, outliers, and other data quality issues:\n",
    "\n",
    "1. Identifying and addressing missing values in all columns\n",
    "2. Detecting and handling outliers using the IQR method\n",
    "3. Validating geographic coordinates to ensure they're within Turkey's boundaries\n",
    "4. Standardizing features that require normalization\n",
    "5. Creating a clean dataset for the modeling phase\n",
    "\n",
    "Proper preprocessing ensures our models receive high-quality input data, which is essential for accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e239550c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing Section\n",
    "print(\"Starting data preprocessing...\")\n",
    "\n",
    "# Check for missing values again to confirm\n",
    "missing_values = clean_df.isnull().sum()\n",
    "print(f\"Missing values in each column:\\n{missing_values}\")\n",
    "\n",
    "# Handle missing values\n",
    "# For numerical columns: fill with median\n",
    "numerical_cols = ['Longitude', 'Latitude', 'Depth', 'Magnitude']\n",
    "for col in numerical_cols:\n",
    "    if missing_values[col] > 0:\n",
    "        median_value = clean_df[col].median()\n",
    "        clean_df[col].fillna(median_value, inplace=True)\n",
    "        print(f\"Filled {missing_values[col]} missing values in {col} with median: {median_value}\")\n",
    "\n",
    "# For categorical columns: fill with mode\n",
    "categorical_cols = [col for col in clean_df.columns if col not in numerical_cols \n",
    "                   and col not in ['Date', 'Year', 'Month', 'Day', 'YearMonth']]\n",
    "for col in categorical_cols:\n",
    "    if col in missing_values and missing_values[col] > 0:\n",
    "        mode_value = clean_df[col].mode()[0]\n",
    "        clean_df[col].fillna(mode_value, inplace=True)\n",
    "        print(f\"Filled {missing_values[col]} missing values in {col} with mode: {mode_value}\")\n",
    "\n",
    "# Handle outliers using IQR method for depth and magnitude\n",
    "def handle_outliers(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
    "    print(f\"Found {len(outliers)} outliers in {column}\")\n",
    "    \n",
    "    # Cap outliers instead of removing them\n",
    "    df[column] = np.where(df[column] < lower_bound, lower_bound, df[column])\n",
    "    df[column] = np.where(df[column] > upper_bound, upper_bound, df[column])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply outlier handling to Depth\n",
    "clean_df = handle_outliers(clean_df, 'Depth')\n",
    "\n",
    "# For Magnitude, we may want to keep high values as they're important\n",
    "# But we can still check for potential errors\n",
    "magnitude_outliers = clean_df[clean_df['Magnitude'] > 8.5]\n",
    "print(f\"Extremely high magnitudes (>8.5): {len(magnitude_outliers)}\")\n",
    "if len(magnitude_outliers) > 0:\n",
    "    print(magnitude_outliers[['Date', 'Magnitude', 'Location']])\n",
    "\n",
    "# Standardize coordinates if needed\n",
    "print(\"\\nCoordinate ranges:\")\n",
    "print(f\"Longitude: {clean_df['Longitude'].min()} to {clean_df['Longitude'].max()}\")\n",
    "print(f\"Latitude: {clean_df['Latitude'].min()} to {clean_df['Latitude'].max()}\")\n",
    "\n",
    "# Verify coordinates are in the Turkey region (already done in previous step)\n",
    "# This is now redundant since we've already filtered the coordinates\n",
    "turkey_coords = clean_df[\n",
    "    (clean_df['Longitude'] >= 25) & \n",
    "    (clean_df['Longitude'] <= 45) & \n",
    "    (clean_df['Latitude'] >= 35) & \n",
    "    (clean_df['Latitude'] <= 43)\n",
    "]\n",
    "outside_turkey = len(clean_df) - len(turkey_coords)\n",
    "print(f\"Records potentially outside Turkey region: {outside_turkey}\")\n",
    "\n",
    "# Create a copy of the dataframe for modeling\n",
    "model_df = clean_df.copy()\n",
    "\n",
    "print(\"\\nData preprocessing completed!\")\n",
    "model_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb628dc1",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering\n",
    "\n",
    "Feature engineering is a critical step that can significantly improve model performance. We'll create new features that may help predict earthquake magnitudes:\n",
    "\n",
    "1. Temporal features: cyclical encoding of time components (month, day of year) to capture seasonal patterns\n",
    "2. Geographic features: grid-based location encoding and regional activity metrics\n",
    "3. Historical activity features: information about previous earthquakes in the same region\n",
    "4. Distance-based features: measuring proximity to fault lines and fault characteristics\n",
    "5. Interaction features: combinations of existing features that might have predictive power together\n",
    "\n",
    "These engineered features will provide our models with richer information than the raw data alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0aba8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "print(\"Starting feature engineering...\")\n",
    "\n",
    "# Create time-based features\n",
    "model_df['DayOfYear'] = model_df['Date'].dt.dayofyear\n",
    "model_df['WeekOfYear'] = model_df['Date'].dt.isocalendar().week\n",
    "model_df['IsWeekend'] = model_df['DayOfWeek'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "\n",
    "# Encode seasonal information using cyclical encoding\n",
    "model_df['MonthSin'] = np.sin(2 * np.pi * model_df['Month']/12)\n",
    "model_df['MonthCos'] = np.cos(2 * np.pi * model_df['Month']/12)\n",
    "model_df['DayOfYearSin'] = np.sin(2 * np.pi * model_df['DayOfYear']/365)\n",
    "model_df['DayOfYearCos'] = np.cos(2 * np.pi * model_df['DayOfYear']/365)\n",
    "\n",
    "# Create regional activity features\n",
    "# Group by regions and calculate historical earthquake counts\n",
    "# First, create a spatial grid\n",
    "lon_grid = pd.cut(clean_df['Longitude'], bins=10)\n",
    "lat_grid = pd.cut(clean_df['Latitude'], bins=10)\n",
    "clean_df['Grid'] = pd.Series(zip(lon_grid, lat_grid)).astype(str)\n",
    "\n",
    "# For each earthquake, count previous earthquakes in the same grid\n",
    "clean_df = clean_df.sort_values('Date')\n",
    "clean_df['PrevQuakesInGrid'] = clean_df.groupby('Grid').cumcount()\n",
    "\n",
    "# Calculate distances between consecutive earthquakes\n",
    "clean_df['PrevLon'] = clean_df['Longitude'].shift(1)\n",
    "clean_df['PrevLat'] = clean_df['Latitude'].shift(1)\n",
    "\n",
    "# Haversine formula to calculate distance in km\n",
    "from math import radians, sin, cos, sqrt, asin\n",
    "\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    # Convert decimal degrees to radians\n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "    \n",
    "    # Haversine formula\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a))\n",
    "    r = 6371  # Radius of earth in km\n",
    "    return c * r\n",
    "\n",
    "# Apply haversine to calculate distance from previous earthquake\n",
    "clean_df['DistFromPrev'] = clean_df.apply(\n",
    "    lambda x: haversine(x['Longitude'], x['Latitude'], x['PrevLon'], x['PrevLat']) \n",
    "    if not pd.isna(x['PrevLon']) else np.nan, axis=1)\n",
    "\n",
    "# Add distance features to model_df\n",
    "model_df['PrevQuakesInGrid'] = clean_df['PrevQuakesInGrid']\n",
    "model_df['DistFromPrev'] = clean_df['DistFromPrev']\n",
    "model_df['DistFromPrev'].fillna(model_df['DistFromPrev'].median(), inplace=True)\n",
    "\n",
    "# Create feature for time since last earthquake (in days)\n",
    "clean_df['PrevDate'] = clean_df['Date'].shift(1)\n",
    "clean_df['DaysSinceLastQuake'] = (clean_df['Date'] - clean_df['PrevDate']).dt.total_seconds() / (24 * 3600)\n",
    "model_df['DaysSinceLastQuake'] = clean_df['DaysSinceLastQuake']\n",
    "model_df['DaysSinceLastQuake'].fillna(model_df['DaysSinceLastQuake'].median(), inplace=True)\n",
    "\n",
    "# Add historical magnitude information\n",
    "clean_df['PrevMagnitude'] = clean_df['Magnitude'].shift(1)\n",
    "model_df['PrevMagnitude'] = clean_df['PrevMagnitude']\n",
    "model_df['PrevMagnitude'].fillna(model_df['PrevMagnitude'].median(), inplace=True)\n",
    "\n",
    "# Create interaction features\n",
    "model_df['DepthByLat'] = model_df['Depth'] * model_df['Latitude']\n",
    "model_df['DepthByLon'] = model_df['Depth'] * model_df['Longitude']\n",
    "\n",
    "# Add fault-related features - calculate for all data points\n",
    "print(\"Calculating fault-related features...\")\n",
    "fault_features = clean_df.apply(lambda row: calc_fault_distance(row, fault_gdf), axis=1)\n",
    "clean_df = pd.concat([clean_df, fault_features], axis=1)\n",
    "model_df = pd.concat([model_df, fault_features], axis=1)\n",
    "\n",
    "# Calculate fault density in a radius\n",
    "def calc_fault_density(lat, lon, fault_gdf, radius=50):\n",
    "    \"\"\"Calculate fault density within radius (km) of a point\"\"\"\n",
    "    point = Point(lon, lat)\n",
    "    buffer_degrees = radius / 111  # Convert km to approximate degrees\n",
    "    \n",
    "    # Create a buffer around the point\n",
    "    buffer = point.buffer(buffer_degrees)\n",
    "    \n",
    "    # Count intersecting faults and sum their lengths\n",
    "    intersecting_faults = 0\n",
    "    total_length = 0\n",
    "    \n",
    "    for _, fault in fault_gdf.iterrows():\n",
    "        if buffer.intersects(fault.geometry):\n",
    "            intersecting_faults += 1\n",
    "            # Calculate length of intersection\n",
    "            intersection = buffer.intersection(fault.geometry)\n",
    "            total_length += intersection.length * 111  # Convert to km\n",
    "    \n",
    "    return pd.Series({\n",
    "        'fault_count_50km': intersecting_faults,\n",
    "        'fault_length_50km': total_length,\n",
    "        'fault_density': total_length / (math.pi * radius**2) if radius > 0 else 0\n",
    "    })\n",
    "\n",
    "# Calculate fault density for strategic points (grid centers) to avoid heavy computation\n",
    "print(\"Calculating fault density (this may take a while)...\")\n",
    "# Create a grid for Turkey\n",
    "lon_range = np.linspace(25, 45, 10)\n",
    "lat_range = np.linspace(35, 43, 10)\n",
    "grid_points = []\n",
    "\n",
    "for lon in lon_range:\n",
    "    for lat in lat_range:\n",
    "        grid_points.append((lon, lat))\n",
    "\n",
    "# Calculate density at grid points\n",
    "grid_densities = []\n",
    "for lon, lat in grid_points:\n",
    "    density = calc_fault_density(lat, lon, fault_gdf)\n",
    "    density['lon'] = lon\n",
    "    density['lat'] = lat\n",
    "    grid_densities.append(density)\n",
    "\n",
    "grid_df = pd.DataFrame(grid_densities)\n",
    "\n",
    "# For each earthquake, find nearest grid point and assign its density\n",
    "def assign_grid_density(row, grid_df):\n",
    "    distances = []\n",
    "    for idx, grid_point in grid_df.iterrows():\n",
    "        dist = haversine(row['Longitude'], row['Latitude'], grid_point['lon'], grid_point['lat'])\n",
    "        distances.append((dist, idx))\n",
    "    \n",
    "    closest_idx = min(distances, key=lambda x: x[0])[1]\n",
    "    return pd.Series({\n",
    "        'fault_count_50km': grid_df.iloc[closest_idx]['fault_count_50km'],\n",
    "        'fault_length_50km': grid_df.iloc[closest_idx]['fault_length_50km'],\n",
    "        'fault_density': grid_df.iloc[closest_idx]['fault_density']\n",
    "    })\n",
    "\n",
    "# Apply grid-based density estimation\n",
    "density_features = clean_df.apply(lambda row: assign_grid_density(row, grid_df), axis=1)\n",
    "clean_df = pd.concat([clean_df, density_features], axis=1)\n",
    "model_df = pd.concat([model_df, density_features], axis=1)\n",
    "\n",
    "# Add magnitude-distance interaction feature\n",
    "model_df['magnitude_fault_interaction'] = model_df['Magnitude'] / (model_df['distance_to_fault'] + 1)\n",
    "\n",
    "print(\"Feature engineering completed!\")\n",
    "model_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735a39cd",
   "metadata": {},
   "source": [
    "## 5. Model Selection and Training\n",
    "\n",
    "In this section, we'll train and compare multiple regression models to predict earthquake magnitude:\n",
    "\n",
    "1. Linear models: Linear Regression, Ridge, and Lasso\n",
    "2. Tree-based models: Random Forest and Gradient Boosting\n",
    "3. Advanced gradient boosting: XGBoost and LightGBM\n",
    "\n",
    "We'll evaluate each model using:\n",
    "- Cross-validation to ensure robust performance estimates\n",
    "- RMSE (Root Mean Squared Error) as our primary metric\n",
    "- MAE (Mean Absolute Error) and R² as secondary metrics\n",
    "\n",
    "This comprehensive evaluation will help us identify the most suitable model for earthquake magnitude prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd6c306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for modeling\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# Model Selection and Training\n",
    "print(\"Setting up model training...\")\n",
    "\n",
    "# Define features and target\n",
    "target = 'Magnitude'\n",
    "# Remove non-feature columns\n",
    "drops = ['Date', 'Location', 'EventID', 'TimeName', 'TypeName', \n",
    "         'MagnitudeName', 'Grid', 'PrevLon', 'PrevLat', 'PrevDate',\n",
    "         'nearest_fault_name']  # Remove string columns\n",
    "\n",
    "# Check if these optional columns exist and add them to drops if they do\n",
    "optional_drops = ['YearMonth']\n",
    "for col in optional_drops:\n",
    "    if col in model_df.columns:\n",
    "        drops.append(col)\n",
    "\n",
    "# First, create a preliminary feature list\n",
    "preliminary_features = [col for col in model_df.columns if col != target and col not in drops]\n",
    "\n",
    "# Check for non-numeric columns in our features\n",
    "for col in preliminary_features:\n",
    "    if col in model_df.columns and model_df[col].dtype == 'object':\n",
    "        print(f\"Removing non-numeric column: {col}\")\n",
    "        drops.append(col)\n",
    "\n",
    "# Final feature list with only numeric columns\n",
    "features = [col for col in model_df.columns if col != target and col not in drops]\n",
    "\n",
    "print(f\"Selected features: {features}\")\n",
    "\n",
    "# Define features to scale\n",
    "features_to_scale = ['Longitude', 'Latitude', 'Depth']\n",
    "other_features = [f for f in features if f not in features_to_scale]\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X = model_df[features]\n",
    "y = model_df[target]\n",
    "\n",
    "print(\"Columns with NaN values:\")\n",
    "for col in X.columns:\n",
    "    nan_count = X[col].isna().sum()\n",
    "    if nan_count > 0:\n",
    "        print(f\"- {col}: {nan_count} NaNs\")\n",
    "\n",
    "# Fill missing values appropriately for each column\n",
    "for col in X.columns:\n",
    "    if X[col].isna().sum() > 0:\n",
    "        # For numeric columns, use median\n",
    "        X[col] = X[col].fillna(X[col].median())\n",
    "\n",
    "# Also check target variable\n",
    "if y.isna().sum() > 0:\n",
    "    print(f\"Target has {y.isna().sum()} NaN values, filling with median\")\n",
    "    y = y.fillna(y.median())\n",
    "\n",
    "# Verify all NaNs are fixed\n",
    "print(f\"Remaining NaN values in X: {X.isna().sum().sum()}\")\n",
    "print(f\"Remaining NaN values in y: {y.isna().sum()}\")\n",
    "\n",
    "# Create new train-test split with cleaned data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Testing set size: {X_test.shape[0]}\")\n",
    "\n",
    "# Create preprocessing transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('geo_features', StandardScaler(), features_to_scale),\n",
    "        ('other_features', 'passthrough', other_features)\n",
    "    ])\n",
    "\n",
    "# Set up models with pipelines\n",
    "models = {\n",
    "    'Linear Regression': Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', LinearRegression())\n",
    "    ]),\n",
    "    'Ridge Regression': Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', Ridge())\n",
    "    ]),\n",
    "    'Lasso Regression': Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', Lasso())\n",
    "    ]),\n",
    "    'Random Forest': Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "    ]),\n",
    "    'Gradient Boosting': Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', GradientBoostingRegressor(n_estimators=100, random_state=42))\n",
    "    ]),\n",
    "    'XGBoost': Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', XGBRegressor(n_estimators=100, random_state=42))\n",
    "    ]),\n",
    "    'LightGBM': Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', LGBMRegressor(n_estimators=100, random_state=42))\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Function to evaluate models\n",
    "def evaluate_model(pipeline, X_train, X_test, y_train, y_test):\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    return mae, rmse, r2, pipeline\n",
    "\n",
    "# Cross-validation for more robust evaluation\n",
    "results = {}\n",
    "cv_results = {}\n",
    "fitted_models = {}\n",
    "\n",
    "for name, pipeline in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    mae, rmse, r2, fitted_pipeline = evaluate_model(pipeline, X_train, X_test, y_train, y_test)\n",
    "    fitted_models[name] = fitted_pipeline\n",
    "    \n",
    "    # 5-fold cross-validation for RMSE\n",
    "    cv_scores = -cross_val_score(pipeline, X, y, cv=5, scoring='neg_root_mean_squared_error')\n",
    "    \n",
    "    results[name] = {'MAE': mae, 'RMSE': rmse, 'R²': r2}\n",
    "    cv_results[name] = {'Mean RMSE': cv_scores.mean(), 'Std RMSE': cv_scores.std()}\n",
    "    \n",
    "    print(f\"{name} - MAE: {mae:.4f}, RMSE: {rmse:.4f}, R²: {r2:.4f}, CV RMSE: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "\n",
    "# Convert results to DataFrames for better visualization\n",
    "results_df = pd.DataFrame(results).T\n",
    "cv_results_df = pd.DataFrame(cv_results).T\n",
    "\n",
    "print(\"\\nTest Results:\")\n",
    "print(results_df.sort_values('RMSE'))\n",
    "\n",
    "print(\"\\nCross-Validation Results:\")\n",
    "print(cv_results_df.sort_values('Mean RMSE'))\n",
    "\n",
    "# Visualize model performance\n",
    "plt.figure(figsize=(12, 6))\n",
    "results_df['RMSE'].sort_values().plot(kind='bar')\n",
    "plt.title('RMSE by Model')\n",
    "plt.ylabel('RMSE (lower is better)')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Select the best performing model based on CV results\n",
    "best_model_name = cv_results_df.sort_values('Mean RMSE').index[0]\n",
    "print(f\"\\nBest model based on cross-validation: {best_model_name}\")\n",
    "best_pipeline = fitted_models[best_model_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb33df1b",
   "metadata": {},
   "source": [
    "## 6. Hyperparameter Optimization\n",
    "\n",
    "Fine-tuning model hyperparameters is essential for maximizing prediction accuracy. For our best-performing model, we'll:\n",
    "\n",
    "1. Define a comprehensive hyperparameter grid to explore\n",
    "2. Use RandomizedSearchCV for efficient optimization\n",
    "3. Evaluate results using cross-validation to avoid overfitting\n",
    "4. Select the optimal hyperparameter configuration\n",
    "5. Create a final model with the optimized parameters\n",
    "\n",
    "This optimization process will ensure our model achieves its maximum potential on this specific earthquake prediction task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cf8c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Optimization\n",
    "print(f\"Optimizing hyperparameters for {best_model_name}...\")\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "# Define hyperparameter grids for each model type\n",
    "# You may need to adjust these based on your selected best model\n",
    "param_grids = {\n",
    "    'Random Forest': {\n",
    "        'model__n_estimators': [50, 100, 200],\n",
    "        'model__max_depth': [None, 10, 20, 30],\n",
    "        'model__min_samples_split': [2, 5, 10],\n",
    "        'model__min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'model__n_estimators': [50, 100, 200],\n",
    "        'model__learning_rate': [0.01, 0.1, 0.2],\n",
    "        'model__max_depth': [3, 5, 7],\n",
    "        'model__min_samples_split': [2, 5]\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'model__n_estimators': [50, 100, 200],\n",
    "        'model__learning_rate': [0.01, 0.1, 0.2],\n",
    "        'model__max_depth': [3, 5, 7],\n",
    "        'model__colsample_bytree': [0.7, 0.8, 0.9]\n",
    "    },\n",
    "    'LightGBM': {\n",
    "        'model__n_estimators': [50, 100, 200],\n",
    "        'model__learning_rate': [0.01, 0.1, 0.2],\n",
    "        'model__max_depth': [3, 5, 7],\n",
    "        'model__num_leaves': [31, 50, 70]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Get the appropriate parameter grid\n",
    "if best_model_name in param_grids:\n",
    "    param_grid = param_grids[best_model_name]\n",
    "    \n",
    "    # Use RandomizedSearchCV for efficiency\n",
    "    random_search = RandomizedSearchCV(\n",
    "        best_pipeline, \n",
    "        param_distributions=param_grid,\n",
    "        n_iter=20,  # Number of parameter settings sampled\n",
    "        cv=5,\n",
    "        scoring='neg_root_mean_squared_error',\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Fit the random search\n",
    "    random_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Print best parameters and score\n",
    "    print(f\"Best parameters: {random_search.best_params_}\")\n",
    "    print(f\"Best RMSE: {-random_search.best_score_:.4f}\")\n",
    "    \n",
    "    # Create the optimized model\n",
    "    best_pipeline = random_search.best_estimator_\n",
    "else:\n",
    "    print(f\"No parameter grid defined for {best_model_name}. Using default model.\")\n",
    "    best_pipeline = fitted_models[best_model_name]\n",
    "\n",
    "# Final evaluation with the best model\n",
    "y_pred = best_pipeline.predict(X_test)\n",
    "final_mae = mean_absolute_error(y_test, y_pred)\n",
    "final_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "final_r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nFinal model performance:\")\n",
    "print(f\"MAE: {final_mae:.4f}\")\n",
    "print(f\"RMSE: {final_rmse:.4f}\")\n",
    "print(f\"R²: {final_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa2c93b",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation and Interpretation\n",
    "\n",
    "Thoroughly evaluating and interpreting our model helps us understand its strengths, limitations, and the factors influencing earthquake magnitude:\n",
    "\n",
    "1. Visualizing actual vs. predicted magnitudes\n",
    "2. Analyzing prediction errors through residual plots\n",
    "3. Examining the distribution of residuals\n",
    "4. Identifying the most important features for prediction\n",
    "5. Saving the model pipeline for deployment\n",
    "\n",
    "Understanding feature importance will provide insights into which factors most strongly influence earthquake magnitudes in Turkey, which may be valuable for risk assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76df36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation and Interpretation\n",
    "print(\"Evaluating final model...\")\n",
    "\n",
    "# Visualize actual vs predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "plt.xlabel('Actual Magnitude')\n",
    "plt.ylabel('Predicted Magnitude')\n",
    "plt.title('Actual vs Predicted Earthquake Magnitude')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot residuals\n",
    "residuals = y_test - y_pred\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_pred, residuals, alpha=0.5)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Predicted Magnitude')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residual Plot')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze residual distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(residuals, kde=True)\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Residuals')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Feature importance (for tree-based models)\n",
    "try:\n",
    "    # Extract the model component from the pipeline\n",
    "    model_component = best_pipeline.named_steps['model']\n",
    "    \n",
    "    # Check if it has feature importances\n",
    "    if hasattr(model_component, 'feature_importances_'):\n",
    "        # Get preprocessed feature names - slightly tricky with ColumnTransformer\n",
    "        # For simplicity, we'll use the original feature names\n",
    "        # Create DataFrame of feature importances\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'Feature': features,\n",
    "            'Importance': model_component.feature_importances_\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "        \n",
    "        # Visualize feature importances\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.barplot(x='Importance', y='Feature', data=feature_importance.head(15))\n",
    "        plt.title('Top 15 Feature Importances')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"Top 10 most important features:\")\n",
    "        print(feature_importance.head(10))\n",
    "except:\n",
    "    print(\"Could not extract feature importances from the model.\")\n",
    "\n",
    "# Save the entire pipeline - this contains both preprocessing and model\n",
    "import joblib\n",
    "joblib.dump(best_pipeline, 'models/earthquake_pipeline.pkl')\n",
    "print(\"Pipeline saved as 'models/earthquake_pipeline.pkl'\")\n",
    "\n",
    "# Also save the clean dataset with original coordinates for unsupervised learning\n",
    "clean_df.to_csv('produced_data/clean_earthquake_data.csv', index=False)\n",
    "print(\"Clean data with original coordinates saved as 'produced_data/clean_earthquake_data.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a31152",
   "metadata": {},
   "source": [
    "## 8. Conclusion and Next Steps\n",
    "\n",
    "Our supervised learning model predicts earthquake magnitudes in Turkey with reasonable accuracy based on geographic, temporal, and fault-related features. The processed data has been saved for further analysis.\n",
    "\n",
    "These unsupervised techniques will complement our supervised prediction model by identifying natural groupings and high-risk regions that might not be evident through regression analysis alone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8ceb2b",
   "metadata": {},
   "source": [
    "## 9. GPU Acceleration with PyTorch (Bonus)\n",
    "\n",
    "In this section, we'll implement a deep learning approach using PyTorch with GPU acceleration to compare with our traditional machine learning models. This addresses the bonus requirement for utilizing GPU libraries.\n",
    "\n",
    "### 9.1 PyTorch Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b05b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if needed (uncomment and run if necessary)\n",
    "# !pip install torch torchvision torchaudio\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7e6e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Neural Network architecture for earthquake prediction\n",
    "class EarthquakeNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(EarthquakeNN, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c07fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for PyTorch\n",
    "# Use the same features as in our traditional models\n",
    "X_torch = X.copy()\n",
    "\n",
    "# Scale the features\n",
    "scaler_torch = StandardScaler()\n",
    "X_scaled = scaler_torch.fit_transform(X_torch)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.FloatTensor(X_scaled)\n",
    "y_tensor = torch.FloatTensor(y.values.reshape(-1, 1))\n",
    "\n",
    "# Create train/test split - use the same proportions but create fresh tensors\n",
    "X_train_tensor = torch.FloatTensor(scaler_torch.transform(X_train))\n",
    "y_train_tensor = torch.FloatTensor(y_train.values.reshape(-1, 1))\n",
    "X_test_tensor = torch.FloatTensor(scaler_torch.transform(X_test))\n",
    "y_test_tensor = torch.FloatTensor(y_test.values.reshape(-1, 1))\n",
    "\n",
    "# Create PyTorch datasets and dataloaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Testing samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e959385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PyTorch model, loss function and optimizer\n",
    "model = EarthquakeNN(input_size=X_train.shape[1])\n",
    "model = model.to(device)  # Move model to GPU if available\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c5962c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "def train_and_evaluate():\n",
    "    epochs = 100\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 10\n",
    "    counter = 0\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    # Record training time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        train_loss = running_loss / len(train_dataset)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        val_loss = val_loss / len(test_dataset)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "        \n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "        \n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    # Calculate total training time\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"Training completed in {training_time:.2f} seconds\")\n",
    "    \n",
    "    return train_losses, val_losses, training_time\n",
    "\n",
    "# Run training\n",
    "train_losses, val_losses, gpu_training_time = train_and_evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e5b2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on test data\n",
    "def evaluate_model():\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            all_preds.append(outputs.cpu().numpy())\n",
    "            all_targets.append(targets.cpu().numpy())\n",
    "    \n",
    "    # Concatenate all predictions and targets\n",
    "    y_pred_torch = np.vstack(all_preds).flatten()\n",
    "    y_test_torch = np.vstack(all_targets).flatten()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    torch_mae = mean_absolute_error(y_test_torch, y_pred_torch)\n",
    "    torch_rmse = np.sqrt(mean_squared_error(y_test_torch, y_pred_torch))\n",
    "    torch_r2 = r2_score(y_test_torch, y_pred_torch)\n",
    "    \n",
    "    print(f\"PyTorch Neural Network Results:\")\n",
    "    print(f\"MAE: {torch_mae:.4f}\")\n",
    "    print(f\"RMSE: {torch_rmse:.4f}\")\n",
    "    print(f\"R²: {torch_r2:.4f}\")\n",
    "    \n",
    "    return torch_mae, torch_rmse, torch_r2, y_pred_torch\n",
    "\n",
    "# Evaluate model\n",
    "torch_mae, torch_rmse, torch_r2, y_pred_torch = evaluate_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d92de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize actual vs predicted\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred_torch, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "plt.xlabel('Actual Magnitude')\n",
    "plt.ylabel('Predicted Magnitude (PyTorch)')\n",
    "plt.title('Actual vs Predicted Earthquake Magnitude (PyTorch)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbef7d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare PyTorch with traditional model results\n",
    "comparison_data = {\n",
    "    'Model': ['PyTorch NN (GPU)', f'{best_model_name} (CPU)'],\n",
    "    'MAE': [torch_mae, final_mae],\n",
    "    'RMSE': [torch_rmse, final_rmse],\n",
    "    'R²': [torch_r2, final_r2],\n",
    "    'Training Time (s)': [gpu_training_time, None]  # We don't have CPU time recorded\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"Model Performance Comparison:\")\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90da5118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the PyTorch model\n",
    "torch.save(model.state_dict(), 'models/earthquake_pytorch_model.pt')\n",
    "print(\"PyTorch model saved as 'models/earthquake_pytorch_model.pt'\")\n",
    "\n",
    "# Also save a script to load and use the model\n",
    "with open('models/load_pytorch_model.py', 'w') as f:\n",
    "    f.write(\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class EarthquakeNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(EarthquakeNN, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Function to load model\n",
    "def load_model(model_path, input_size, device='cpu'):\n",
    "    model = EarthquakeNN(input_size)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "    return model\n",
    "\"\"\")\n",
    "print(\"Model loading script saved as 'models/load_pytorch_model.py'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
