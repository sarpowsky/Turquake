{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "708e6cfa",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Initial Exploration\n",
    "\n",
    "First, we'll load the dataset and examine its basic structure and properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a335ec9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "from datetime import datetime\n",
    "import plotly.express as px\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, LineString\n",
    "import math\n",
    "\n",
    "# Set visualization settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('viridis')\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Load the earthquake dataset\n",
    "earthquake_df = pd.read_csv('UI\\\\earthquake_data.csv')\n",
    "\n",
    "# Load fault line data\n",
    "fault_gdf = gpd.read_file('UI\\\\tr_faults_imp.geojson')\n",
    "print(f\"Number of fault lines: {len(fault_gdf)}\")\n",
    "print(f\"Available properties: {fault_gdf.columns.tolist()}\")\n",
    "\n",
    "# Display first few rows to understand the structure\n",
    "earthquake_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1471f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic information about the dataset\n",
    "print(f\"Dataset shape: {earthquake_df.shape}\")\n",
    "print(f\"Number of earthquakes: {len(earthquake_df)}\")\n",
    "print(\"\\nData types:\")\n",
    "print(earthquake_df.dtypes)\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values:\")\n",
    "print(earthquake_df.isnull().sum())\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\nBasic statistics:\")\n",
    "earthquake_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9213cf9d",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis (EDA)\n",
    "\n",
    "### 2.1 Temporal Feature Creation\n",
    "\n",
    "Converting date information to structured temporal features for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a506fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Date column to datetime format with explicit format\n",
    "earthquake_df['Date'] = pd.to_datetime(earthquake_df['Date'], format=\"%d/%m/%Y %H:%M:%S\", errors='coerce')\n",
    "\n",
    "# Check if any dates couldn't be parsed\n",
    "null_dates = earthquake_df['Date'].isnull().sum()\n",
    "print(f\"Number of dates that couldn't be parsed: {null_dates}\")\n",
    "\n",
    "# If we have null dates, we can try alternate formats\n",
    "if null_dates > 0:\n",
    "    print(\"Trying alternative date formats...\")\n",
    "    # Try another common format\n",
    "    earthquake_df['Date'] = pd.to_datetime(earthquake_df['Date'], format=\"%d-%m-%Y %H:%M:%S\", errors='coerce')\n",
    "    # If still having issues, try auto-detection with dayfirst=True\n",
    "    if earthquake_df['Date'].isnull().sum() > 0:\n",
    "        earthquake_df['Date'] = pd.to_datetime(earthquake_df['Date'], dayfirst=True, errors='coerce')\n",
    "    \n",
    "    print(f\"Remaining null dates after fixes: {earthquake_df['Date'].isnull().sum()}\")\n",
    "\n",
    "# Create additional time-based features\n",
    "earthquake_df['Year'] = earthquake_df['Date'].dt.year\n",
    "earthquake_df['Month'] = earthquake_df['Date'].dt.month\n",
    "earthquake_df['Day'] = earthquake_df['Date'].dt.day\n",
    "earthquake_df['DayOfWeek'] = earthquake_df['Date'].dt.dayofweek\n",
    "earthquake_df['Season'] = earthquake_df['Month'].apply(lambda x: \n",
    "                                                     'Winter' if x in [12, 1, 2] else\n",
    "                                                     'Spring' if x in [3, 4, 5] else\n",
    "                                                     'Summer' if x in [6, 7, 8] else\n",
    "                                                     'Fall')\n",
    "\n",
    "# Display the updated dataframe\n",
    "earthquake_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0e7d30",
   "metadata": {},
   "source": [
    "### 2.2 Geographic Visualization\n",
    "\n",
    "Creating maps to visualize the earthquake distribution across Turkey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a98bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First check and clean coordinate data\n",
    "print(\"Coordinate ranges before cleaning:\")\n",
    "print(f\"Longitude: {earthquake_df['Longitude'].min()} to {earthquake_df['Longitude'].max()}\")\n",
    "print(f\"Latitude: {earthquake_df['Latitude'].min()} to {earthquake_df['Latitude'].max()}\")\n",
    "\n",
    "# Filter out any extreme outliers (coordinates that are clearly wrong)\n",
    "# Turkey coordinates should be roughly: Longitude 26-45 E, Latitude 36-42 N\n",
    "valid_coords = (\n",
    "    (earthquake_df['Longitude'] >= 25) & \n",
    "    (earthquake_df['Longitude'] <= 45) & \n",
    "    (earthquake_df['Latitude'] >= 35) & \n",
    "    (earthquake_df['Latitude'] <= 43)\n",
    ")\n",
    "\n",
    "# Filter the dataframe to keep only valid coordinates\n",
    "clean_df = earthquake_df[valid_coords].copy()\n",
    "outliers_removed = len(earthquake_df) - len(clean_df)\n",
    "print(f\"Removed {outliers_removed} records with coordinates outside Turkey's boundaries\")\n",
    "\n",
    "print(\"Coordinate ranges after cleaning:\")\n",
    "print(f\"Longitude: {clean_df['Longitude'].min()} to {clean_df['Longitude'].max()}\")\n",
    "print(f\"Latitude: {clean_df['Latitude'].min()} to {clean_df['Latitude'].max()}\")\n",
    "\n",
    "# Create a map centered on Turkey\n",
    "turkey_map = folium.Map(location=[38.5, 35.5], zoom_start=6)\n",
    "\n",
    "# Sample points for better visualization performance\n",
    "sample_df = clean_df.sample(min(2000, len(clean_df)))\n",
    "\n",
    "# Create a heatmap layer with cleaned data\n",
    "heat_data = [[row['Latitude'], row['Longitude']] for index, row in sample_df.iterrows()]\n",
    "HeatMap(heat_data, radius=8, gradient={'0.4': 'blue', '0.6': 'cyan', '0.8': 'yellow', '1.0': 'red'}).add_to(turkey_map)\n",
    "\n",
    "# Add markers for strong earthquakes (magnitude > 6)\n",
    "for idx, row in clean_df[clean_df['Magnitude'] > 6].iterrows():\n",
    "    folium.CircleMarker(\n",
    "        location=[row['Latitude'], row['Longitude']],\n",
    "        radius=row['Magnitude'] * 1.5,\n",
    "        color='red',\n",
    "        fill=True,\n",
    "        fill_color='red',\n",
    "        fill_opacity=0.7,\n",
    "        popup=f\"Magnitude: {row['Magnitude']}<br>Date: {row['Date']}<br>Location: {row['Location']}\",\n",
    "    ).add_to(turkey_map)\n",
    "\n",
    "# Add fault lines to the map\n",
    "def add_faults_to_map(map_obj, fault_gdf, importance_threshold=0):\n",
    "    # Filter faults by importance if desired\n",
    "    if importance_threshold > 0:\n",
    "        fault_data = fault_gdf[fault_gdf['importance'] >= importance_threshold]\n",
    "    else:\n",
    "        fault_data = fault_gdf\n",
    "    \n",
    "    # Color by importance\n",
    "    def style_function(feature):\n",
    "        importance = feature['properties']['importance']\n",
    "        color = '#FF0000' if importance >= 4 else '#FFA500' if importance >= 3 else '#FFFF00'\n",
    "        return {\n",
    "            'color': color,\n",
    "            'weight': importance * 0.5,  # Thicker lines for more important faults\n",
    "            'opacity': 0.7\n",
    "        }\n",
    "    \n",
    "    # Add GeoJSON to map\n",
    "    folium.GeoJson(\n",
    "        fault_data,\n",
    "        name='Fault Lines',\n",
    "        style_function=style_function,\n",
    "        tooltip=folium.GeoJsonTooltip(fields=['FAULT_NAME', 'importance']),\n",
    "    ).add_to(map_obj)\n",
    "    \n",
    "    return map_obj\n",
    "\n",
    "# Add fault lines to the map\n",
    "turkey_map = add_faults_to_map(turkey_map, fault_gdf, importance_threshold=3)\n",
    "\n",
    "# Add a tile layer for better visualization\n",
    "folium.TileLayer('cartodbpositron').add_to(turkey_map)\n",
    "\n",
    "# Save map to HTML file to view it\n",
    "turkey_map.save('earthquake_map.html')\n",
    "\n",
    "# Display in notebook if you have ipywidgets installed\n",
    "# from IPython.display import display\n",
    "# display(turkey_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ae904e",
   "metadata": {},
   "source": [
    "### 2.3 Temporal Analysis\n",
    "\n",
    "Analyzing patterns in earthquake frequency over time (yearly, monthly, seasonal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35e3dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the cleaned dataframe for temporal analysis\n",
    "# Yearly earthquake frequency\n",
    "yearly_counts = clean_df.groupby('Year').size()\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "yearly_counts.plot(kind='bar')\n",
    "plt.title('Yearly Earthquake Frequency')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of Earthquakes')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Seasonal patterns\n",
    "seasonal_counts = clean_df.groupby('Season').size()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "seasonal_counts.plot(kind='pie', autopct='%1.1f%%')\n",
    "plt.title('Seasonal Distribution of Earthquakes')\n",
    "plt.ylabel('')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Monthly patterns\n",
    "monthly_counts = clean_df.groupby('Month').size()\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "monthly_counts.plot(kind='bar')\n",
    "plt.title('Monthly Earthquake Frequency')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Number of Earthquakes')\n",
    "plt.xticks(range(12), ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800470e6",
   "metadata": {},
   "source": [
    "### 2.4 Magnitude and Depth Analysis\n",
    "\n",
    "Examining the distribution of earthquake magnitudes and depths, and their relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0816c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magnitude distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(clean_df['Magnitude'], bins=30, kde=True)\n",
    "plt.title('Distribution of Earthquake Magnitudes')\n",
    "plt.xlabel('Magnitude')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axvline(clean_df['Magnitude'].mean(), color='red', linestyle='--', label=f'Mean: {clean_df[\"Magnitude\"].mean():.2f}')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Depth distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(clean_df['Depth'], bins=30, kde=True)\n",
    "plt.title('Distribution of Earthquake Depths')\n",
    "plt.xlabel('Depth (km)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axvline(clean_df['Depth'].mean(), color='red', linestyle='--', label=f'Mean: {clean_df[\"Depth\"].mean():.2f}')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Relationship between magnitude and depth\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(x='Depth', y='Magnitude', data=clean_df, alpha=0.6)\n",
    "plt.title('Relationship Between Earthquake Depth and Magnitude')\n",
    "plt.xlabel('Depth (km)')\n",
    "plt.ylabel('Magnitude')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f2d9ac",
   "metadata": {},
   "source": [
    "### 2.5 Correlation Analysis\n",
    "\n",
    "Exploring correlations between numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fecb856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis of numerical columns\n",
    "numerical_cols = clean_df.select_dtypes(include=[np.number]).columns\n",
    "correlation_matrix = clean_df[numerical_cols].corr()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e413279a",
   "metadata": {},
   "source": [
    "### 2.6 Additional Visualizations\n",
    "\n",
    "Further exploration of earthquake patterns through geographic, temporal, and magnitude-based visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8756ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geographic distribution by magnitude\n",
    "plt.figure(figsize=(14, 10))\n",
    "scatter = plt.scatter(clean_df['Longitude'], clean_df['Latitude'], \n",
    "                     c=clean_df['Magnitude'], cmap='YlOrRd', \n",
    "                     alpha=0.7, s=clean_df['Magnitude']**2)\n",
    "plt.colorbar(scatter, label='Magnitude')\n",
    "plt.title('Geographic Distribution of Earthquakes by Magnitude')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68364255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magnitude distribution over years (box plot)\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.boxplot(x='Year', y='Magnitude', data=clean_df)\n",
    "plt.title('Magnitude Distribution Over Years')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Magnitude')\n",
    "plt.xticks(rotation=90)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f6dc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depth vs Year analysis\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.boxplot(x='Year', y='Depth', data=clean_df)\n",
    "plt.title('Depth Distribution Over Years')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Depth (km)')\n",
    "plt.xticks(rotation=90)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bc98ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D visualization with Plotly\n",
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter_3d(clean_df.sample(min(3000, len(clean_df))), \n",
    "                   x='Longitude', y='Latitude', z='Depth',\n",
    "                   color='Magnitude', size='Magnitude',\n",
    "                   color_continuous_scale='Viridis',\n",
    "                   title='3D Visualization of Earthquakes')\n",
    "# Ensure proper axis orientation\n",
    "fig.update_layout(scene=dict(\n",
    "    xaxis_title='Longitude',\n",
    "    yaxis_title='Latitude',\n",
    "    zaxis_title='Depth (km)',\n",
    "    # Reverse the depth axis to show deeper earthquakes lower\n",
    "    zaxis=dict(autorange=\"reversed\")\n",
    "))\n",
    "fig.write_html('earthquake_3d.html')  # Save the interactive plot\n",
    "# fig.show()  # Display in notebook if supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ac56e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magnitude frequency plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "counts, bins, _ = plt.hist(clean_df['Magnitude'], bins=30, alpha=0.7)\n",
    "plt.plot(bins[:-1], counts, '-o', color='darkred')\n",
    "plt.title('Frequency Distribution of Earthquake Magnitudes')\n",
    "plt.xlabel('Magnitude')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adafc833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regional magnitude comparison\n",
    "# Extract region from location (assuming format includes region at end)\n",
    "# Modify this based on your actual data format\n",
    "if 'Location' in clean_df.columns:\n",
    "    # Extract the first part of the location as the region\n",
    "    clean_df['Region'] = clean_df['Location'].str.split(',').str[-1].str.strip()\n",
    "    \n",
    "    # Get top 10 regions by earthquake count\n",
    "    top_regions = clean_df['Region'].value_counts().head(10).index\n",
    "    \n",
    "    # Plot magnitude distribution by region\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.boxplot(x='Region', y='Magnitude', data=clean_df[clean_df['Region'].isin(top_regions)])\n",
    "    plt.title('Magnitude Distribution by Top 10 Regions')\n",
    "    plt.xlabel('Region')\n",
    "    plt.ylabel('Magnitude')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecce5ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of earthquake frequency by month and year\n",
    "if len(clean_df) > 0:\n",
    "    # Create pivot table\n",
    "    heatmap_data = pd.pivot_table(\n",
    "        clean_df,\n",
    "        values='Magnitude',\n",
    "        index=clean_df['Date'].dt.year,\n",
    "        columns=clean_df['Date'].dt.month,\n",
    "        aggfunc='count'\n",
    "    )\n",
    "    \n",
    "    plt.figure(figsize=(14, 10))\n",
    "    sns.heatmap(heatmap_data, cmap='YlOrRd', annot=False)\n",
    "    plt.title('Earthquake Frequency by Month and Year')\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Year')\n",
    "    plt.xticks(range(1, 13), ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f86caf",
   "metadata": {},
   "source": [
    "### 2.7 Fault Line Analysis\n",
    "Examining the relationship between earthquakes and fault lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6636c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate distances to fault lines\n",
    "def calc_fault_distance(row, fault_gdf):\n",
    "    point = Point(row['Longitude'], row['Latitude'])\n",
    "    \n",
    "    # Calculate distance to each fault line\n",
    "    distances = []\n",
    "    for idx, fault in fault_gdf.iterrows():\n",
    "        fault_geom = fault.geometry\n",
    "        dist = point.distance(fault_geom)\n",
    "        distances.append((dist, idx))\n",
    "    \n",
    "    # Find the closest fault\n",
    "    closest_dist, closest_idx = min(distances, key=lambda x: x[0])\n",
    "    \n",
    "    # Convert distance to kilometers (approximation)\n",
    "    # 1 degree ≈ 111 km at the equator\n",
    "    dist_km = closest_dist * 111\n",
    "    \n",
    "    # Get fault properties\n",
    "    closest_fault = fault_gdf.iloc[closest_idx]\n",
    "    \n",
    "    return pd.Series({\n",
    "        'distance_to_fault': dist_km,\n",
    "        'nearest_fault_name': closest_fault.get('FAULT_NAME', 'Unknown'),\n",
    "        'nearest_fault_importance': closest_fault.get('importance', 0)\n",
    "    })\n",
    "\n",
    "# Apply to a sample for visualization (full calculation will be done later)\n",
    "sample_size = min(1000, len(clean_df))\n",
    "fault_distance_sample = clean_df.sample(sample_size).apply(\n",
    "    lambda row: calc_fault_distance(row, fault_gdf), axis=1\n",
    ")\n",
    "\n",
    "# Visualize relationship between earthquake magnitude and distance to fault\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(fault_distance_sample['distance_to_fault'], \n",
    "           clean_df.loc[fault_distance_sample.index, 'Magnitude'],\n",
    "           alpha=0.6, c=fault_distance_sample['nearest_fault_importance'], \n",
    "           cmap='viridis')\n",
    "plt.colorbar(label='Fault Importance')\n",
    "plt.xlabel('Distance to Nearest Fault (km)')\n",
    "plt.ylabel('Magnitude')\n",
    "plt.title('Relationship Between Earthquake Magnitude and Distance to Fault')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7d0bbc",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n",
    "\n",
    "Handling missing values, outliers, and preparing data for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e239550c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing Section\n",
    "print(\"Starting data preprocessing...\")\n",
    "\n",
    "# Check for missing values again to confirm\n",
    "missing_values = clean_df.isnull().sum()\n",
    "print(f\"Missing values in each column:\\n{missing_values}\")\n",
    "\n",
    "# Handle missing values\n",
    "# For numerical columns: fill with median\n",
    "numerical_cols = ['Longitude', 'Latitude', 'Depth', 'Magnitude']\n",
    "for col in numerical_cols:\n",
    "    if missing_values[col] > 0:\n",
    "        median_value = clean_df[col].median()\n",
    "        clean_df[col].fillna(median_value, inplace=True)\n",
    "        print(f\"Filled {missing_values[col]} missing values in {col} with median: {median_value}\")\n",
    "\n",
    "# For categorical columns: fill with mode\n",
    "categorical_cols = [col for col in clean_df.columns if col not in numerical_cols \n",
    "                   and col not in ['Date', 'Year', 'Month', 'Day', 'YearMonth']]\n",
    "for col in categorical_cols:\n",
    "    if col in missing_values and missing_values[col] > 0:\n",
    "        mode_value = clean_df[col].mode()[0]\n",
    "        clean_df[col].fillna(mode_value, inplace=True)\n",
    "        print(f\"Filled {missing_values[col]} missing values in {col} with mode: {mode_value}\")\n",
    "\n",
    "# Handle outliers using IQR method for depth and magnitude\n",
    "def handle_outliers(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
    "    print(f\"Found {len(outliers)} outliers in {column}\")\n",
    "    \n",
    "    # Cap outliers instead of removing them\n",
    "    df[column] = np.where(df[column] < lower_bound, lower_bound, df[column])\n",
    "    df[column] = np.where(df[column] > upper_bound, upper_bound, df[column])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply outlier handling to Depth\n",
    "clean_df = handle_outliers(clean_df, 'Depth')\n",
    "\n",
    "# For Magnitude, we may want to keep high values as they're important\n",
    "# But we can still check for potential errors\n",
    "magnitude_outliers = clean_df[clean_df['Magnitude'] > 8.5]\n",
    "print(f\"Extremely high magnitudes (>8.5): {len(magnitude_outliers)}\")\n",
    "if len(magnitude_outliers) > 0:\n",
    "    print(magnitude_outliers[['Date', 'Magnitude', 'Location']])\n",
    "\n",
    "# Standardize coordinates if needed\n",
    "print(\"\\nCoordinate ranges:\")\n",
    "print(f\"Longitude: {clean_df['Longitude'].min()} to {clean_df['Longitude'].max()}\")\n",
    "print(f\"Latitude: {clean_df['Latitude'].min()} to {clean_df['Latitude'].max()}\")\n",
    "\n",
    "# Verify coordinates are in the Turkey region (already done in previous step)\n",
    "# This is now redundant since we've already filtered the coordinates\n",
    "turkey_coords = clean_df[\n",
    "    (clean_df['Longitude'] >= 25) & \n",
    "    (clean_df['Longitude'] <= 45) & \n",
    "    (clean_df['Latitude'] >= 35) & \n",
    "    (clean_df['Latitude'] <= 43)\n",
    "]\n",
    "outside_turkey = len(clean_df) - len(turkey_coords)\n",
    "print(f\"Records potentially outside Turkey region: {outside_turkey}\")\n",
    "\n",
    "# Normalize numerical features for modeling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create a copy of the dataframe for modeling\n",
    "model_df = clean_df.copy()\n",
    "\n",
    "# Select features for scaling\n",
    "features_to_scale = ['Longitude', 'Latitude', 'Depth']\n",
    "scaler = StandardScaler()\n",
    "model_df[features_to_scale] = scaler.fit_transform(model_df[features_to_scale])\n",
    "\n",
    "print(\"\\nData preprocessing completed!\")\n",
    "model_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb628dc1",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering\n",
    "\n",
    "Creating new features to improve model performance, including fault-related features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0aba8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "print(\"Starting feature engineering...\")\n",
    "\n",
    "# Create time-based features\n",
    "model_df['DayOfYear'] = model_df['Date'].dt.dayofyear\n",
    "model_df['WeekOfYear'] = model_df['Date'].dt.isocalendar().week\n",
    "model_df['IsWeekend'] = model_df['DayOfWeek'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "\n",
    "# Encode seasonal information using cyclical encoding\n",
    "model_df['MonthSin'] = np.sin(2 * np.pi * model_df['Month']/12)\n",
    "model_df['MonthCos'] = np.cos(2 * np.pi * model_df['Month']/12)\n",
    "model_df['DayOfYearSin'] = np.sin(2 * np.pi * model_df['DayOfYear']/365)\n",
    "model_df['DayOfYearCos'] = np.cos(2 * np.pi * model_df['DayOfYear']/365)\n",
    "\n",
    "# Create regional activity features\n",
    "# Group by regions and calculate historical earthquake counts\n",
    "# First, create a spatial grid\n",
    "lon_grid = pd.cut(clean_df['Longitude'], bins=10)\n",
    "lat_grid = pd.cut(clean_df['Latitude'], bins=10)\n",
    "clean_df['Grid'] = pd.Series(zip(lon_grid, lat_grid)).astype(str)\n",
    "\n",
    "# For each earthquake, count previous earthquakes in the same grid\n",
    "clean_df = clean_df.sort_values('Date')\n",
    "clean_df['PrevQuakesInGrid'] = clean_df.groupby('Grid').cumcount()\n",
    "\n",
    "# Calculate distances between consecutive earthquakes\n",
    "clean_df['PrevLon'] = clean_df['Longitude'].shift(1)\n",
    "clean_df['PrevLat'] = clean_df['Latitude'].shift(1)\n",
    "\n",
    "# Haversine formula to calculate distance in km\n",
    "from math import radians, sin, cos, sqrt, asin\n",
    "\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    # Convert decimal degrees to radians\n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "    \n",
    "    # Haversine formula\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a))\n",
    "    r = 6371  # Radius of earth in km\n",
    "    return c * r\n",
    "\n",
    "# Apply haversine to calculate distance from previous earthquake\n",
    "clean_df['DistFromPrev'] = clean_df.apply(\n",
    "    lambda x: haversine(x['Longitude'], x['Latitude'], x['PrevLon'], x['PrevLat']) \n",
    "    if not pd.isna(x['PrevLon']) else np.nan, axis=1)\n",
    "\n",
    "# Add distance features to model_df\n",
    "model_df['PrevQuakesInGrid'] = clean_df['PrevQuakesInGrid']\n",
    "model_df['DistFromPrev'] = clean_df['DistFromPrev']\n",
    "model_df['DistFromPrev'].fillna(model_df['DistFromPrev'].median(), inplace=True)\n",
    "\n",
    "# Create feature for time since last earthquake (in days)\n",
    "clean_df['PrevDate'] = clean_df['Date'].shift(1)\n",
    "clean_df['DaysSinceLastQuake'] = (clean_df['Date'] - clean_df['PrevDate']).dt.total_seconds() / (24 * 3600)\n",
    "model_df['DaysSinceLastQuake'] = clean_df['DaysSinceLastQuake']\n",
    "model_df['DaysSinceLastQuake'].fillna(model_df['DaysSinceLastQuake'].median(), inplace=True)\n",
    "\n",
    "# Add historical magnitude information\n",
    "clean_df['PrevMagnitude'] = clean_df['Magnitude'].shift(1)\n",
    "model_df['PrevMagnitude'] = clean_df['PrevMagnitude']\n",
    "model_df['PrevMagnitude'].fillna(model_df['PrevMagnitude'].median(), inplace=True)\n",
    "\n",
    "# Create interaction features\n",
    "model_df['DepthByLat'] = model_df['Depth'] * model_df['Latitude']\n",
    "model_df['DepthByLon'] = model_df['Depth'] * model_df['Longitude']\n",
    "\n",
    "# Add fault-related features - calculate for all data points\n",
    "print(\"Calculating fault-related features...\")\n",
    "fault_features = clean_df.apply(lambda row: calc_fault_distance(row, fault_gdf), axis=1)\n",
    "clean_df = pd.concat([clean_df, fault_features], axis=1)\n",
    "model_df = pd.concat([model_df, fault_features], axis=1)\n",
    "\n",
    "# Calculate fault density in a radius\n",
    "def calc_fault_density(lat, lon, fault_gdf, radius=50):\n",
    "    \"\"\"Calculate fault density within radius (km) of a point\"\"\"\n",
    "    point = Point(lon, lat)\n",
    "    buffer_degrees = radius / 111  # Convert km to approximate degrees\n",
    "    \n",
    "    # Create a buffer around the point\n",
    "    buffer = point.buffer(buffer_degrees)\n",
    "    \n",
    "    # Count intersecting faults and sum their lengths\n",
    "    intersecting_faults = 0\n",
    "    total_length = 0\n",
    "    \n",
    "    for _, fault in fault_gdf.iterrows():\n",
    "        if buffer.intersects(fault.geometry):\n",
    "            intersecting_faults += 1\n",
    "            # Calculate length of intersection\n",
    "            intersection = buffer.intersection(fault.geometry)\n",
    "            total_length += intersection.length * 111  # Convert to km\n",
    "    \n",
    "    return pd.Series({\n",
    "        'fault_count_50km': intersecting_faults,\n",
    "        'fault_length_50km': total_length,\n",
    "        'fault_density': total_length / (math.pi * radius**2) if radius > 0 else 0\n",
    "    })\n",
    "\n",
    "# Calculate fault density for strategic points (grid centers) to avoid heavy computation\n",
    "print(\"Calculating fault density (this may take a while)...\")\n",
    "# Create a grid for Turkey\n",
    "lon_range = np.linspace(25, 45, 10)\n",
    "lat_range = np.linspace(35, 43, 10)\n",
    "grid_points = []\n",
    "\n",
    "for lon in lon_range:\n",
    "    for lat in lat_range:\n",
    "        grid_points.append((lon, lat))\n",
    "\n",
    "# Calculate density at grid points\n",
    "grid_densities = []\n",
    "for lon, lat in grid_points:\n",
    "    density = calc_fault_density(lat, lon, fault_gdf)\n",
    "    density['lon'] = lon\n",
    "    density['lat'] = lat\n",
    "    grid_densities.append(density)\n",
    "\n",
    "grid_df = pd.DataFrame(grid_densities)\n",
    "\n",
    "# For each earthquake, find nearest grid point and assign its density\n",
    "def assign_grid_density(row, grid_df):\n",
    "    distances = []\n",
    "    for idx, grid_point in grid_df.iterrows():\n",
    "        dist = haversine(row['Longitude'], row['Latitude'], grid_point['lon'], grid_point['lat'])\n",
    "        distances.append((dist, idx))\n",
    "    \n",
    "    closest_idx = min(distances, key=lambda x: x[0])[1]\n",
    "    return pd.Series({\n",
    "        'fault_count_50km': grid_df.iloc[closest_idx]['fault_count_50km'],\n",
    "        'fault_length_50km': grid_df.iloc[closest_idx]['fault_length_50km'],\n",
    "        'fault_density': grid_df.iloc[closest_idx]['fault_density']\n",
    "    })\n",
    "\n",
    "# Apply grid-based density estimation\n",
    "density_features = clean_df.apply(lambda row: assign_grid_density(row, grid_df), axis=1)\n",
    "clean_df = pd.concat([clean_df, density_features], axis=1)\n",
    "model_df = pd.concat([model_df, density_features], axis=1)\n",
    "\n",
    "# Add magnitude-distance interaction feature\n",
    "model_df['magnitude_fault_interaction'] = model_df['Magnitude'] / (model_df['distance_to_fault'] + 1)\n",
    "\n",
    "print(\"Feature engineering completed!\")\n",
    "model_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735a39cd",
   "metadata": {},
   "source": [
    "## 5. Model Selection and Training\n",
    "\n",
    "Training and comparing multiple regression models to predict earthquake magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd6c306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for modeling\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# Model Selection and Training\n",
    "print(\"Setting up model training...\")\n",
    "\n",
    "# Define features and target\n",
    "target = 'Magnitude'\n",
    "# Remove non-feature columns\n",
    "drops = ['Date', 'Location', 'EventID', 'TimeName', 'TypeName', \n",
    "         'MagnitudeName', 'Grid', 'PrevLon', 'PrevLat', 'PrevDate',\n",
    "         'nearest_fault_name']  # Remove string columns\n",
    "\n",
    "# Check if these optional columns exist and add them to drops if they do\n",
    "optional_drops = ['YearMonth']\n",
    "for col in optional_drops:\n",
    "    if col in model_df.columns:\n",
    "        drops.append(col)\n",
    "\n",
    "# First, create a preliminary feature list\n",
    "preliminary_features = [col for col in model_df.columns if col != target and col not in drops]\n",
    "\n",
    "# Check for non-numeric columns in our features\n",
    "for col in preliminary_features:\n",
    "    if col in model_df.columns and model_df[col].dtype == 'object':\n",
    "        print(f\"Removing non-numeric column: {col}\")\n",
    "        drops.append(col)\n",
    "\n",
    "# Final feature list with only numeric columns\n",
    "features = [col for col in model_df.columns if col != target and col not in drops]\n",
    "\n",
    "print(f\"Selected features: {features}\")\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X = model_df[features]\n",
    "y = model_df[target]\n",
    "\n",
    "print(\"Columns with NaN values:\")\n",
    "for col in X.columns:\n",
    "    nan_count = X[col].isna().sum()\n",
    "    if nan_count > 0:\n",
    "        print(f\"- {col}: {nan_count} NaNs\")\n",
    "\n",
    "# Fill missing values appropriately for each column\n",
    "for col in X.columns:\n",
    "    if X[col].isna().sum() > 0:\n",
    "        # For numeric columns, use median\n",
    "        X[col] = X[col].fillna(X[col].median())\n",
    "\n",
    "# Also check target variable\n",
    "if y.isna().sum() > 0:\n",
    "    print(f\"Target has {y.isna().sum()} NaN values, filling with median\")\n",
    "    y = y.fillna(y.median())\n",
    "\n",
    "# Verify all NaNs are fixed\n",
    "print(f\"Remaining NaN values in X: {X.isna().sum().sum()}\")\n",
    "print(f\"Remaining NaN values in y: {y.isna().sum()}\")\n",
    "\n",
    "# Create new train-test split with cleaned data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Testing set size: {X_test.shape[0]}\")\n",
    "\n",
    "# Set up models to try\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge Regression': Ridge(),\n",
    "    'Lasso Regression': Lasso(),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "    'XGBoost': XGBRegressor(n_estimators=100, random_state=42),\n",
    "    'LightGBM': LGBMRegressor(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Function to evaluate models\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    return mae, rmse, r2\n",
    "\n",
    "# Cross-validation for more robust evaluation\n",
    "results = {}\n",
    "cv_results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    mae, rmse, r2 = evaluate_model(model, X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    # 5-fold cross-validation for RMSE\n",
    "    cv_scores = -cross_val_score(model, X, y, cv=5, scoring='neg_root_mean_squared_error')\n",
    "    \n",
    "    results[name] = {'MAE': mae, 'RMSE': rmse, 'R²': r2}\n",
    "    cv_results[name] = {'Mean RMSE': cv_scores.mean(), 'Std RMSE': cv_scores.std()}\n",
    "    \n",
    "    print(f\"{name} - MAE: {mae:.4f}, RMSE: {rmse:.4f}, R²: {r2:.4f}, CV RMSE: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "\n",
    "# Convert results to DataFrames for better visualization\n",
    "results_df = pd.DataFrame(results).T\n",
    "cv_results_df = pd.DataFrame(cv_results).T\n",
    "\n",
    "print(\"\\nTest Results:\")\n",
    "print(results_df.sort_values('RMSE'))\n",
    "\n",
    "print(\"\\nCross-Validation Results:\")\n",
    "print(cv_results_df.sort_values('Mean RMSE'))\n",
    "\n",
    "# Visualize model performance\n",
    "plt.figure(figsize=(12, 6))\n",
    "results_df['RMSE'].sort_values().plot(kind='bar')\n",
    "plt.title('RMSE by Model')\n",
    "plt.ylabel('RMSE (lower is better)')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Select the best performing model based on CV results\n",
    "best_model_name = cv_results_df.sort_values('Mean RMSE').index[0]\n",
    "print(f\"\\nBest model based on cross-validation: {best_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb33df1b",
   "metadata": {},
   "source": [
    "## 6. Hyperparameter Optimization\n",
    "\n",
    "Fine-tuning the best performing model to maximize prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cf8c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Optimization\n",
    "print(f\"Optimizing hyperparameters for {best_model_name}...\")\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "# Define hyperparameter grids for each model type\n",
    "# You may need to adjust these based on your selected best model\n",
    "param_grids = {\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'min_samples_split': [2, 5]\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'colsample_bytree': [0.7, 0.8, 0.9]\n",
    "    },\n",
    "    'LightGBM': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'num_leaves': [31, 50, 70]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Get the appropriate parameter grid\n",
    "if best_model_name in param_grids:\n",
    "    param_grid = param_grids[best_model_name]\n",
    "    \n",
    "    # Use RandomizedSearchCV for efficiency\n",
    "    random_search = RandomizedSearchCV(\n",
    "        models[best_model_name], \n",
    "        param_distributions=param_grid,\n",
    "        n_iter=20,  # Number of parameter settings sampled\n",
    "        cv=5,\n",
    "        scoring='neg_root_mean_squared_error',\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Fit the random search\n",
    "    random_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Print best parameters and score\n",
    "    print(f\"Best parameters: {random_search.best_params_}\")\n",
    "    print(f\"Best RMSE: {-random_search.best_score_:.4f}\")\n",
    "    \n",
    "    # Create the optimized model\n",
    "    best_model = random_search.best_estimator_\n",
    "else:\n",
    "    print(f\"No parameter grid defined for {best_model_name}. Using default model.\")\n",
    "    best_model = models[best_model_name]\n",
    "\n",
    "# Final evaluation with the best model\n",
    "y_pred = best_model.predict(X_test)\n",
    "final_mae = mean_absolute_error(y_test, y_pred)\n",
    "final_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "final_r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nFinal model performance:\")\n",
    "print(f\"MAE: {final_mae:.4f}\")\n",
    "print(f\"RMSE: {final_rmse:.4f}\")\n",
    "print(f\"R²: {final_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa2c93b",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation and Interpretation\n",
    "\n",
    "Assessing model performance, analyzing prediction errors, and identifying the most important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76df36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation and Interpretation\n",
    "print(\"Evaluating final model...\")\n",
    "\n",
    "# Visualize actual vs predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "plt.xlabel('Actual Magnitude')\n",
    "plt.ylabel('Predicted Magnitude')\n",
    "plt.title('Actual vs Predicted Earthquake Magnitude')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot residuals\n",
    "residuals = y_test - y_pred\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_pred, residuals, alpha=0.5)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Predicted Magnitude')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residual Plot')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze residual distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(residuals, kde=True)\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Residuals')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Feature importance (for tree-based models)\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    # Create DataFrame of feature importances\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': X_train.columns,\n",
    "        'Importance': best_model.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    # Visualize feature importances\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='Importance', y='Feature', data=feature_importance.head(15))\n",
    "    plt.title('Top 15 Feature Importances')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Top 10 most important features:\")\n",
    "    print(feature_importance.head(10))\n",
    "\n",
    "# Save the model\n",
    "import joblib\n",
    "joblib.dump(best_model, 'earthquake_magnitude_model.pkl')\n",
    "print(\"Model saved as 'earthquake_magnitude_model.pkl'\")\n",
    "\n",
    "# Save the scaler for later use\n",
    "joblib.dump(scaler, 'earthquake_scaler.pkl')\n",
    "print(\"Scaler saved as 'earthquake_scaler.pkl'\")\n",
    "\n",
    "# Also save the clean dataset with original coordinates for unsupervised learning\n",
    "clean_df.to_csv('clean_earthquake_data.csv', index=False)\n",
    "print(\"Clean data with original coordinates saved as 'clean_earthquake_data.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a31152",
   "metadata": {},
   "source": [
    "## 8. Conclusion and Next Steps\n",
    "\n",
    "The model predicts earthquake magnitudes in Turkey with reasonable accuracy. The processed data has been saved for further unsupervised learning analysis and UI development.\n",
    "\n",
    "Next steps:\n",
    "1. Develop clustering models in unsupervised.ipynb\n",
    "2. Create an interactive UI application\n",
    "3. Document findings in the project README"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
