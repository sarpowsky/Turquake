{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "708e6cfa",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Initial Exploration\n",
    "\n",
    "In this section, we'll load the Turkish earthquake dataset from AFAD (Disaster and Emergency Management Presidency) and perform initial exploratory analysis. AFAD is Turkey's official disaster management authority, providing authoritative seismic data.\n",
    "\n",
    "Our dataset contains earthquake records with magnitude >4.0, which represents seismically significant events that can potentially cause damage to structures. Lower magnitude events (2.0-3.9) are more numerous but less impactful, which is why our analysis focuses on events ≥4.0. The dataset includes:\n",
    "\n",
    "- **Geographic coordinates**: Longitude and Latitude in decimal degrees using the WGS84 reference system\n",
    "- **Earthquake characteristics**: \n",
    "  - Magnitude (Richter scale/moment magnitude)\n",
    "  - Depth (km beneath surface)\n",
    "  - Type (fault mechanism classifications)\n",
    "- **Temporal information**: Date and time stamps for chronological analysis\n",
    "- **Location descriptions**: Named regions/cities for geographical context\n",
    "- **Fault line data**: Vector data of Turkey's active fault systems with importance classifications\n",
    "\n",
    "The fault line data is particularly valuable as Turkey sits on multiple active fault systems, primarily the North Anatolian Fault (NAF) and East Anatolian Fault (EAF), which are responsible for most major earthquakes in the region.\n",
    "\n",
    "We'll establish an organized directory structure (maps/, models/, produced_data/) to systematically store our analytical outputs, visualizations, and trained models for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a335ec9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of fault lines: 926\n",
      "Available properties: ['fid', 'RATE', 'CONF', 'SENS1', 'SENS2', 'UPSIDE', 'ZONE_NAME', 'FAULT_NAME', 'AUTH', 'PARM', 'TEXT', 'FAULT_ID', 'importance', 'geometry']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Depth</th>\n",
       "      <th>Type</th>\n",
       "      <th>Magnitude</th>\n",
       "      <th>Location</th>\n",
       "      <th>EventID</th>\n",
       "      <th>TimeName</th>\n",
       "      <th>TypeName</th>\n",
       "      <th>MagnitudeName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12/05/2025 23:30:12</td>\n",
       "      <td>25.7561</td>\n",
       "      <td>36.7292</td>\n",
       "      <td>7.00</td>\n",
       "      <td>MW</td>\n",
       "      <td>4.9</td>\n",
       "      <td>Ege Denizi</td>\n",
       "      <td>661069</td>\n",
       "      <td>AFAD</td>\n",
       "      <td>AFAD</td>\n",
       "      <td>AFAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12/05/2025 09:26:26</td>\n",
       "      <td>35.1944</td>\n",
       "      <td>40.8211</td>\n",
       "      <td>19.00</td>\n",
       "      <td>MW</td>\n",
       "      <td>4.2</td>\n",
       "      <td>Gümüşhacıköy (Amasya)</td>\n",
       "      <td>661003</td>\n",
       "      <td>AFAD</td>\n",
       "      <td>AFAD</td>\n",
       "      <td>AFAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12/05/2025 09:23:55</td>\n",
       "      <td>35.1919</td>\n",
       "      <td>40.8150</td>\n",
       "      <td>10.07</td>\n",
       "      <td>MW</td>\n",
       "      <td>4.4</td>\n",
       "      <td>Gümüşhacıköy (Amasya)</td>\n",
       "      <td>661002</td>\n",
       "      <td>AFAD</td>\n",
       "      <td>AFAD</td>\n",
       "      <td>AFAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>03/05/2025 23:53:41</td>\n",
       "      <td>28.9431</td>\n",
       "      <td>39.2400</td>\n",
       "      <td>12.55</td>\n",
       "      <td>ML</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Simav (Kütahya)</td>\n",
       "      <td>660145</td>\n",
       "      <td>AFAD</td>\n",
       "      <td>AFAD</td>\n",
       "      <td>AFAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>02/05/2025 12:51:15</td>\n",
       "      <td>28.9989</td>\n",
       "      <td>39.2233</td>\n",
       "      <td>12.55</td>\n",
       "      <td>MW</td>\n",
       "      <td>4.2</td>\n",
       "      <td>Simav (Kütahya)</td>\n",
       "      <td>659964</td>\n",
       "      <td>AFAD</td>\n",
       "      <td>AFAD</td>\n",
       "      <td>AFAD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Date  Longitude  Latitude  Depth Type  Magnitude  \\\n",
       "0  12/05/2025 23:30:12    25.7561   36.7292   7.00   MW        4.9   \n",
       "1  12/05/2025 09:26:26    35.1944   40.8211  19.00   MW        4.2   \n",
       "2  12/05/2025 09:23:55    35.1919   40.8150  10.07   MW        4.4   \n",
       "3  03/05/2025 23:53:41    28.9431   39.2400  12.55   ML        4.0   \n",
       "4  02/05/2025 12:51:15    28.9989   39.2233  12.55   MW        4.2   \n",
       "\n",
       "                Location  EventID TimeName TypeName MagnitudeName  \n",
       "0             Ege Denizi   661069     AFAD     AFAD          AFAD  \n",
       "1  Gümüşhacıköy (Amasya)   661003     AFAD     AFAD          AFAD  \n",
       "2  Gümüşhacıköy (Amasya)   661002     AFAD     AFAD          AFAD  \n",
       "3        Simav (Kütahya)   660145     AFAD     AFAD          AFAD  \n",
       "4        Simav (Kütahya)   659964     AFAD     AFAD          AFAD  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "from datetime import datetime\n",
    "import plotly.express as px\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, LineString\n",
    "import math\n",
    "import os\n",
    "\n",
    "# Set visualization settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('viridis')\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Load the earthquake dataset\n",
    "earthquake_df = pd.read_csv('data\\\\earthquake_data.csv')\n",
    "\n",
    "# Load fault line data\n",
    "fault_gdf = gpd.read_file('data\\\\tr_faults_imp.geojson')\n",
    "print(f\"Number of fault lines: {len(fault_gdf)}\")\n",
    "print(f\"Available properties: {fault_gdf.columns.tolist()}\")\n",
    "\n",
    "# Display first few rows to understand the structure\n",
    "earthquake_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1471f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (11166, 11)\n",
      "Number of earthquakes: 11166\n",
      "\n",
      "Data types:\n",
      "Date              object\n",
      "Longitude        float64\n",
      "Latitude         float64\n",
      "Depth            float64\n",
      "Type              object\n",
      "Magnitude        float64\n",
      "Location          object\n",
      "EventID            int64\n",
      "TimeName          object\n",
      "TypeName          object\n",
      "MagnitudeName     object\n",
      "dtype: object\n",
      "\n",
      "Missing values:\n",
      "Date             0\n",
      "Longitude        0\n",
      "Latitude         0\n",
      "Depth            0\n",
      "Type             0\n",
      "Magnitude        0\n",
      "Location         0\n",
      "EventID          0\n",
      "TimeName         0\n",
      "TypeName         0\n",
      "MagnitudeName    0\n",
      "dtype: int64\n",
      "\n",
      "Basic statistics:\n",
      "Created output directories: maps, models, produced_data\n"
     ]
    }
   ],
   "source": [
    "# Basic information about the dataset\n",
    "print(f\"Dataset shape: {earthquake_df.shape}\")\n",
    "print(f\"Number of earthquakes: {len(earthquake_df)}\")\n",
    "print(\"\\nData types:\")\n",
    "print(earthquake_df.dtypes)\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values:\")\n",
    "print(earthquake_df.isnull().sum())\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\nBasic statistics:\")\n",
    "earthquake_df.describe()\n",
    "\n",
    "os.makedirs(\"maps\", exist_ok=True)\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "os.makedirs(\"produced_data\", exist_ok=True)\n",
    "\n",
    "print(\"Created output directories: maps, models, produced_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9213cf9d",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis (EDA)\n",
    "\n",
    "### 2.1 Temporal Feature Creation\n",
    "\n",
    "Converting raw datetime information into structured temporal features enables detection of cyclical patterns and long-term trends in seismic activity. This process, known as datetime feature engineering, is essential for time series analysis of earthquake occurrences.\n",
    "\n",
    "We'll extract:\n",
    "- **Calendar components**: Year, month, day, and day of week from timestamps\n",
    "- **Seasonal classifications**: Winter (Dec-Feb), Spring (Mar-May), Summer (Jun-Aug), Fall (Sep-Nov)\n",
    "\n",
    "These derived temporal features allow us to:\n",
    "- Analyze periodicity in seismic activity (potential seasonal correlations)\n",
    "- Detect long-term increasing/decreasing trends in earthquake frequency\n",
    "- Identify anomalous periods with unusually high/low activity\n",
    "- Prepare time-based features for machine learning models\n",
    "\n",
    "While earthquakes are primarily driven by tectonic forces, seasonal factors like groundwater variations, reservoir levels, and even atmospheric pressure changes have been hypothesized to influence triggering mechanisms for earthquakes that are already near their critical stress threshold. These temporal features will help us investigate such potential correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a506fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dates that couldn't be parsed: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Depth</th>\n",
       "      <th>Type</th>\n",
       "      <th>Magnitude</th>\n",
       "      <th>Location</th>\n",
       "      <th>EventID</th>\n",
       "      <th>TimeName</th>\n",
       "      <th>TypeName</th>\n",
       "      <th>MagnitudeName</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>Season</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-05-12 23:30:12</td>\n",
       "      <td>25.7561</td>\n",
       "      <td>36.7292</td>\n",
       "      <td>7.00</td>\n",
       "      <td>MW</td>\n",
       "      <td>4.9</td>\n",
       "      <td>Ege Denizi</td>\n",
       "      <td>661069</td>\n",
       "      <td>AFAD</td>\n",
       "      <td>AFAD</td>\n",
       "      <td>AFAD</td>\n",
       "      <td>2025</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>Spring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-05-12 09:26:26</td>\n",
       "      <td>35.1944</td>\n",
       "      <td>40.8211</td>\n",
       "      <td>19.00</td>\n",
       "      <td>MW</td>\n",
       "      <td>4.2</td>\n",
       "      <td>Gümüşhacıköy (Amasya)</td>\n",
       "      <td>661003</td>\n",
       "      <td>AFAD</td>\n",
       "      <td>AFAD</td>\n",
       "      <td>AFAD</td>\n",
       "      <td>2025</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>Spring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-05-12 09:23:55</td>\n",
       "      <td>35.1919</td>\n",
       "      <td>40.8150</td>\n",
       "      <td>10.07</td>\n",
       "      <td>MW</td>\n",
       "      <td>4.4</td>\n",
       "      <td>Gümüşhacıköy (Amasya)</td>\n",
       "      <td>661002</td>\n",
       "      <td>AFAD</td>\n",
       "      <td>AFAD</td>\n",
       "      <td>AFAD</td>\n",
       "      <td>2025</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>Spring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-05-03 23:53:41</td>\n",
       "      <td>28.9431</td>\n",
       "      <td>39.2400</td>\n",
       "      <td>12.55</td>\n",
       "      <td>ML</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Simav (Kütahya)</td>\n",
       "      <td>660145</td>\n",
       "      <td>AFAD</td>\n",
       "      <td>AFAD</td>\n",
       "      <td>AFAD</td>\n",
       "      <td>2025</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>Spring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-05-02 12:51:15</td>\n",
       "      <td>28.9989</td>\n",
       "      <td>39.2233</td>\n",
       "      <td>12.55</td>\n",
       "      <td>MW</td>\n",
       "      <td>4.2</td>\n",
       "      <td>Simav (Kütahya)</td>\n",
       "      <td>659964</td>\n",
       "      <td>AFAD</td>\n",
       "      <td>AFAD</td>\n",
       "      <td>AFAD</td>\n",
       "      <td>2025</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>Spring</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Date  Longitude  Latitude  Depth Type  Magnitude  \\\n",
       "0 2025-05-12 23:30:12    25.7561   36.7292   7.00   MW        4.9   \n",
       "1 2025-05-12 09:26:26    35.1944   40.8211  19.00   MW        4.2   \n",
       "2 2025-05-12 09:23:55    35.1919   40.8150  10.07   MW        4.4   \n",
       "3 2025-05-03 23:53:41    28.9431   39.2400  12.55   ML        4.0   \n",
       "4 2025-05-02 12:51:15    28.9989   39.2233  12.55   MW        4.2   \n",
       "\n",
       "                Location  EventID TimeName TypeName MagnitudeName  Year  \\\n",
       "0             Ege Denizi   661069     AFAD     AFAD          AFAD  2025   \n",
       "1  Gümüşhacıköy (Amasya)   661003     AFAD     AFAD          AFAD  2025   \n",
       "2  Gümüşhacıköy (Amasya)   661002     AFAD     AFAD          AFAD  2025   \n",
       "3        Simav (Kütahya)   660145     AFAD     AFAD          AFAD  2025   \n",
       "4        Simav (Kütahya)   659964     AFAD     AFAD          AFAD  2025   \n",
       "\n",
       "   Month  Day  DayOfWeek  Season  \n",
       "0      5   12          0  Spring  \n",
       "1      5   12          0  Spring  \n",
       "2      5   12          0  Spring  \n",
       "3      5    3          5  Spring  \n",
       "4      5    2          4  Spring  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert Date column to datetime format with explicit format\n",
    "earthquake_df['Date'] = pd.to_datetime(earthquake_df['Date'], format=\"%d/%m/%Y %H:%M:%S\", errors='coerce')\n",
    "\n",
    "# Check if any dates couldn't be parsed\n",
    "null_dates = earthquake_df['Date'].isnull().sum()\n",
    "print(f\"Number of dates that couldn't be parsed: {null_dates}\")\n",
    "\n",
    "# If we have null dates, we can try alternate formats\n",
    "if null_dates > 0:\n",
    "    print(\"Trying alternative date formats...\")\n",
    "    # Try another common format\n",
    "    earthquake_df['Date'] = pd.to_datetime(earthquake_df['Date'], format=\"%d-%m-%Y %H:%M:%S\", errors='coerce')\n",
    "    # If still having issues, try auto-detection with dayfirst=True\n",
    "    if earthquake_df['Date'].isnull().sum() > 0:\n",
    "        earthquake_df['Date'] = pd.to_datetime(earthquake_df['Date'], dayfirst=True, errors='coerce')\n",
    "    \n",
    "    print(f\"Remaining null dates after fixes: {earthquake_df['Date'].isnull().sum()}\")\n",
    "\n",
    "# Create additional time-based features\n",
    "earthquake_df['Year'] = earthquake_df['Date'].dt.year\n",
    "earthquake_df['Month'] = earthquake_df['Date'].dt.month\n",
    "earthquake_df['Day'] = earthquake_df['Date'].dt.day\n",
    "earthquake_df['DayOfWeek'] = earthquake_df['Date'].dt.dayofweek\n",
    "earthquake_df['Season'] = earthquake_df['Month'].apply(lambda x: \n",
    "                                                     'Winter' if x in [12, 1, 2] else\n",
    "                                                     'Spring' if x in [3, 4, 5] else\n",
    "                                                     'Summer' if x in [6, 7, 8] else\n",
    "                                                     'Fall')\n",
    "\n",
    "# Display the updated dataframe\n",
    "earthquake_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0e7d30",
   "metadata": {},
   "source": [
    "### 2.2 Geographic Visualization\n",
    "\n",
    "Creating geographic visualizations is fundamental to understanding the spatial distribution of seismic activity across Turkey's complex tectonic landscape. Interactive maps will reveal patterns that tabular data cannot effectively communicate.\n",
    "\n",
    "We'll generate:\n",
    "\n",
    "1. **Earthquake density heatmaps** using kernel density estimation (KDE) to identify hotspots of seismic activity\n",
    "2. **Magnitude-differentiated markers** for strong earthquakes (magnitude > 6.0), which represent the most destructive events with exponentially higher energy release (~32× more energy per +1.0 magnitude increase)\n",
    "3. **Fault line overlays** with importance classification, visualizing Turkey's major fault systems like the North Anatolian Fault (NAF) and East Anatolian Fault (EAF)\n",
    "\n",
    "These visualizations illuminate the spatial relationship between:\n",
    "- Earthquake epicenters and known fault lines\n",
    "- Magnitude distribution across different regions\n",
    "- Clustering patterns of seismic events\n",
    "\n",
    "Turkey's position at the boundary of the Anatolian, Arabian, and Eurasian tectonic plates creates a complex seismic landscape. Effective visualization of this complexity is crucial for both scientific understanding and risk assessment applications. The interactive maps will allow exploration of these relationships from multiple perspectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d86924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First check and clean coordinate data\n",
    "print(\"Coordinate ranges before cleaning:\")\n",
    "print(f\"Longitude: {earthquake_df['Longitude'].min()} to {earthquake_df['Longitude'].max()}\")\n",
    "print(f\"Latitude: {earthquake_df['Latitude'].min()} to {earthquake_df['Latitude'].max()}\")\n",
    "\n",
    "# Filter out any extreme outliers (coordinates that are clearly wrong)\n",
    "# Turkey coordinates should be roughly: Longitude 26-45 E, Latitude 36-42 N\n",
    "valid_coords = (\n",
    "    (earthquake_df['Longitude'] >= 25) & \n",
    "    (earthquake_df['Longitude'] <= 45) & \n",
    "    (earthquake_df['Latitude'] >= 35) & \n",
    "    (earthquake_df['Latitude'] <= 43)\n",
    ")\n",
    "\n",
    "# Filter the dataframe to keep only valid coordinates\n",
    "clean_df = earthquake_df[valid_coords].copy()\n",
    "outliers_removed = len(earthquake_df) - len(clean_df)\n",
    "print(f\"Removed {outliers_removed} records with coordinates outside Turkey's boundaries\")\n",
    "\n",
    "print(\"Coordinate ranges after cleaning:\")\n",
    "print(f\"Longitude: {clean_df['Longitude'].min()} to {clean_df['Longitude'].max()}\")\n",
    "print(f\"Latitude: {clean_df['Latitude'].min()} to {clean_df['Latitude'].max()}\")\n",
    "\n",
    "# Create a map centered on Turkey\n",
    "# Create a map centered on Turkey\n",
    "turkey_map = folium.Map(location=[38.5, 35.5], zoom_start=6)\n",
    "\n",
    "# Sample points for better visualization performance\n",
    "sample_df = clean_df.sample(min(2000, len(clean_df)))\n",
    "\n",
    "# Create a heatmap layer with cleaned data\n",
    "heat_data = [[row['Latitude'], row['Longitude']] for index, row in sample_df.iterrows()]\n",
    "HeatMap(heat_data, radius=8, gradient={'0.4': 'blue', '0.6': 'cyan', '0.8': 'yellow', '1.0': 'red'}).add_to(turkey_map)\n",
    "\n",
    "# Add markers for strong earthquakes (magnitude > 6)\n",
    "for idx, row in clean_df[clean_df['Magnitude'] > 6].iterrows():\n",
    "    # Create enhanced popup content with styled HTML\n",
    "    popup_content = f\"\"\"\n",
    "    <div style=\"font-family: Arial; min-width: 200px;\">\n",
    "        <h4 style=\"margin-bottom: 5px; color: #d32f2f;\">Earthquake Details</h4>\n",
    "        <b>Magnitude:</b> {row['Magnitude']:.1f}<br>\n",
    "        <b>Depth:</b> {row['Depth']:.1f} km<br>\n",
    "        <b>Date:</b> {row['Date']}<br>\n",
    "        <b>Location:</b> {row['Location']}<br>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add type information if available\n",
    "    if 'Type' in row:\n",
    "        popup_content += f\"<b>Type:</b> {row['Type']}<br>\"\n",
    "    \n",
    "    # Add additional information if available  \n",
    "    if 'TypeName' in row and not pd.isna(row['TypeName']):\n",
    "        popup_content += f\"<b>Type Description:</b> {row['TypeName']}<br>\"\n",
    "        \n",
    "    # Add EventID if available\n",
    "    if 'EventID' in row and not pd.isna(row['EventID']):\n",
    "        popup_content += f\"<b>Event ID:</b> {row['EventID']}<br>\"\n",
    "    \n",
    "    popup_content += \"</div>\"\n",
    "    \n",
    "    folium.CircleMarker(\n",
    "        location=[row['Latitude'], row['Longitude']],\n",
    "        radius=row['Magnitude'] * 1.5,\n",
    "        color='red',\n",
    "        fill=True,\n",
    "        fill_color='red',\n",
    "        fill_opacity=0.7,\n",
    "        popup=folium.Popup(popup_content, max_width=300),\n",
    "    ).add_to(turkey_map)\n",
    "\n",
    "# Add fault lines to the map\n",
    "def add_faults_to_map(map_obj, fault_gdf, importance_threshold=0):\n",
    "    # Filter faults by importance if desired\n",
    "    if importance_threshold > 0:\n",
    "        fault_data = fault_gdf[fault_gdf['importance'] >= importance_threshold]\n",
    "    else:\n",
    "        fault_data = fault_gdf\n",
    "    \n",
    "    # Color by importance\n",
    "    def style_function(feature):\n",
    "        importance = feature['properties']['importance']\n",
    "        color = '#FF0000' if importance >= 4 else '#FFA500' if importance >= 3 else '#FFFF00'\n",
    "        return {\n",
    "            'color': color,\n",
    "            'weight': importance * 0.5,  # Thicker lines for more important faults\n",
    "            'opacity': 0.7\n",
    "        }\n",
    "    \n",
    "    # Add GeoJSON to map\n",
    "    folium.GeoJson(\n",
    "        fault_data,\n",
    "        name='Fault Lines',\n",
    "        style_function=style_function,\n",
    "        tooltip=folium.GeoJsonTooltip(fields=['FAULT_NAME', 'importance']),\n",
    "    ).add_to(map_obj)\n",
    "    \n",
    "    return map_obj\n",
    "\n",
    "# Add fault lines to the map\n",
    "turkey_map = add_faults_to_map(turkey_map, fault_gdf, importance_threshold=3)\n",
    "\n",
    "# Add a tile layer for better visualization\n",
    "folium.TileLayer('cartodbpositron').add_to(turkey_map)\n",
    "\n",
    "# Add a legend for the map\n",
    "legend_html = '''\n",
    "<div style=\"position: fixed; bottom: 20px; right: 20px; width: 200px; height: auto; \n",
    "    background-color: white; border:2px solid grey; z-index:9999; font-size:12px;\n",
    "    padding: 10px; border-radius: 5px;\">\n",
    "    <p><b>Earthquake Map Legend</b></p>\n",
    "    <p><i class=\"fa fa-circle\" style=\"color:red\"></i> Magnitude > 6</p>\n",
    "    <div style=\"margin-top:5px;\">\n",
    "      <p><b>Heatmap Intensity:</b></p>\n",
    "      <div style=\"display:inline-block; height:15px; width:30px; background:blue;\"></div>\n",
    "      <div style=\"display:inline-block; height:15px; width:30px; background:cyan;\"></div>\n",
    "      <div style=\"display:inline-block; height:15px; width:30px; background:yellow;\"></div>\n",
    "      <div style=\"display:inline-block; height:15px; width:30px; background:red;\"></div>\n",
    "      <p style=\"font-size:10px;\">Low &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; High</p>\n",
    "    </div>\n",
    "    <div style=\"margin-top:5px;\">\n",
    "      <p><b>Fault Line Importance:</b></p>\n",
    "      <p><span style=\"color:#FF0000;\">━━━</span> High (4+)</p>\n",
    "      <p><span style=\"color:#FFA500;\">━━━</span> Medium (3)</p>\n",
    "      <p><span style=\"color:#FFFF00;\">━━━</span> Low (<3)</p>\n",
    "    </div>\n",
    "</div>\n",
    "'''\n",
    "turkey_map.get_root().html.add_child(folium.Element(legend_html))\n",
    "\n",
    "# Save map to HTML file to view it\n",
    "turkey_map.save('maps/earthquake_map.html')\n",
    "\n",
    "# Display in notebook if you have ipywidgets installed\n",
    "# from IPython.display import display\n",
    "# display(turkey_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ae904e",
   "metadata": {},
   "source": [
    "### 2.3 Temporal Analysis\n",
    "\n",
    "Analyzing earthquake patterns across different timescales reveals both long-term trends and cyclical behaviors in seismic activity. This multiscale temporal analysis is essential for understanding the time-variant nature of earthquake occurrences.\n",
    "\n",
    "We'll examine:\n",
    "\n",
    "1. **Yearly frequency distribution**: Plotted as time series and histogram to identify long-term trends and potential periods of increased activity that might correlate with major tectonic adjustments\n",
    "   \n",
    "2. **Seasonal variation**: Aggregating earthquakes by season to detect potential patterns related to environmental factors like precipitation, groundwater levels, or reservoir loading\n",
    "   \n",
    "3. **Monthly patterns**: Finer-grained analysis to identify monthly anomalies and verify any apparent seasonal effects\n",
    "\n",
    "The statistical significance of any observed temporal patterns will be evaluated, as distinguishing between random fluctuations and meaningful patterns is critical in seismological analysis. While the primary drivers of earthquakes are tectonic stresses that accumulate over decades or centuries, short-term variations in frequency can provide insights into triggering mechanisms.\n",
    "\n",
    "Understanding these temporal patterns contributes to both the scientific knowledge base and potentially to improved forecasting methodologies, though it's important to note that precise earthquake prediction remains beyond current scientific capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35e3dc9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clean_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Use the cleaned dataframe for temporal analysis\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Yearly earthquake frequency\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m yearly_counts = \u001b[43mclean_df\u001b[49m.groupby(\u001b[33m'\u001b[39m\u001b[33mYear\u001b[39m\u001b[33m'\u001b[39m).size()\n\u001b[32m      5\u001b[39m plt.figure(figsize=(\u001b[32m14\u001b[39m, \u001b[32m6\u001b[39m))\n\u001b[32m      6\u001b[39m yearly_counts.plot(kind=\u001b[33m'\u001b[39m\u001b[33mbar\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'clean_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Use the cleaned dataframe for temporal analysis\n",
    "# Yearly earthquake frequency\n",
    "yearly_counts = clean_df.groupby('Year').size()\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "yearly_counts.plot(kind='bar')\n",
    "plt.title('Yearly Earthquake Frequency')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of Earthquakes')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Seasonal patterns\n",
    "seasonal_counts = clean_df.groupby('Season').size()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "seasonal_counts.plot(kind='pie', autopct='%1.1f%%')\n",
    "plt.title('Seasonal Distribution of Earthquakes')\n",
    "plt.ylabel('')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Monthly patterns\n",
    "monthly_counts = clean_df.groupby('Month').size()\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "monthly_counts.plot(kind='bar')\n",
    "plt.title('Monthly Earthquake Frequency')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Number of Earthquakes')\n",
    "plt.xticks(range(12), ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800470e6",
   "metadata": {},
   "source": [
    "### 2.4 Magnitude and Depth Analysis\n",
    "\n",
    "Examining the distribution of earthquake magnitudes and depths provides critical insight into the mechanical properties of Turkey's seismic system. These parameters directly relate to the physics of earthquake generation and propagation.\n",
    "\n",
    "Our analysis will focus on:\n",
    "\n",
    "1. **Magnitude distribution**: Visualizing the Gutenberg-Richter relationship, which typically follows a power-law distribution where frequency decreases exponentially as magnitude increases. Deviations from this pattern may indicate catalog incompleteness or unusual seismic characteristics.\n",
    "\n",
    "2. **Depth distribution**: Analyzing hypocenter depths to understand the vertical structure of seismicity. In Turkey, most earthquakes occur in the upper 30km of crust, but deeper events may indicate different tectonic processes or subduction-related activity.\n",
    "\n",
    "3. **Magnitude-depth relationships**: Investigating correlations that might reveal whether larger earthquakes systematically occur at specific depths, which relates to the mechanical properties of the crust and stress distribution.\n",
    "\n",
    "The magnitude scale used (Richter/local magnitude or moment magnitude) is logarithmic, meaning a magnitude 6.0 earthquake releases approximately 32 times more energy than a magnitude 5.0. This exponential relationship dramatically impacts potential damage and is crucial for risk assessment.\n",
    "\n",
    "Depth analysis is equally important, as shallow earthquakes typically cause more surface damage than deeper events of equivalent magnitude due to reduced wave attenuation and higher peak ground accelerations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0816c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magnitude distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(clean_df['Magnitude'], bins=30, kde=True)\n",
    "plt.title('Distribution of Earthquake Magnitudes')\n",
    "plt.xlabel('Magnitude')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axvline(clean_df['Magnitude'].mean(), color='red', linestyle='--', label=f'Mean: {clean_df[\"Magnitude\"].mean():.2f}')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Depth distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(clean_df['Depth'], bins=30, kde=True)\n",
    "plt.title('Distribution of Earthquake Depths')\n",
    "plt.xlabel('Depth (km)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axvline(clean_df['Depth'].mean(), color='red', linestyle='--', label=f'Mean: {clean_df[\"Depth\"].mean():.2f}')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Relationship between magnitude and depth\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(x='Depth', y='Magnitude', data=clean_df, alpha=0.6)\n",
    "plt.title('Relationship Between Earthquake Depth and Magnitude')\n",
    "plt.xlabel('Depth (km)')\n",
    "plt.ylabel('Magnitude')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f2d9ac",
   "metadata": {},
   "source": [
    "### 2.5 Correlation Analysis\n",
    "\n",
    "Exploring statistical relationships between numerical features reveals patterns that can improve our understanding of earthquake characteristics and potentially enhance predictive models. Correlation analysis quantifies these relationships and helps identify the most informative variables.\n",
    "\n",
    "We'll create a comprehensive correlation matrix to visualize relationships between:\n",
    "\n",
    "- **Spatial parameters**: Longitude and Latitude correlations may reveal directional trends in seismic activity\n",
    "- **Physical characteristics**: Relationships between Depth and Magnitude might indicate how energy release varies with crustal depth\n",
    "- **Temporal-physical interactions**: Correlations between time variables and earthquake properties could suggest seasonal effects on seismic behavior\n",
    "- **Fault proximity measures**: Correlations between distance to faults and other earthquake parameters\n",
    "\n",
    "The Pearson correlation coefficient (r) will quantify linear relationships, ranging from -1 (perfect negative correlation) to +1 (perfect positive correlation), with 0 indicating no linear relationship. While correlation doesn't necessarily imply causation, strong correlations can indicate physically meaningful relationships worth further investigation.\n",
    "\n",
    "For earthquake data specifically, we'll be cautious about interpreting correlations, as seismic processes involve complex, often non-linear interactions. Nonetheless, these correlation patterns can guide feature selection for our machine learning models and help formulate geophysical hypotheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fecb856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis of numerical columns\n",
    "numerical_cols = clean_df.select_dtypes(include=[np.number]).columns\n",
    "correlation_matrix = clean_df[numerical_cols].corr()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e413279a",
   "metadata": {},
   "source": [
    "### 2.6 Additional Visualizations\n",
    "\n",
    "Further exploration through specialized visualization techniques provides deeper insights into the multidimensional nature of earthquake patterns. These enhanced visualizations go beyond standard plots to reveal complex relationships.\n",
    "\n",
    "We'll implement:\n",
    "\n",
    "1. **Magnitude-encoded geographic scatterplots**: Visualizing earthquake locations with color and size encoding for magnitude, revealing spatial patterns in energy release\n",
    "   \n",
    "2. **Temporal magnitude boxplots**: Analyzing the distribution of magnitudes across years to identify potential temporal trends in earthquake intensity\n",
    "   \n",
    "3. **Depth evolution analysis**: Tracking changes in hypocenter depths over time, which might indicate evolving stress patterns in the crust\n",
    "   \n",
    "4. **3D visualization (longitude × latitude × depth)**: Creating interactive three-dimensional plots that represent the true spatial distribution of earthquakes, including their subsurface positions\n",
    "\n",
    "These advanced visualizations help detect patterns that might be obscured in two-dimensional or non-visual analyses. For example, the 3D visualization allows us to identify potential fault planes in space, as earthquakes often align along these failure surfaces.\n",
    "\n",
    "Interactive elements in these visualizations enable exploration of the dataset from multiple perspectives, facilitating the discovery of relationships that static visualizations might miss. This multifaceted approach is particularly valuable for complex geophysical data where patterns may exist across multiple dimensions simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8756ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geographic distribution by magnitude\n",
    "plt.figure(figsize=(14, 10))\n",
    "scatter = plt.scatter(clean_df['Longitude'], clean_df['Latitude'], \n",
    "                     c=clean_df['Magnitude'], cmap='YlOrRd', \n",
    "                     alpha=0.7, s=clean_df['Magnitude']**2)\n",
    "plt.colorbar(scatter, label='Magnitude')\n",
    "plt.title('Geographic Distribution of Earthquakes by Magnitude')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68364255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magnitude distribution over years (box plot)\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.boxplot(x='Year', y='Magnitude', data=clean_df)\n",
    "plt.title('Magnitude Distribution Over Years')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Magnitude')\n",
    "plt.xticks(rotation=90)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f6dc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depth vs Year analysis\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.boxplot(x='Year', y='Depth', data=clean_df)\n",
    "plt.title('Depth Distribution Over Years')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Depth (km)')\n",
    "plt.xticks(rotation=90)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bc98ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D visualization with Plotly\n",
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter_3d(clean_df.sample(min(3000, len(clean_df))), \n",
    "                   x='Longitude', y='Latitude', z='Depth',\n",
    "                   color='Magnitude', size='Magnitude',\n",
    "                   color_continuous_scale='Viridis',\n",
    "                   title='3D Visualization of Earthquakes')\n",
    "# Ensure proper axis orientation\n",
    "fig.update_layout(scene=dict(\n",
    "    xaxis_title='Longitude',\n",
    "    yaxis_title='Latitude',\n",
    "    zaxis_title='Depth (km)',\n",
    "    # Reverse the depth axis to show deeper earthquakes lower\n",
    "    zaxis=dict(autorange=\"reversed\")\n",
    "))\n",
    "fig.write_html('maps/earthquake_3d.html')  # Save the interactive plot\n",
    "# fig.show()  # Display in notebook if supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ac56e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magnitude frequency plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "counts, bins, _ = plt.hist(clean_df['Magnitude'], bins=30, alpha=0.7)\n",
    "plt.plot(bins[:-1], counts, '-o', color='darkred')\n",
    "plt.title('Frequency Distribution of Earthquake Magnitudes')\n",
    "plt.xlabel('Magnitude')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adafc833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regional magnitude comparison\n",
    "# Extract region from location (assuming format includes region at end)\n",
    "# Modify this based on your actual data format\n",
    "if 'Location' in clean_df.columns:\n",
    "    # Extract the first part of the location as the region\n",
    "    clean_df['Region'] = clean_df['Location'].str.split(',').str[-1].str.strip()\n",
    "    \n",
    "    # Get top 10 regions by earthquake count\n",
    "    top_regions = clean_df['Region'].value_counts().head(10).index\n",
    "    \n",
    "    # Plot magnitude distribution by region\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.boxplot(x='Region', y='Magnitude', data=clean_df[clean_df['Region'].isin(top_regions)])\n",
    "    plt.title('Magnitude Distribution by Top 10 Regions')\n",
    "    plt.xlabel('Region')\n",
    "    plt.ylabel('Magnitude')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecce5ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of earthquake frequency by month and year\n",
    "if len(clean_df) > 0:\n",
    "    # Create pivot table\n",
    "    heatmap_data = pd.pivot_table(\n",
    "        clean_df,\n",
    "        values='Magnitude',\n",
    "        index=clean_df['Date'].dt.year,\n",
    "        columns=clean_df['Date'].dt.month,\n",
    "        aggfunc='count'\n",
    "    )\n",
    "    \n",
    "    plt.figure(figsize=(14, 10))\n",
    "    sns.heatmap(heatmap_data, cmap='YlOrRd', annot=False)\n",
    "    plt.title('Earthquake Frequency by Month and Year')\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Year')\n",
    "    plt.xticks(range(1, 13), ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f86caf",
   "metadata": {},
   "source": [
    "### 2.7 Fault Line Analysis\n",
    "\n",
    "Examining the relationship between earthquakes and fault systems is fundamental to understanding seismic hazard. Faults represent zones of crustal weakness where accumulated stress is released through sudden displacement, generating earthquakes.\n",
    "\n",
    "Our analysis will quantify these relationships by:\n",
    "\n",
    "1. **Calculating minimum distances**: For each earthquake, we'll compute the minimum Euclidean distance to the nearest mapped fault line using spatial algorithms, converting geometric distances to kilometers\n",
    "   \n",
    "2. **Magnitude-proximity analysis**: Investigating whether larger magnitude events tend to occur closer to major fault lines, which would align with theoretical expectations about stress accumulation\n",
    "   \n",
    "3. **Fault importance correlation**: Analyzing whether earthquakes near higher-importance faults (classified by length, slip rate, and historical activity) tend to have different characteristics\n",
    "\n",
    "Turkey's fault systems are particularly complex, with the North Anatolian Fault (NAF) being one of the most active strike-slip faults globally, similar to California's San Andreas Fault. The East Anatolian Fault (EAF) and numerous smaller fault systems create a complex network that controls earthquake distribution.\n",
    "\n",
    "This analysis provides crucial context for seismic hazard assessment, as proximity to active faults is one of the primary factors in earthquake risk. Understanding these spatial relationships helps explain the observed distribution of seismic events and identifies zones of potentially elevated future risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6636c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate distances to fault lines\n",
    "def calc_fault_distance(row, fault_gdf):\n",
    "    point = Point(row['Longitude'], row['Latitude'])\n",
    "    \n",
    "    # Calculate distance to each fault line\n",
    "    distances = []\n",
    "    for idx, fault in fault_gdf.iterrows():\n",
    "        fault_geom = fault.geometry\n",
    "        dist = point.distance(fault_geom)\n",
    "        distances.append((dist, idx))\n",
    "    \n",
    "    # Find the closest fault\n",
    "    closest_dist, closest_idx = min(distances, key=lambda x: x[0])\n",
    "    \n",
    "    # Convert distance to kilometers (approximation)\n",
    "    # 1 degree ≈ 111 km at the equator\n",
    "    dist_km = closest_dist * 111\n",
    "    \n",
    "    # Get fault properties\n",
    "    closest_fault = fault_gdf.iloc[closest_idx]\n",
    "    \n",
    "    return pd.Series({\n",
    "        'distance_to_fault': dist_km,\n",
    "        'nearest_fault_name': closest_fault.get('FAULT_NAME', 'Unknown'),\n",
    "        'nearest_fault_importance': closest_fault.get('importance', 0)\n",
    "    })\n",
    "\n",
    "# Apply to a sample for visualization (full calculation will be done later)\n",
    "sample_size = min(1000, len(clean_df))\n",
    "fault_distance_sample = clean_df.sample(sample_size).apply(\n",
    "    lambda row: calc_fault_distance(row, fault_gdf), axis=1\n",
    ")\n",
    "\n",
    "# Visualize relationship between earthquake magnitude and distance to fault\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(fault_distance_sample['distance_to_fault'], \n",
    "           clean_df.loc[fault_distance_sample.index, 'Magnitude'],\n",
    "           alpha=0.6, c=fault_distance_sample['nearest_fault_importance'], \n",
    "           cmap='viridis')\n",
    "plt.colorbar(label='Fault Importance')\n",
    "plt.xlabel('Distance to Nearest Fault (km)')\n",
    "plt.ylabel('Magnitude')\n",
    "plt.title('Relationship Between Earthquake Magnitude and Distance to Fault')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7d0bbc",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n",
    "\n",
    "This section focuses on preparing our dataset for machine learning by addressing data quality issues that could compromise model performance. Proper preprocessing is essential for reliable predictions, as models can only perform as well as the data they're trained on.\n",
    "\n",
    "We'll implement a systematic preprocessing pipeline:\n",
    "\n",
    "1. **Missing value treatment**: \n",
    "   - Identifying nulls in each column through exploratory statistics\n",
    "   - Applying appropriate imputation strategies based on data distribution (median for numerical features, mode for categorical)\n",
    "   - Documenting imputation decisions to maintain analytical transparency\n",
    "\n",
    "2. **Outlier detection and handling**:\n",
    "   - Implementing the Interquartile Range (IQR) method, which identifies outliers as values below Q1-1.5×IQR or above Q3+1.5×IQR\n",
    "   - Applying domain-specific knowledge to distinguish between true outliers and legitimate extreme values\n",
    "   - Using capping/winsorization rather than removal for magnitude values, as extreme earthquakes contain valuable information\n",
    "\n",
    "3. **Geospatial validation**:\n",
    "   - Verifying coordinates fall within Turkey's geographical boundaries (approx. 25-45°E, 35-43°N)\n",
    "   - Filtering records with clearly erroneous coordinates that would introduce noise\n",
    "\n",
    "4. **Feature scaling**:\n",
    "   - Standardizing features to zero mean and unit variance using StandardScaler\n",
    "   - Ensuring all features contribute proportionally to distance-based algorithms\n",
    "\n",
    "5. **Dataset finalization**:\n",
    "   - Creating a clean, analysis-ready dataset with consistent structure\n",
    "   - Documenting all preprocessing steps for reproducibility\n",
    "\n",
    "These preprocessing steps mitigate the common data quality issues that can bias machine learning models. By addressing these issues systematically, we establish a solid foundation for the subsequent modeling stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e239550c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing Section\n",
    "print(\"Starting data preprocessing...\")\n",
    "\n",
    "# Check for missing values again to confirm\n",
    "missing_values = clean_df.isnull().sum()\n",
    "print(f\"Missing values in each column:\\n{missing_values}\")\n",
    "\n",
    "# Handle missing values\n",
    "# For numerical columns: fill with median\n",
    "numerical_cols = ['Longitude', 'Latitude', 'Depth', 'Magnitude']\n",
    "for col in numerical_cols:\n",
    "    if missing_values[col] > 0:\n",
    "        median_value = clean_df[col].median()\n",
    "        clean_df[col].fillna(median_value, inplace=True)\n",
    "        print(f\"Filled {missing_values[col]} missing values in {col} with median: {median_value}\")\n",
    "\n",
    "# For categorical columns: fill with mode\n",
    "categorical_cols = [col for col in clean_df.columns if col not in numerical_cols \n",
    "                   and col not in ['Date', 'Year', 'Month', 'Day', 'YearMonth']]\n",
    "for col in categorical_cols:\n",
    "    if col in missing_values and missing_values[col] > 0:\n",
    "        mode_value = clean_df[col].mode()[0]\n",
    "        clean_df[col].fillna(mode_value, inplace=True)\n",
    "        print(f\"Filled {missing_values[col]} missing values in {col} with mode: {mode_value}\")\n",
    "\n",
    "# Handle outliers using IQR method for depth and magnitude\n",
    "def handle_outliers(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
    "    print(f\"Found {len(outliers)} outliers in {column}\")\n",
    "    \n",
    "    # Cap outliers instead of removing them\n",
    "    df[column] = np.where(df[column] < lower_bound, lower_bound, df[column])\n",
    "    df[column] = np.where(df[column] > upper_bound, upper_bound, df[column])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply outlier handling to Depth\n",
    "clean_df = handle_outliers(clean_df, 'Depth')\n",
    "\n",
    "# For Magnitude, we may want to keep high values as they're important\n",
    "# But we can still check for potential errors\n",
    "magnitude_outliers = clean_df[clean_df['Magnitude'] > 8.5]\n",
    "print(f\"Extremely high magnitudes (>8.5): {len(magnitude_outliers)}\")\n",
    "if len(magnitude_outliers) > 0:\n",
    "    print(magnitude_outliers[['Date', 'Magnitude', 'Location']])\n",
    "\n",
    "# Standardize coordinates if needed\n",
    "print(\"\\nCoordinate ranges:\")\n",
    "print(f\"Longitude: {clean_df['Longitude'].min()} to {clean_df['Longitude'].max()}\")\n",
    "print(f\"Latitude: {clean_df['Latitude'].min()} to {clean_df['Latitude'].max()}\")\n",
    "\n",
    "# Verify coordinates are in the Turkey region (already done in previous step)\n",
    "# This is now redundant since we've already filtered the coordinates\n",
    "turkey_coords = clean_df[\n",
    "    (clean_df['Longitude'] >= 25) & \n",
    "    (clean_df['Longitude'] <= 45) & \n",
    "    (clean_df['Latitude'] >= 35) & \n",
    "    (clean_df['Latitude'] <= 43)\n",
    "]\n",
    "outside_turkey = len(clean_df) - len(turkey_coords)\n",
    "print(f\"Records potentially outside Turkey region: {outside_turkey}\")\n",
    "\n",
    "# Create a copy of the dataframe for modeling\n",
    "model_df = clean_df.copy()\n",
    "\n",
    "print(\"\\nData preprocessing completed!\")\n",
    "model_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb628dc1",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering\n",
    "\n",
    "Feature engineering transforms raw data into more informative inputs that enhance model performance by incorporating domain knowledge and exposing underlying patterns. For earthquake data, effective feature engineering can significantly improve predictive power by capturing complex geophysical relationships.\n",
    "\n",
    "We'll create several categories of engineered features:\n",
    "\n",
    "1. **Temporal features**: \n",
    "   - **Cyclical encoding**: Converting cyclical time variables (month, day of year) using sine and cosine transformations:\n",
    "     - `sin(2π×month/12)` and `cos(2π×month/12)`\n",
    "     - `sin(2π×day/365)` and `cos(2π×day/365)`\n",
    "   - This preserves the circular nature of time, ensuring December is properly represented as being close to January\n",
    "\n",
    "2. **Geographic features**:\n",
    "   - **Grid-based encoding**: Discretizing Turkey into geographical grid cells to capture regional patterns\n",
    "   - **Regional activity metrics**: Calculating historical earthquake counts and densities within defined regions\n",
    "\n",
    "3. **Historical activity features**:\n",
    "   - **Previous event metrics**: For each earthquake, computing the count, magnitude, and timing of previous events in the same region\n",
    "   - **Time-since-last-event**: Capturing the temporal spacing between sequential earthquakes\n",
    "\n",
    "4. **Distance-based features**:\n",
    "   - **Fault proximity metrics**: Distance to nearest fault, weighted by fault importance\n",
    "   - **Fault density**: Calculating the density of fault lines within defined radius\n",
    "   - **Multiple fault interaction**: Metrics capturing the influence of multiple nearby faults\n",
    "\n",
    "5. **Interaction features**:\n",
    "   - **Feature crosses**: Creating multiplicative combinations of features that might interact (e.g., depth×latitude)\n",
    "   - **Ratio features**: Developing ratios between related features that might have physical significance\n",
    "\n",
    "These engineered features incorporate domain-specific knowledge about earthquake processes, such as the tendency for seismic events to cluster in space and time, the influence of fault systems on earthquake occurrence, and potential seasonal patterns in activity.\n",
    "\n",
    "By transforming raw data into these more informative representations, we provide our models with richer inputs that better capture the underlying physics of earthquake generation, potentially leading to more accurate magnitude predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0aba8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "print(\"Starting feature engineering...\")\n",
    "\n",
    "# Create time-based features\n",
    "model_df['DayOfYear'] = model_df['Date'].dt.dayofyear\n",
    "model_df['WeekOfYear'] = model_df['Date'].dt.isocalendar().week\n",
    "model_df['IsWeekend'] = model_df['DayOfWeek'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "\n",
    "# Encode seasonal information using cyclical encoding\n",
    "model_df['MonthSin'] = np.sin(2 * np.pi * model_df['Month']/12)\n",
    "model_df['MonthCos'] = np.cos(2 * np.pi * model_df['Month']/12)\n",
    "model_df['DayOfYearSin'] = np.sin(2 * np.pi * model_df['DayOfYear']/365)\n",
    "model_df['DayOfYearCos'] = np.cos(2 * np.pi * model_df['DayOfYear']/365)\n",
    "\n",
    "# Create regional activity features\n",
    "# Group by regions and calculate historical earthquake counts\n",
    "# First, create a spatial grid\n",
    "lon_grid = pd.cut(clean_df['Longitude'], bins=10)\n",
    "lat_grid = pd.cut(clean_df['Latitude'], bins=10)\n",
    "clean_df['Grid'] = pd.Series(zip(lon_grid, lat_grid)).astype(str)\n",
    "\n",
    "# For each earthquake, count previous earthquakes in the same grid\n",
    "clean_df = clean_df.sort_values('Date')\n",
    "clean_df['PrevQuakesInGrid'] = clean_df.groupby('Grid').cumcount()\n",
    "\n",
    "# Calculate distances between consecutive earthquakes\n",
    "clean_df['PrevLon'] = clean_df['Longitude'].shift(1)\n",
    "clean_df['PrevLat'] = clean_df['Latitude'].shift(1)\n",
    "\n",
    "# Haversine formula to calculate distance in km\n",
    "from math import radians, sin, cos, sqrt, asin\n",
    "\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    # Convert decimal degrees to radians\n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "    \n",
    "    # Haversine formula\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a))\n",
    "    r = 6371  # Radius of earth in km\n",
    "    return c * r\n",
    "\n",
    "# Apply haversine to calculate distance from previous earthquake\n",
    "clean_df['DistFromPrev'] = clean_df.apply(\n",
    "    lambda x: haversine(x['Longitude'], x['Latitude'], x['PrevLon'], x['PrevLat']) \n",
    "    if not pd.isna(x['PrevLon']) else np.nan, axis=1)\n",
    "\n",
    "# Add distance features to model_df\n",
    "model_df['PrevQuakesInGrid'] = clean_df['PrevQuakesInGrid']\n",
    "model_df['DistFromPrev'] = clean_df['DistFromPrev']\n",
    "model_df['DistFromPrev'].fillna(model_df['DistFromPrev'].median(), inplace=True)\n",
    "\n",
    "# Create feature for time since last earthquake (in days)\n",
    "clean_df['PrevDate'] = clean_df['Date'].shift(1)\n",
    "clean_df['DaysSinceLastQuake'] = (clean_df['Date'] - clean_df['PrevDate']).dt.total_seconds() / (24 * 3600)\n",
    "model_df['DaysSinceLastQuake'] = clean_df['DaysSinceLastQuake']\n",
    "model_df['DaysSinceLastQuake'].fillna(model_df['DaysSinceLastQuake'].median(), inplace=True)\n",
    "\n",
    "# Add historical magnitude information\n",
    "clean_df['PrevMagnitude'] = clean_df['Magnitude'].shift(1)\n",
    "model_df['PrevMagnitude'] = clean_df['PrevMagnitude']\n",
    "model_df['PrevMagnitude'].fillna(model_df['PrevMagnitude'].median(), inplace=True)\n",
    "\n",
    "# Create interaction features\n",
    "model_df['DepthByLat'] = model_df['Depth'] * model_df['Latitude']\n",
    "model_df['DepthByLon'] = model_df['Depth'] * model_df['Longitude']\n",
    "\n",
    "# Add fault-related features - calculate for all data points\n",
    "print(\"Calculating fault-related features...\")\n",
    "fault_features = clean_df.apply(lambda row: calc_fault_distance(row, fault_gdf), axis=1)\n",
    "clean_df = pd.concat([clean_df, fault_features], axis=1)\n",
    "model_df = pd.concat([model_df, fault_features], axis=1)\n",
    "\n",
    "# Calculate fault density in a radius\n",
    "def calc_fault_density(lat, lon, fault_gdf, radius=50):\n",
    "    \"\"\"Calculate fault density within radius (km) of a point\"\"\"\n",
    "    point = Point(lon, lat)\n",
    "    buffer_degrees = radius / 111  # Convert km to approximate degrees\n",
    "    \n",
    "    # Create a buffer around the point\n",
    "    buffer = point.buffer(buffer_degrees)\n",
    "    \n",
    "    # Count intersecting faults and sum their lengths\n",
    "    intersecting_faults = 0\n",
    "    total_length = 0\n",
    "    \n",
    "    for _, fault in fault_gdf.iterrows():\n",
    "        if buffer.intersects(fault.geometry):\n",
    "            intersecting_faults += 1\n",
    "            # Calculate length of intersection\n",
    "            intersection = buffer.intersection(fault.geometry)\n",
    "            total_length += intersection.length * 111  # Convert to km\n",
    "    \n",
    "    return pd.Series({\n",
    "        'fault_count_50km': intersecting_faults,\n",
    "        'fault_length_50km': total_length,\n",
    "        'fault_density': total_length / (math.pi * radius**2) if radius > 0 else 0\n",
    "    })\n",
    "\n",
    "# Calculate fault density for strategic points (grid centers) to avoid heavy computation\n",
    "print(\"Calculating fault density (this may take a while)...\")\n",
    "# Create a grid for Turkey\n",
    "lon_range = np.linspace(25, 45, 10)\n",
    "lat_range = np.linspace(35, 43, 10)\n",
    "grid_points = []\n",
    "\n",
    "for lon in lon_range:\n",
    "    for lat in lat_range:\n",
    "        grid_points.append((lon, lat))\n",
    "\n",
    "# Calculate density at grid points\n",
    "grid_densities = []\n",
    "for lon, lat in grid_points:\n",
    "    density = calc_fault_density(lat, lon, fault_gdf)\n",
    "    density['lon'] = lon\n",
    "    density['lat'] = lat\n",
    "    grid_densities.append(density)\n",
    "\n",
    "grid_df = pd.DataFrame(grid_densities)\n",
    "\n",
    "# For each earthquake, find nearest grid point and assign its density\n",
    "def assign_grid_density(row, grid_df):\n",
    "    distances = []\n",
    "    for idx, grid_point in grid_df.iterrows():\n",
    "        dist = haversine(row['Longitude'], row['Latitude'], grid_point['lon'], grid_point['lat'])\n",
    "        distances.append((dist, idx))\n",
    "    \n",
    "    closest_idx = min(distances, key=lambda x: x[0])[1]\n",
    "    return pd.Series({\n",
    "        'fault_count_50km': grid_df.iloc[closest_idx]['fault_count_50km'],\n",
    "        'fault_length_50km': grid_df.iloc[closest_idx]['fault_length_50km'],\n",
    "        'fault_density': grid_df.iloc[closest_idx]['fault_density']\n",
    "    })\n",
    "\n",
    "# Apply grid-based density estimation\n",
    "density_features = clean_df.apply(lambda row: assign_grid_density(row, grid_df), axis=1)\n",
    "clean_df = pd.concat([clean_df, density_features], axis=1)\n",
    "model_df = pd.concat([model_df, density_features], axis=1)\n",
    "\n",
    "# Add magnitude-distance interaction feature\n",
    "model_df['magnitude_fault_interaction'] = model_df['Magnitude'] / (model_df['distance_to_fault'] + 1)\n",
    "\n",
    "print(\"Feature engineering completed!\")\n",
    "model_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735a39cd",
   "metadata": {},
   "source": [
    "## 5. Model Selection and Training\n",
    "\n",
    "In this section, we'll implement and evaluate multiple regression models to predict earthquake magnitude based on our engineered features. We'll progress from simpler linear models to more complex ensemble methods, comparing their performance systematically.\n",
    "\n",
    "Our model selection strategy includes:\n",
    "\n",
    "1. **Linear models**:\n",
    "   - **Linear Regression**: Establishing a baseline with simple linear relationships\n",
    "   - **Ridge Regression** (L2 regularization): Adding penalty on coefficient magnitude to reduce overfitting\n",
    "   - **Lasso Regression** (L1 regularization): Implementing feature selection through coefficient shrinkage\n",
    "\n",
    "2. **Tree-based ensemble models**:\n",
    "   - **Random Forest**: Combining multiple decision trees through bagging (bootstrap aggregating)\n",
    "   - **Gradient Boosting**: Sequentially building trees that correct errors of previous trees\n",
    "\n",
    "3. **Advanced gradient boosting implementations**:\n",
    "   - **XGBoost**: Utilizing regularized gradient boosting with second-order gradients\n",
    "   - **LightGBM**: Implementing gradient-based one-side sampling and exclusive feature bundling for efficiency\n",
    "\n",
    "We'll implement a robust evaluation framework:\n",
    "- **Pipeline architecture**: Encapsulating preprocessing and modeling steps for reproducibility\n",
    "- **K-fold cross-validation**: Using 5-fold cross-validation to assess generalization performance\n",
    "- **Metrics suite**:\n",
    "  - RMSE (Root Mean Squared Error): Primary metric penalizing larger errors\n",
    "  - MAE (Mean Absolute Error): Secondary metric for error magnitude in original units\n",
    "  - R² (Coefficient of determination): Assessing proportion of variance explained\n",
    "\n",
    "For geophysical prediction problems like earthquake magnitude, tree-based ensemble methods often outperform linear models due to their ability to capture non-linear relationships and complex interactions between features. However, we'll evaluate all model classes to confirm this empirically for our specific dataset.\n",
    "\n",
    "The results of this comparative analysis will guide our selection of the most promising model for hyperparameter optimization in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd6c306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for modeling\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# Model Selection and Training\n",
    "print(\"Setting up model training...\")\n",
    "\n",
    "# Define features and target\n",
    "target = 'Magnitude'\n",
    "# Remove non-feature columns\n",
    "drops = ['Date', 'Location', 'EventID', 'TimeName', 'TypeName', \n",
    "         'MagnitudeName', 'Grid', 'PrevLon', 'PrevLat', 'PrevDate',\n",
    "         'nearest_fault_name']  # Remove string columns\n",
    "\n",
    "# Check if these optional columns exist and add them to drops if they do\n",
    "optional_drops = ['YearMonth']\n",
    "for col in optional_drops:\n",
    "    if col in model_df.columns:\n",
    "        drops.append(col)\n",
    "\n",
    "# First, create a preliminary feature list\n",
    "preliminary_features = [col for col in model_df.columns if col != target and col not in drops]\n",
    "\n",
    "# Check for non-numeric columns in our features\n",
    "for col in preliminary_features:\n",
    "    if col in model_df.columns and model_df[col].dtype == 'object':\n",
    "        print(f\"Removing non-numeric column: {col}\")\n",
    "        drops.append(col)\n",
    "\n",
    "# Final feature list with only numeric columns\n",
    "features = [col for col in model_df.columns if col != target and col not in drops]\n",
    "\n",
    "print(f\"Selected features: {features}\")\n",
    "\n",
    "# Define features to scale\n",
    "features_to_scale = ['Longitude', 'Latitude', 'Depth']\n",
    "other_features = [f for f in features if f not in features_to_scale]\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X = model_df[features]\n",
    "y = model_df[target]\n",
    "\n",
    "print(\"Columns with NaN values:\")\n",
    "for col in X.columns:\n",
    "    nan_count = X[col].isna().sum()\n",
    "    if nan_count > 0:\n",
    "        print(f\"- {col}: {nan_count} NaNs\")\n",
    "\n",
    "# Fill missing values appropriately for each column\n",
    "for col in X.columns:\n",
    "    if X[col].isna().sum() > 0:\n",
    "        # For numeric columns, use median\n",
    "        X[col] = X[col].fillna(X[col].median())\n",
    "\n",
    "# Also check target variable\n",
    "if y.isna().sum() > 0:\n",
    "    print(f\"Target has {y.isna().sum()} NaN values, filling with median\")\n",
    "    y = y.fillna(y.median())\n",
    "\n",
    "# Verify all NaNs are fixed\n",
    "print(f\"Remaining NaN values in X: {X.isna().sum().sum()}\")\n",
    "print(f\"Remaining NaN values in y: {y.isna().sum()}\")\n",
    "\n",
    "# Create new train-test split with cleaned data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Testing set size: {X_test.shape[0]}\")\n",
    "\n",
    "# Create preprocessing transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('geo_features', StandardScaler(), features_to_scale),\n",
    "        ('other_features', 'passthrough', other_features)\n",
    "    ])\n",
    "\n",
    "# Set up models with pipelines\n",
    "models = {\n",
    "    'Linear Regression': Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', LinearRegression())\n",
    "    ]),\n",
    "    'Ridge Regression': Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', Ridge())\n",
    "    ]),\n",
    "    'Lasso Regression': Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', Lasso())\n",
    "    ]),\n",
    "    'Random Forest': Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "    ]),\n",
    "    'Gradient Boosting': Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', GradientBoostingRegressor(n_estimators=100, random_state=42))\n",
    "    ]),\n",
    "    'XGBoost': Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', XGBRegressor(n_estimators=100, random_state=42))\n",
    "    ]),\n",
    "    'LightGBM': Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', LGBMRegressor(n_estimators=100, random_state=42))\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Function to evaluate models\n",
    "def evaluate_model(pipeline, X_train, X_test, y_train, y_test):\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    return mae, rmse, r2, pipeline\n",
    "\n",
    "# Cross-validation for more robust evaluation\n",
    "results = {}\n",
    "cv_results = {}\n",
    "fitted_models = {}\n",
    "\n",
    "for name, pipeline in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    mae, rmse, r2, fitted_pipeline = evaluate_model(pipeline, X_train, X_test, y_train, y_test)\n",
    "    fitted_models[name] = fitted_pipeline\n",
    "    \n",
    "    # 5-fold cross-validation for RMSE\n",
    "    cv_scores = -cross_val_score(pipeline, X, y, cv=5, scoring='neg_root_mean_squared_error')\n",
    "    \n",
    "    results[name] = {'MAE': mae, 'RMSE': rmse, 'R²': r2}\n",
    "    cv_results[name] = {'Mean RMSE': cv_scores.mean(), 'Std RMSE': cv_scores.std()}\n",
    "    \n",
    "    print(f\"{name} - MAE: {mae:.4f}, RMSE: {rmse:.4f}, R²: {r2:.4f}, CV RMSE: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "\n",
    "# Convert results to DataFrames for better visualization\n",
    "results_df = pd.DataFrame(results).T\n",
    "cv_results_df = pd.DataFrame(cv_results).T\n",
    "\n",
    "print(\"\\nTest Results:\")\n",
    "print(results_df.sort_values('RMSE'))\n",
    "\n",
    "print(\"\\nCross-Validation Results:\")\n",
    "print(cv_results_df.sort_values('Mean RMSE'))\n",
    "\n",
    "# Visualize model performance\n",
    "plt.figure(figsize=(12, 6))\n",
    "results_df['RMSE'].sort_values().plot(kind='bar')\n",
    "plt.title('RMSE by Model')\n",
    "plt.ylabel('RMSE (lower is better)')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Select the best performing model based on CV results\n",
    "best_model_name = cv_results_df.sort_values('Mean RMSE').index[0]\n",
    "print(f\"\\nBest model based on cross-validation: {best_model_name}\")\n",
    "best_pipeline = fitted_models[best_model_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb33df1b",
   "metadata": {},
   "source": [
    "## 6. Hyperparameter Optimization\n",
    "\n",
    "Fine-tuning model hyperparameters is crucial for maximizing predictive performance. While our previous step identified the best model class, optimal hyperparameter configuration can significantly enhance accuracy. We'll implement a systematic optimization approach for our best-performing model.\n",
    "\n",
    "Our hyperparameter optimization strategy includes:\n",
    "\n",
    "1. **Hyperparameter search space definition**:\n",
    "   - For tree-based models: `n_estimators` (number of trees), `max_depth`, `min_samples_split`, `learning_rate` (for boosting algorithms)\n",
    "   - For regularized linear models: regularization strength (`alpha`/`C`), `l1_ratio` (for ElasticNet)\n",
    "   - Model-specific parameters (e.g., `colsample_bytree` for XGBoost, `num_leaves` for LightGBM)\n",
    "\n",
    "2. **Search methodology**:\n",
    "   - **RandomizedSearchCV**: Implementing efficient stochastic search through hyperparameter space\n",
    "   - **n_iter=20**: Searching 20 random combinations to balance exploration with computational efficiency\n",
    "   - **5-fold cross-validation**: Ensuring optimized parameters generalize well\n",
    "\n",
    "3. **Evaluation strategy**:\n",
    "   - Using negative RMSE as scoring metric to align with our primary evaluation criterion\n",
    "   - Parallel processing (`n_jobs=-1`) to accelerate search across available CPU cores\n",
    "\n",
    "4. **Optimization outcome assessment**:\n",
    "   - Comparing best model performance against baseline configuration\n",
    "   - Analyzing which hyperparameters most strongly influence performance\n",
    "\n",
    "5. **Final model construction**:\n",
    "   - Building model with optimal hyperparameter configuration\n",
    "   - Validating performance improvement on held-out test set\n",
    "\n",
    "Hyperparameter optimization addresses the bias-variance tradeoff inherent in machine learning models. For complex tasks like earthquake magnitude prediction, finding the right balance between model complexity and generalization ability is essential. Parameters like tree depth and ensemble size directly control this tradeoff.\n",
    "\n",
    "The optimized model will represent the best compromise between capturing underlying patterns in the training data and maintaining generalization ability to new, unseen earthquake events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cf8c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Optimization\n",
    "print(f\"Optimizing hyperparameters for {best_model_name}...\")\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "# Define hyperparameter grids for each model type\n",
    "# You may need to adjust these based on your selected best model\n",
    "param_grids = {\n",
    "    'Random Forest': {\n",
    "        'model__n_estimators': [50, 100, 200],\n",
    "        'model__max_depth': [None, 10, 20, 30],\n",
    "        'model__min_samples_split': [2, 5, 10],\n",
    "        'model__min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'model__n_estimators': [50, 100, 200],\n",
    "        'model__learning_rate': [0.01, 0.1, 0.2],\n",
    "        'model__max_depth': [3, 5, 7],\n",
    "        'model__min_samples_split': [2, 5]\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'model__n_estimators': [50, 100, 200],\n",
    "        'model__learning_rate': [0.01, 0.1, 0.2],\n",
    "        'model__max_depth': [3, 5, 7],\n",
    "        'model__colsample_bytree': [0.7, 0.8, 0.9]\n",
    "    },\n",
    "    'LightGBM': {\n",
    "        'model__n_estimators': [50, 100, 200],\n",
    "        'model__learning_rate': [0.01, 0.1, 0.2],\n",
    "        'model__max_depth': [3, 5, 7],\n",
    "        'model__num_leaves': [31, 50, 70]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Get the appropriate parameter grid\n",
    "if best_model_name in param_grids:\n",
    "    param_grid = param_grids[best_model_name]\n",
    "    \n",
    "    # Use RandomizedSearchCV for efficiency\n",
    "    random_search = RandomizedSearchCV(\n",
    "        best_pipeline, \n",
    "        param_distributions=param_grid,\n",
    "        n_iter=20,  # Number of parameter settings sampled\n",
    "        cv=5,\n",
    "        scoring='neg_root_mean_squared_error',\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Fit the random search\n",
    "    random_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Print best parameters and score\n",
    "    print(f\"Best parameters: {random_search.best_params_}\")\n",
    "    print(f\"Best RMSE: {-random_search.best_score_:.4f}\")\n",
    "    \n",
    "    # Create the optimized model\n",
    "    best_pipeline = random_search.best_estimator_\n",
    "else:\n",
    "    print(f\"No parameter grid defined for {best_model_name}. Using default model.\")\n",
    "    best_pipeline = fitted_models[best_model_name]\n",
    "\n",
    "# Final evaluation with the best model\n",
    "y_pred = best_pipeline.predict(X_test)\n",
    "final_mae = mean_absolute_error(y_test, y_pred)\n",
    "final_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "final_r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nFinal model performance:\")\n",
    "print(f\"MAE: {final_mae:.4f}\")\n",
    "print(f\"RMSE: {final_rmse:.4f}\")\n",
    "print(f\"R²: {final_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa2c93b",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation and Interpretation\n",
    "\n",
    "Rigorous evaluation and interpretation of our optimized model provides insights into both its predictive performance and the underlying factors driving earthquake magnitudes. This analysis goes beyond simple accuracy metrics to understand model behavior and limitations.\n",
    "\n",
    "Our comprehensive evaluation framework includes:\n",
    "\n",
    "1. **Performance visualization**:\n",
    "   - **Actual vs. predicted scatter plot**: Visualizing prediction accuracy across the magnitude range\n",
    "   - **Perfect prediction line**: Adding y=x reference line to identify systematic over/under-prediction\n",
    "   - **Magnitude-specific accuracy assessment**: Evaluating whether prediction quality varies with earthquake size\n",
    "\n",
    "2. **Residual analysis**:\n",
    "   - **Residual scatter plot**: Examining errors (y_true - y_pred) across the prediction range to identify heteroscedasticity\n",
    "   - **Zero-error reference line**: Establishing baseline for unbiased predictions\n",
    "   - **Residual distribution histogram**: Assessing normality of errors using KDE overlay\n",
    "\n",
    "3. **Feature importance analysis**:\n",
    "   - For tree-based models: Extracting and visualizing feature importance based on mean decrease in impurity\n",
    "   - For linear models: Analyzing standardized coefficients to assess feature influence\n",
    "   - Relating feature importance to geophysical understanding of earthquake mechanisms\n",
    "\n",
    "4. **Model persistence**:\n",
    "   - Saving the complete model pipeline as a serialized object using joblib\n",
    "   - Documenting model dependencies and versioning for reproducibility\n",
    "\n",
    "Understanding feature importance provides valuable scientific insights beyond predictive performance. For instance, if fault proximity features rank highly, this confirms the geological understanding that major faults drive larger earthquakes. If temporal features show importance, this might suggest periodicity in large earthquake occurrences.\n",
    "\n",
    "Model limitations should also be acknowledged - particularly the challenge of predicting rare, high-magnitude events which may be underrepresented in the training data. Such limitations inform the appropriate application of model predictions in risk assessment contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76df36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation and Interpretation\n",
    "print(\"Evaluating final model...\")\n",
    "\n",
    "# Visualize actual vs predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "plt.xlabel('Actual Magnitude')\n",
    "plt.ylabel('Predicted Magnitude')\n",
    "plt.title('Actual vs Predicted Earthquake Magnitude')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot residuals\n",
    "residuals = y_test - y_pred\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_pred, residuals, alpha=0.5)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Predicted Magnitude')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residual Plot')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze residual distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(residuals, kde=True)\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Residuals')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Feature importance (for tree-based models)\n",
    "try:\n",
    "    # Extract the model component from the pipeline\n",
    "    model_component = best_pipeline.named_steps['model']\n",
    "    \n",
    "    # Check if it has feature importances\n",
    "    if hasattr(model_component, 'feature_importances_'):\n",
    "        # Get preprocessed feature names - slightly tricky with ColumnTransformer\n",
    "        # For simplicity, we'll use the original feature names\n",
    "        # Create DataFrame of feature importances\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'Feature': features,\n",
    "            'Importance': model_component.feature_importances_\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "        \n",
    "        # Visualize feature importances\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.barplot(x='Importance', y='Feature', data=feature_importance.head(15))\n",
    "        plt.title('Top 15 Feature Importances')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"Top 10 most important features:\")\n",
    "        print(feature_importance.head(10))\n",
    "except:\n",
    "    print(\"Could not extract feature importances from the model.\")\n",
    "\n",
    "# Save the entire pipeline - this contains both preprocessing and model\n",
    "import joblib\n",
    "joblib.dump(best_pipeline, 'models/earthquake_pipeline.pkl')\n",
    "print(\"Pipeline saved as 'models/earthquake_pipeline.pkl'\")\n",
    "\n",
    "# Also save the clean dataset with original coordinates for unsupervised learning\n",
    "clean_df.to_csv('produced_data/clean_earthquake_data.csv', index=False)\n",
    "print(\"Clean data with original coordinates saved as 'produced_data/clean_earthquake_data.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a31152",
   "metadata": {},
   "source": [
    "## 8. Conclusion and Next Steps\n",
    "\n",
    "Our supervised learning approach has successfully developed a predictive model for earthquake magnitude in Turkey based on geographic, temporal, and fault-related features. This model provides valuable insights into the factors influencing earthquake severity and establishes a foundation for seismic risk assessment.\n",
    "\n",
    "**Key achievements**:\n",
    "- Created a comprehensive feature engineering pipeline capturing multiple aspects of seismic activity\n",
    "- Implemented and compared multiple regression algorithms for magnitude prediction\n",
    "- Optimized hyperparameters to maximize predictive performance\n",
    "- Identified the most influential factors in determining earthquake magnitude\n",
    "- Established a quantitative framework for earthquake magnitude estimation\n",
    "\n",
    "**Limitations**:\n",
    "- Prediction of extreme events remains challenging due to their rarity in the training dataset\n",
    "- Temporal coverage limitations in historical earthquake catalogs\n",
    "- Incomplete mapping of smaller fault systems may affect proximity calculations\n",
    "- Inherent unpredictability in complex geophysical systems\n",
    "\n",
    "**Future directions**:\n",
    "1. Integrating additional data sources like geodetic measurements, background seismicity rates, and crustal stress maps\n",
    "2. Exploring sequence-based models (RNNs/LSTMs) that can capture temporal dependencies between events\n",
    "3. Implementing Bayesian methods to quantify prediction uncertainty\n",
    "4. Developing ensemble approaches combining multiple model architectures\n",
    "\n",
    "In the next notebook (unsupervised.ipynb), we'll complement this supervised approach by implementing clustering and density-based analysis to identify natural groupings in earthquake patterns and high-risk zones. These unsupervised techniques will potentially reveal regional patterns and risk concentrations that might not be evident through regression analysis alone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8ceb2b",
   "metadata": {},
   "source": [
    "## 9. GPU Acceleration with PyTorch (Bonus)\n",
    "\n",
    "In this section, we'll leverage GPU computation using PyTorch to implement a deep learning approach for earthquake magnitude prediction. This addresses the bonus requirement for GPU acceleration while exploring whether neural networks can capture more complex relationships than traditional machine learning models.\n",
    "\n",
    "### 9.1 PyTorch Model Setup\n",
    "\n",
    "Neural networks offer several advantages for geophysical modeling:\n",
    "- **Automatic feature interaction detection**: Neural networks can discover complex non-linear relationships without explicit feature engineering\n",
    "- **Hierarchical pattern recognition**: Deep architectures can learn representations at multiple levels of abstraction\n",
    "- **GPU acceleration**: Matrix operations fundamental to neural networks benefit significantly from GPU parallelization\n",
    "\n",
    "We'll implement a feedforward neural network with:\n",
    "- Multiple dense layers with ReLU activations\n",
    "- Batch normalization for training stability\n",
    "- Dropout regularization to prevent overfitting\n",
    "- Adam optimizer with learning rate scheduling\n",
    "- Mean Squared Error loss function\n",
    "\n",
    "The implementation will utilize CUDA cores if available, demonstrating the performance advantages of GPU acceleration for model training. We'll benchmark training times against CPU-based models to quantify the speed improvement, which can be substantial for larger datasets or more complex architectures.\n",
    "\n",
    "This approach allows us to compare both the predictive performance and computational efficiency of deep learning against traditional machine learning methods for earthquake magnitude prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b05b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if needed (uncomment and run if necessary)\n",
    "# !pip install torch torchvision torchaudio\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7e6e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Neural Network architecture for earthquake prediction\n",
    "class EarthquakeNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(EarthquakeNN, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c07fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for PyTorch\n",
    "# Use the same features as in our traditional models\n",
    "X_torch = X.copy()\n",
    "\n",
    "# Scale the features\n",
    "scaler_torch = StandardScaler()\n",
    "X_scaled = scaler_torch.fit_transform(X_torch)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.FloatTensor(X_scaled)\n",
    "y_tensor = torch.FloatTensor(y.values.reshape(-1, 1))\n",
    "\n",
    "# Create train/test split - use the same proportions but create fresh tensors\n",
    "X_train_tensor = torch.FloatTensor(scaler_torch.transform(X_train))\n",
    "y_train_tensor = torch.FloatTensor(y_train.values.reshape(-1, 1))\n",
    "X_test_tensor = torch.FloatTensor(scaler_torch.transform(X_test))\n",
    "y_test_tensor = torch.FloatTensor(y_test.values.reshape(-1, 1))\n",
    "\n",
    "# Create PyTorch datasets and dataloaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Testing samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e959385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PyTorch model, loss function and optimizer\n",
    "model = EarthquakeNN(input_size=X_train.shape[1])\n",
    "model = model.to(device)  # Move model to GPU if available\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c5962c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "def train_and_evaluate():\n",
    "    epochs = 100\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 10\n",
    "    counter = 0\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    # Record training time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        train_loss = running_loss / len(train_dataset)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        val_loss = val_loss / len(test_dataset)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "        \n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "        \n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    # Calculate total training time\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"Training completed in {training_time:.2f} seconds\")\n",
    "    \n",
    "    return train_losses, val_losses, training_time\n",
    "\n",
    "# Run training\n",
    "train_losses, val_losses, gpu_training_time = train_and_evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e5b2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on test data\n",
    "def evaluate_model():\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            all_preds.append(outputs.cpu().numpy())\n",
    "            all_targets.append(targets.cpu().numpy())\n",
    "    \n",
    "    # Concatenate all predictions and targets\n",
    "    y_pred_torch = np.vstack(all_preds).flatten()\n",
    "    y_test_torch = np.vstack(all_targets).flatten()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    torch_mae = mean_absolute_error(y_test_torch, y_pred_torch)\n",
    "    torch_rmse = np.sqrt(mean_squared_error(y_test_torch, y_pred_torch))\n",
    "    torch_r2 = r2_score(y_test_torch, y_pred_torch)\n",
    "    \n",
    "    print(f\"PyTorch Neural Network Results:\")\n",
    "    print(f\"MAE: {torch_mae:.4f}\")\n",
    "    print(f\"RMSE: {torch_rmse:.4f}\")\n",
    "    print(f\"R²: {torch_r2:.4f}\")\n",
    "    \n",
    "    return torch_mae, torch_rmse, torch_r2, y_pred_torch\n",
    "\n",
    "# Evaluate model\n",
    "torch_mae, torch_rmse, torch_r2, y_pred_torch = evaluate_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d92de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize actual vs predicted\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred_torch, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "plt.xlabel('Actual Magnitude')\n",
    "plt.ylabel('Predicted Magnitude (PyTorch)')\n",
    "plt.title('Actual vs Predicted Earthquake Magnitude (PyTorch)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbef7d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare PyTorch with traditional model results\n",
    "comparison_data = {\n",
    "    'Model': ['PyTorch NN (GPU)', f'{best_model_name} (CPU)'],\n",
    "    'MAE': [torch_mae, final_mae],\n",
    "    'RMSE': [torch_rmse, final_rmse],\n",
    "    'R²': [torch_r2, final_r2],\n",
    "    'Training Time (s)': [gpu_training_time, None]  # We don't have CPU time recorded\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"Model Performance Comparison:\")\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90da5118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the PyTorch model\n",
    "torch.save(model.state_dict(), 'models/earthquake_pytorch_model.pt')\n",
    "print(\"PyTorch model saved as 'models/earthquake_pytorch_model.pt'\")\n",
    "\n",
    "# Also save a script to load and use the model\n",
    "with open('models/load_pytorch_model.py', 'w') as f:\n",
    "    f.write(\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class EarthquakeNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(EarthquakeNN, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Function to load model\n",
    "def load_model(model_path, input_size, device='cpu'):\n",
    "    model = EarthquakeNN(input_size)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "    return model\n",
    "\"\"\")\n",
    "print(\"Model loading script saved as 'models/load_pytorch_model.py'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
